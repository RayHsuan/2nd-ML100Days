<!DOCTYPE html>
<!-- saved from url=(0037)https://zhuanlan.zhihu.com/p/30721429 -->
<html lang="zh" data-hairline="true" data-theme="light"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>【博客存档】机器学习模型评估 - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="renderer" content="webkit"><meta name="force-rendering" content="webkit"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"><meta data-react-helmet="true" name="description" content="前言当数据好了之后，你所需的只是调下开源包，然后一个模型就出来了，但是，好与不好？谁来界定？这篇文章，主要针对模型的评估，系统介绍下各种不同的模型的各种评测标准，主要参考Alice Zhang的这篇文章http://…"><meta data-react-helmet="true" property="og:title" content="【博客存档】机器学习模型评估"><meta data-react-helmet="true" property="og:url" content="https://zhuanlan.zhihu.com/p/30721429"><meta data-react-helmet="true" property="og:description" content="前言当数据好了之后，你所需的只是调下开源包，然后一个模型就出来了，但是，好与不好？谁来界定？这篇文章，主要针对模型的评估，系统介绍下各种不同的模型的各种评测标准，主要参考Alice Zhang的这篇文章http://…"><meta data-react-helmet="true" property="og:image" content=""><meta data-react-helmet="true" property="og:type" content="article"><meta data-react-helmet="true" property="og:site_name" content="知乎专栏"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png" sizes="152x152"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.b3e6278d.png" sizes="120x120"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7a750095.png" sizes="76x76"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.a4a761d4.png" sizes="60x60"><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/static/search.xml" title="知乎"><link rel="dns-prefetch" href="https://static.zhimg.com/"><link rel="dns-prefetch" href="https://pic1.zhimg.com/"><link rel="dns-prefetch" href="https://pic2.zhimg.com/"><link rel="dns-prefetch" href="https://pic3.zhimg.com/"><link rel="dns-prefetch" href="https://pic4.zhimg.com/"><style>
.u-safeAreaInset-top {
  height: constant(safe-area-inset-top) !important;
  height: env(safe-area-inset-top) !important;
  
}
.u-safeAreaInset-bottom {
  height: constant(safe-area-inset-bottom) !important;
  height: env(safe-area-inset-bottom) !important;
  
}
</style><link href="./【博客存档】机器学习模型评估 - 知乎_files/app.50fa1c1b9cd3bf447fd0.css" rel="stylesheet"><link rel="stylesheet"><script defer="" crossorigin="anonymous" src="./【博客存档】机器学习模型评估 - 知乎_files/init.js.下載" data-sentry-config="{&quot;dsn&quot;:&quot;https://65e244586890460588f00f2987137aa8@crash2.zhihu.com/193&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;2250-11349395&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrors&quot;:[&quot;origin message&quot;,&quot;Network request failed&quot;,&quot;Loading chunk&quot;,&quot;这个系统不支持该功能。&quot;,&quot;Can&#39;t find variable: webkit&quot;,&quot;Can&#39;t find variable: $&quot;,&quot;内存不足&quot;,&quot;out of memory&quot;,&quot;DOM Exception 18&quot;,&quot;The operation is insecure&quot;,&quot;[object Event]&quot;,&quot;[object FileError]&quot;,&quot;[object DOMError]&quot;,&quot;[object Object]&quot;,&quot;拒绝访问。&quot;,&quot;Maximum call stack size exceeded&quot;,&quot;UploadError&quot;,&quot;无法 fetch&quot;,&quot;draft-js&quot;,&quot;缺少 JavaScript 对象&quot;,&quot;componentWillEnter&quot;,&quot;componentWillLeave&quot;,&quot;componentWillAppear&quot;,&quot;getInlineStyleAt&quot;,&quot;getCharacterList&quot;],&quot;whitelistUrls&quot;:[&quot;static.zhihu.com&quot;]}"></script><link rel="stylesheet" type="text/css" href="./【博客存档】机器学习模型评估 - 知乎_files/modals.e7bf80400b159ca850db.css"><script charset="utf-8" src="./【博客存档】机器学习模型评估 - 知乎_files/column.modals.fb7a3810b3dd3c63cd8c.js.下載"></script><link rel="stylesheet" type="text/css" href="./【博客存档】机器学习模型评估 - 知乎_files/richinput.aaf6d724da7e7bd2d1f4.css"><script charset="utf-8" src="./【博客存档】机器学习模型评估 - 知乎_files/column.richinput.b567a62c816b5449b33f.js.下載"></script></head><body class="WhiteBg-body"><div id="root"><div class="App" data-reactroot=""><div class="LoadingBar"></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;&quot;}" data-zop="{&quot;authorName&quot;:&quot;想飞的石头&quot;,&quot;itemId&quot;:30721429,&quot;title&quot;:&quot;【博客存档】机器学习模型评估&quot;,&quot;type&quot;:&quot;article&quot;}" data-za-detail-view-path-module="PostItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Post&quot;,&quot;token&quot;:&quot;30721429&quot;}}}"><div class="ColumnPageHeader-Wrapper"><div class="AdblockBanner"><div class="AdblockBanner-inner">我们检测到你可能使用了 AdBlock 或 Adblock Plus，它的部分策略可能会影响到正常功能的使用（如关注）。<br>你可以设定特殊规则或将知乎加入白名单，以便我们更好地提供服务。 （<a href="https://www.zhihu.com/question/54919485" target="_blank">为什么？</a>）</div><button type="button" class="Button AdblockBanner-close Button--plain"><svg viewBox="0 0 14 14" class="Icon Icon--remove" width="16" height="16" aria-hidden="true" style="height: 16px; width: 16px;"><title></title><g><path d="M8.486 7l5.208-5.207c.408-.408.405-1.072-.006-1.483-.413-.413-1.074-.413-1.482-.005L7 5.515 1.793.304C1.385-.103.72-.1.31.31-.103.724-.103 1.385.305 1.793L5.515 7l-5.21 5.207c-.407.408-.404 1.072.007 1.483.413.413 1.074.413 1.482.005L7 8.485l5.207 5.21c.408.407 1.072.404 1.483-.007.413-.413.413-1.074.005-1.482L8.485 7z"></path></g></svg></button></div><div><div class="Sticky ColumnPageHeader" style=""><div class="ColumnPageHeader-content"><a href="https://www.zhihu.com/" aria-label="知乎"><svg viewBox="0 0 200 91" class="Icon ZhihuLogo ZhihuLogo--blue Icon--logo" style="height:30px;width:64px" width="64" height="30" aria-hidden="true"><title></title><g><path d="M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z" fill-rule="evenodd"></path></g></svg></a><i class="ColumnPageHeader-Line"></i><div class="ColumnPageHeader-Title"><a class="ColumnLink ColumnPageHeader-Link" href="https://zhuanlan.zhihu.com/burness-DL"><img class="Avatar Avatar--round" width="30" height="30" src="./【博客存档】机器学习模型评估 - 知乎_files/v2-464976646760cff7f02930d5fa850d56_is.jpg" srcset="https://pic3.zhimg.com/v2-464976646760cff7f02930d5fa850d56_im.jpg 2x" alt="小石头的码疯窝"></a><div class="ColumnPageHeader-TitleName"><span class="ColumnPageHeader-TitleMeta">首发于</span><a class="ColumnLink ColumnPageHeader-TitleColumn" href="https://zhuanlan.zhihu.com/burness-DL">小石头的码疯窝</a></div></div><div class="ColumnPageHeader-Button"><button type="button" class="Button FollowButton ColumnPageHeader-FollowButton Button--primary Button--blue">关注专栏</button><button type="button" class="Button ColumnPageHeader-WriteButton Button--blue"><svg class="Zi Zi--EditSurround" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M18.453 7.992l-1.833-1.65.964-.978a1.223 1.223 0 0 1 1.73-.012l.005.006a1.24 1.24 0 0 1 .007 1.748l-.873.886zm-1.178 1.194l-5.578 5.66-1.935.697a.393.393 0 0 1-.504-.504l.697-1.935 5.488-5.567 1.832 1.65zM7.58 5.848l5.654.006-1.539 1.991-3.666.012A1.02 1.02 0 0 0 7 8.868v7.993c0 .558.46 1.01 1.029 1.01l7.941-.01c.568 0 1.03-.453 1.03-1.012v-4.061l2-1.442v6.002c0 1.397-1.2 2.501-2.62 2.501H7.574C6.153 19.85 5 18.717 5 17.32V8.35c0-1.397 1.16-2.502 2.58-2.502z"></path></svg>写文章</button></div></div></div></div></div><article class="Post-Main Post-NormalMain" tabindex="-1"><header class="Post-Header"><h1 class="Post-Title">【博客存档】机器学习模型评估</h1><div class="Post-Author"><div class="AuthorInfo" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="想飞的石头"><meta itemprop="image" content="https://pic3.zhimg.com/v2-0f6e8c23103c91a60ae2d9a9216b835f_l.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/duan-shi-shi-68"><meta itemprop="zhihu:followerCount"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover1-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover1-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/duan-shi-shi-68"><img class="Avatar Avatar--round AuthorInfo-avatar" width="38" height="38" src="./【博客存档】机器学习模型评估 - 知乎_files/v2-0f6e8c23103c91a60ae2d9a9216b835f_xs.jpg" srcset="https://pic3.zhimg.com/v2-0f6e8c23103c91a60ae2d9a9216b835f_l.jpg 2x" alt="想飞的石头"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover2-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover2-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/duan-shi-shi-68">想飞的石头</a></div></div><a class="UserLink-badge" data-tooltip="已认证的个人" href="https://www.zhihu.com/question/48510028" target="_blank" rel="noopener noreferrer"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--BadgeCert" fill="currentColor" viewBox="0 0 24 24" width="18" height="18"><g fill="none" fill-rule="evenodd"><path fill="#0F88EB" d="M2.64 13.39c1.068.895 1.808 2.733 1.66 4.113l.022-.196c-.147 1.384.856 2.4 2.24 2.278l-.198.016c1.387-.122 3.21.655 4.083 1.734l-.125-.154c.876 1.084 2.304 1.092 3.195.027l-.127.152c.895-1.068 2.733-1.808 4.113-1.66l-.198-.022c1.386.147 2.402-.856 2.279-2.238l.017.197c-.122-1.388.655-3.212 1.734-4.084l-.154.125c1.083-.876 1.092-2.304.027-3.195l.152.127c-1.068-.895-1.808-2.732-1.66-4.113l-.022.198c.147-1.386-.856-2.4-2.24-2.279l.198-.017c-1.387.123-3.21-.654-4.083-1.733l.125.153c-.876-1.083-2.304-1.092-3.195-.027l.127-.152c-.895 1.068-2.733 1.808-4.113 1.662l.198.02c-1.386-.147-2.4.857-2.279 2.24L4.4 6.363c.122 1.387-.655 3.21-1.734 4.084l.154-.126c-1.083.878-1.092 2.304-.027 3.195l-.152-.127z"></path><path fill="#FFF" d="M9.78 15.728l-2.633-2.999s-.458-.705.242-1.362c.7-.657 1.328-.219 1.328-.219l1.953 2.132 4.696-4.931s.663-.348 1.299.198c.636.545.27 1.382.27 1.382s-3.466 3.858-5.376 5.782c-.98.93-1.778.017-1.778.017z"></path></g></svg></span></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="AuthorInfo-badgeText">网易 资深算法工程师</div></div></div></div></div></div><div><span class="Voters"><button type="button" class="Button Button--plain">18 人<!-- -->赞同了该文章</button></span></div></header><div class="Post-RichTextContainer"><div class="RichText ztext Post-RichText"><h2><b>前言</b></h2><p>当数据好了之后，你所需的只是调下开源包，然后一个模型就出来了，但是，好与不好？谁来界定？</p><p>这篇文章，主要针对模型的评估，系统介绍下各种不同的模型的各种评测标准，主要参考Alice Zhang的这篇文章<u><a href="https://link.zhihu.com/?target=http%3A//www.oreilly.com/data/free/evaluating-machine-learning-models.csp" class=" external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043"><span class="invisible">http://www.</span><span class="visible">oreilly.com/data/free/e</span><span class="invisible">valuating-machine-learning-models.csp</span><span class="ellipsis"></span></a></u>。</p><p class="ztext-empty-paragraph"><br></p><h2><b>1-基础理解</b></h2><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic1.zhimg.com/v2-1285da230b6a3e31534ee334dfb5a1ac_b.jpg" data-caption="" data-rawwidth="898" data-rawheight="786" class="origin_image zh-lightbox-thumb" width="898" data-original="https://pic1.zhimg.com/v2-1285da230b6a3e31534ee334dfb5a1ac_r.jpg"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-1285da230b6a3e31534ee334dfb5a1ac_hd.jpg" data-caption="" data-rawwidth="898" data-rawheight="786" class="origin_image zh-lightbox-thumb lazy" width="898" data-original="https://pic1.zhimg.com/v2-1285da230b6a3e31534ee334dfb5a1ac_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-1285da230b6a3e31534ee334dfb5a1ac_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><p>Figure1-1是一个比较合理的产生机器学习模型的workflow，首先，我们拿到Historical data 然后应用到我们选择的model，然后对数据进行离线评测，离线评测一般我们会从Historical data中，通过一些策略选择出一些数据作为Validation，用来离线评测我们的模型，进行model selection和model params selection；也会引入一些live data来离线评价模型，待选择出合理的model和对应的params后，会对线上数据来一些相关的线上测试，例如本人所在公司会按流量对新旧model来进行A/B testing，利用最终的kpi指标来作为model的评判标准</p><h2><b>2-模型评估标准</b></h2><p>ML中，有多重不同考量的model，不同的目标有不同的评估标准，本节主要介绍Classification Metrics、Regression Metrics、Ranking Metrics</p><h2><b>2.1-Classification Metrics</b></h2><h2>Accuracy</h2><p>分类Accuracy就是指在分类方法中，被正确分类的样本数据占所有样本数量的比例。</p><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic2.zhimg.com/v2-976346c986cbef853a287db4f77e9e95_b.jpg" data-caption="" data-rawwidth="398" data-rawheight="66" class="content_image" width="398"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-976346c986cbef853a287db4f77e9e95_hd.jpg" data-caption="" data-rawwidth="398" data-rawheight="66" class="content_image lazy" width="398" data-actualsrc="https://pic2.zhimg.com/v2-976346c986cbef853a287db4f77e9e95_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><h2>Confusion Matrix</h2><p>Accuracy的计算十分简便，但是类别之间是等价的，很多时候，由于判断为某类的代价不一致，我们不能简单地利用Accuracy来说明某个分类器的好坏。比如一个医生将患病病人评价为没有患病的情况比将未患病用户判定为患病用户的代价要大得多，后者可以通过其他检测来继续验证，而前者则很难；另外当本身训练数据中各样本数量分布极度不均衡的时候，比如#0/#1=9:1，即使是一个分类器将所有样本全部判断为0时，这个分类的accuracy也达到了90%，很显然这里是有问题的。</p><p>假定某样本有100个正样本与200个负样本，confusion table如下：</p><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic4.zhimg.com/v2-cb8c94e9509e9e005d8ac629f1809a03_b.jpg" data-caption="" data-rawwidth="894" data-rawheight="148" class="origin_image zh-lightbox-thumb" width="894" data-original="https://pic4.zhimg.com/v2-cb8c94e9509e9e005d8ac629f1809a03_r.jpg"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-cb8c94e9509e9e005d8ac629f1809a03_hd.jpg" data-caption="" data-rawwidth="894" data-rawheight="148" class="origin_image zh-lightbox-thumb lazy" width="894" data-original="https://pic4.zhimg.com/v2-cb8c94e9509e9e005d8ac629f1809a03_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-cb8c94e9509e9e005d8ac629f1809a03_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><p>从这张图表中，我们可以很明显的看出在正分类中，我们的分类器有较低的准确率：(80/(20+80)=80%)，负分类中准确率为(195/(195+5)=97.5%)，如果仅仅考虑全局的accuracy，(80+195)/(100+200)=91.7%，丢失了很多信息。</p><h2>Per-Class Accuracy</h2><p>在上面例子中，对每类的accuracy做一个平均：(80%+97.5%)/2=88.75%，和之前的准确率相差较大，尤其是在分布极度不均的正负样本数量时，9+1-判断为10+,accuracy为90%，(100%+0)/2=50%</p><h2>Log-Loss</h2><p>在Logisitic Regression分类器中，最终的分类是指定阈值，然后对predict的值来进行判断进行分类，假定指定阈值0.5，model计算得到属于class 1的概率为0.51，这里有一个错误，但是这里有余概率与分类阈值相差很少，Log-Loss就是一个将此类因素考虑的标准：</p><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic1.zhimg.com/v2-e025588625fb004bc25c3039f2254ee8_b.jpg" data-caption="" data-rawwidth="704" data-rawheight="52" class="origin_image zh-lightbox-thumb" width="704" data-original="https://pic1.zhimg.com/v2-e025588625fb004bc25c3039f2254ee8_r.jpg"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-e025588625fb004bc25c3039f2254ee8_hd.jpg" data-caption="" data-rawwidth="704" data-rawheight="52" class="origin_image zh-lightbox-thumb lazy" width="704" data-original="https://pic1.zhimg.com/v2-e025588625fb004bc25c3039f2254ee8_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-e025588625fb004bc25c3039f2254ee8_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><p>pi是属于ith class的概率，yi是第ith的真实label，如果数据功底较强的人可能一眼就可以看出，这里其实就是y和p分布的Cross-Entropy，即真实label与预测的y的分布之间的差异。最小化Log-Loss即为最大化分类器的性能。</p><h2>AUC</h2><p>AUC即Area Under the Curve，这里的Curve就是ROC曲线，ROC的横坐标为Flase positive rate，纵坐标为Ture Positive Rate，用分类器的FP和TP来衡量分类器的性能好坏。而这里ROC是一个曲线而非一个值，AUC就是将该ROC用一个数值表示，这个数值就是曲线之下的面积。</p><p class="ztext-empty-paragraph"><br></p><h2><b>2.2-Ranking Metrics</b></h2><p>Ranking Metrics和前面的分类的merics，有很多相似的地方，例如，用户给定一个query，然后搜索引擎会反馈一个item list， 这个item list会按照与用户query的相关性来进行排序，其本质就是一个0/1的二元分类器，其中score是分类为1的概率，以此为标准来进行相关性的判定。当然Ranking Metrics很多时候也使用Regression的Metrics，例如在个性化推荐系统中，会通过各种数据的feature来进行一个score的计算，并以此为标准对推荐结果进行排序。</p><p>这里，我们首先介绍下Precision-Recall，也就是在分类中经常使用的来作为Ranking Metrics</p><h2>Precision Recall</h2><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic2.zhimg.com/v2-58da2d74cf160b96f49ad6574dcfe171_b.jpg" data-caption="" data-rawwidth="766" data-rawheight="496" class="origin_image zh-lightbox-thumb" width="766" data-original="https://pic2.zhimg.com/v2-58da2d74cf160b96f49ad6574dcfe171_r.jpg"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-58da2d74cf160b96f49ad6574dcfe171_hd.jpg" data-caption="" data-rawwidth="766" data-rawheight="496" class="origin_image zh-lightbox-thumb lazy" width="766" data-original="https://pic2.zhimg.com/v2-58da2d74cf160b96f49ad6574dcfe171_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-58da2d74cf160b96f49ad6574dcfe171_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic4.zhimg.com/v2-4d31ad9683e05e0e380c4589138ff51b_b.jpg" data-caption="" data-rawwidth="554" data-rawheight="136" class="origin_image zh-lightbox-thumb" width="554" data-original="https://pic4.zhimg.com/v2-4d31ad9683e05e0e380c4589138ff51b_r.jpg"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-4d31ad9683e05e0e380c4589138ff51b_hd.jpg" data-caption="" data-rawwidth="554" data-rawheight="136" class="origin_image zh-lightbox-thumb lazy" width="554" data-original="https://pic4.zhimg.com/v2-4d31ad9683e05e0e380c4589138ff51b_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-4d31ad9683e05e0e380c4589138ff51b_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><p>通常，我们在ranking中只对top K来进行计算，就是所谓的precision@k,recall@k，precision和recall之间的关系有点类似于True Postive 和False Postive之间的关系，单独谈其中一样是没有意义的，通常我们使用F1 score来表明其好坏：</p><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic2.zhimg.com/v2-b42d87f76ff7235dd5f6d022eeda3075_b.jpg" data-caption="" data-rawwidth="318" data-rawheight="70" class="content_image" width="318"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-b42d87f76ff7235dd5f6d022eeda3075_hd.jpg" data-caption="" data-rawwidth="318" data-rawheight="70" class="content_image lazy" width="318" data-actualsrc="https://pic2.zhimg.com/v2-b42d87f76ff7235dd5f6d022eeda3075_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><h2>NDCG</h2><p>NDCG是另一种很有效地排序标准，这里不对其做详细概念说明，只举一个例子就明白了，如想详细了解，请阅读<u><a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Discounted_cumulative_gain" class=" external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043"><span class="invisible">https://</span><span class="visible">en.wikipedia.org/wiki/D</span><span class="invisible">iscounted_cumulative_gain</span><span class="ellipsis"></span></a></u></p><p>假定某一个排序方法，给出的结果为D1,D2,D3,D4,D5,D6,而用户的相关得分（比如通过用户对其点击率来计算）为3,2,3,0,1,2。</p><p>则这个搜索的累积的熵为：</p><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic3.zhimg.com/v2-a990a86c731453186fa8f2155fe6ed4e_b.jpg" data-caption="" data-rawwidth="758" data-rawheight="118" class="origin_image zh-lightbox-thumb" width="758" data-original="https://pic3.zhimg.com/v2-a990a86c731453186fa8f2155fe6ed4e_r.jpg"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-a990a86c731453186fa8f2155fe6ed4e_hd.jpg" data-caption="" data-rawwidth="758" data-rawheight="118" class="origin_image zh-lightbox-thumb lazy" width="758" data-original="https://pic3.zhimg.com/v2-a990a86c731453186fa8f2155fe6ed4e_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-a990a86c731453186fa8f2155fe6ed4e_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><p>明显可知，CG对排序间item的相对位置不敏感，改变item彼此间的位置不影响CG的值，这是不合理的，这里我们添加一个Discounted信息：</p><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic2.zhimg.com/v2-b5918aa19b93041b0f3a49a5fd82ae45_b.jpg" data-caption="" data-rawwidth="350" data-rawheight="434" class="content_image" width="350"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-b5918aa19b93041b0f3a49a5fd82ae45_hd.jpg" data-caption="" data-rawwidth="350" data-rawheight="434" class="content_image lazy" width="350" data-actualsrc="https://pic2.zhimg.com/v2-b5918aa19b93041b0f3a49a5fd82ae45_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><p>这个ranking的DCG计算如下：</p><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic4.zhimg.com/v2-77b442036fde895638d451ba82a52173_b.jpg" data-caption="" data-rawwidth="1188" data-rawheight="132" class="origin_image zh-lightbox-thumb" width="1188" data-original="https://pic4.zhimg.com/v2-77b442036fde895638d451ba82a52173_r.jpg"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-77b442036fde895638d451ba82a52173_hd.jpg" data-caption="" data-rawwidth="1188" data-rawheight="132" class="origin_image zh-lightbox-thumb lazy" width="1188" data-original="https://pic4.zhimg.com/v2-77b442036fde895638d451ba82a52173_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-77b442036fde895638d451ba82a52173_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><p>同理，我们做一个最佳的排序的计算，这里最佳的排序是按照用户相关得分的排序：</p><p>3,3,2,2,1,0</p><p>此时，最佳的DCG = 8.69</p><p>最终的Normalize DCG=8.10/8.69=0.932</p><h2><b>2.3-Regression Metrics</b></h2><p>在回归任务中，我们一般需要去预测数值型的得分，例如我们会预测未来一段时间股票的价格，另外个性化系统预测用户对某个item的得分，类似的这些任务我们都会用到回归方法。</p><h2>RMSE</h2><p>在回归任务中，最普通的评估标准是RMSE（root-mean-square error)：</p><p class="ztext-empty-paragraph"><br></p><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic4.zhimg.com/v2-4b18bec63d9623b4ed5debba2ff8d007_b.jpg" data-caption="" data-rawwidth="288" data-rawheight="96" class="content_image" width="288"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-4b18bec63d9623b4ed5debba2ff8d007_hd.jpg" data-caption="" data-rawwidth="288" data-rawheight="96" class="content_image lazy" width="288" data-actualsrc="https://pic4.zhimg.com/v2-4b18bec63d9623b4ed5debba2ff8d007_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><h2>Quantiles of Errors</h2><p>RMSE有个比较严重的问题，它对large outliers比较敏感，通常一个比较大的离群值会很大地影响最终的RMSE值。Quantiles在某一方面来说，相对于RMSE来说鲁棒性比较高。</p><p>Median Absolute Percentage一般能够有效地减少离群值的影响：</p><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic1.zhimg.com/v2-b203dbef81f86269aaf545b9b576c89c_b.jpg" data-caption="" data-rawwidth="420" data-rawheight="52" class="content_image" width="420"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-b203dbef81f86269aaf545b9b576c89c_hd.jpg" data-caption="" data-rawwidth="420" data-rawheight="52" class="content_image lazy" width="420" data-actualsrc="https://pic1.zhimg.com/v2-b203dbef81f86269aaf545b9b576c89c_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><p>当然，我们也可以使用第&gt;90%的数据来找到数据当中的worst case，或者用&lt;0.1来表示数据当中的best case。</p><p class="ztext-empty-paragraph"><br></p><h2><b>2.4-Cautions</b></h2><h2>Training Metrics 和Evaluation Metrics的差异</h2><p>很多时候，Evaluation Metrics 和Training Metrics可以通用，我们可以直接选定Evaluation Metrics为目标函数来对其优化，例如RMSE，但是也有很多Evaluation Metrics 不能直接作为目标函数来优化。</p><h2>Skewed Datasets：Imbalanced classes，outliers， and Rare Data</h2><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic3.zhimg.com/v2-4b7ebe11879da830fbd9e947573db3a6_b.jpg" data-caption="" data-rawwidth="900" data-rawheight="430" class="origin_image zh-lightbox-thumb" width="900" data-original="https://pic3.zhimg.com/v2-4b7ebe11879da830fbd9e947573db3a6_r.jpg"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-4b7ebe11879da830fbd9e947573db3a6_hd.jpg" data-caption="" data-rawwidth="900" data-rawheight="430" class="origin_image zh-lightbox-thumb lazy" width="900" data-original="https://pic3.zhimg.com/v2-4b7ebe11879da830fbd9e947573db3a6_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-4b7ebe11879da830fbd9e947573db3a6_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><p>如果在datasets中，正负样本数相差很大，比如99/1，这样我们的分类器很容易全1，来达到accuracy达到99%，ROC也很好看，但此时其实算法的泛化能力很差，应该是无效的。</p><h2><b>3-线下评估机制</b></h2><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic4.zhimg.com/v2-ae5c8c9ef3389cc079899fa07d3ed67f_b.jpg" data-caption="" data-rawwidth="888" data-rawheight="554" class="origin_image zh-lightbox-thumb" width="888" data-original="https://pic4.zhimg.com/v2-ae5c8c9ef3389cc079899fa07d3ed67f_r.jpg"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-ae5c8c9ef3389cc079899fa07d3ed67f_hd.jpg" data-caption="" data-rawwidth="888" data-rawheight="554" class="origin_image zh-lightbox-thumb lazy" width="888" data-original="https://pic4.zhimg.com/v2-ae5c8c9ef3389cc079899fa07d3ed67f_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-ae5c8c9ef3389cc079899fa07d3ed67f_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><p>一般，我们将历史数据按某种策略分为训练数据和验证数据，以此我们做Model Training，根据相关评估标准来做Model Selection，选定好model方法之后，利用Validation data去做Hyperparameter tuner，选择出在验证集数据中性能最好的Hyperparameter sets。</p><p>很多时候，获取一个有效地历史训练数据集代价很大，我们通常只能获取到相对于真实数据很小的一部分数据，为了保证model的泛化能力，我们通常会采用很多其他的方法来充分验证，例如Hold-Out Validation,Cross-Validation,Bootstrap and Jackknife，这三种基本思想都相同，其中Hold-Out实现最简单，只是简单地将整个训练集分为训练集和验证集，然后用验证集的数据对训练集生成的model验证model有效性，Cross-Validation是将整个训练数据集划分为k-fold，多次取其中某一个fold做验证数据集，相对于Hold-Out Validation来说，相当于多次操作；前面两种可能大部分人都听说过，而Bootstrap很少有人了解，相对于Cross-Validation,其实质我们可以理解为，每次取K-fold里面的某部分做验证集，这其实是一种不放回的采样，而Bootstrap则恰好相反，它实质是一种由放回的采样原理：每次取其中某些数据做验证数据，然后放回重新选取，为什么要选择放回呢？统计学家们认为训练数据本身就有一种潜在的分布信息，我们称为”经验分布”，每次随机选取，然后不放回能够保证每次的经验分布都为原始的训练数据本身的分布信息，那么如此一来，bootstrap set中有很多数据是重复的（即为我们的经验分布），有个文档<u><a href="https://link.zhihu.com/?target=https%3A//lagunita.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/cv_boot.pdf" class=" external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043"><span class="invisible">https://</span><span class="visible">lagunita.stanford.edu/c</span><span class="invisible">4x/HumanitiesScience/StatLearning/asset/cv_boot.pdf</span><span class="ellipsis"></span></a></u> <u><a href="https://link.zhihu.com/?target=http%3A//www.americanscientist.org/issues/pub/2010/3/the-bootstrap/1" class=" external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043"><span class="invisible">http://www.</span><span class="visible">americanscientist.org/i</span><span class="invisible">ssues/pub/2010/3/the-bootstrap/1</span><span class="ellipsis"></span></a></u>里面有详细的说明。如果想试试具体效果，可以去sklearn里面尝试下：<u><a href="https://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/modules/grid_search.html%23out-of-bag-estimates" class=" external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043"><span class="invisible">http://</span><span class="visible">scikit-learn.org/stable</span><span class="invisible">/modules/grid_search.html#out-of-bag-estimates</span><span class="ellipsis"></span></a></u></p><h2><b>4-Hyperparameter Tuning</b></h2><p>首先，明白下Hyperparameter是个啥，和模型参数有啥区别</p><h2>4.1-Model Parameter vs Hyperparameter</h2><p>这里举个例子：我们有一个线性回归的模型来表示features和target之间关系：</p><p class="ztext-empty-paragraph"><br></p><h2><b>前言</b></h2><p>当数据好了之后，你所需的只是调下开源包，然后一个模型就出来了，但是，好与不好？谁来界定？</p><p>这篇文章，主要针对模型的评估，系统介绍下各种不同的模型的各种评测标准，主要参考Alice Zhang的这篇文章<u><a href="https://link.zhihu.com/?target=http%3A//www.oreilly.com/data/free/evaluating-machine-learning-models.csp" class=" external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043"><span class="invisible">http://www.</span><span class="visible">oreilly.com/data/free/e</span><span class="invisible">valuating-machine-learning-models.csp</span><span class="ellipsis"></span></a></u>。</p><p class="ztext-empty-paragraph"><br></p><h2><b>1-基础理解</b></h2><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic1.zhimg.com/v2-1285da230b6a3e31534ee334dfb5a1ac_b.jpg" data-caption="" data-rawwidth="898" data-rawheight="786" class="origin_image zh-lightbox-thumb" width="898" data-original="https://pic1.zhimg.com/v2-1285da230b6a3e31534ee334dfb5a1ac_r.jpg"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-1285da230b6a3e31534ee334dfb5a1ac_hd.jpg" data-caption="" data-rawwidth="898" data-rawheight="786" class="origin_image zh-lightbox-thumb lazy" width="898" data-original="https://pic1.zhimg.com/v2-1285da230b6a3e31534ee334dfb5a1ac_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-1285da230b6a3e31534ee334dfb5a1ac_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><p>Figure1-1是一个比较合理的产生机器学习模型的workflow，首先，我们拿到Historical data 然后应用到我们选择的model，然后对数据进行离线评测，离线评测一般我们会从Historical data中，通过一些策略选择出一些数据作为Validation，用来离线评测我们的模型，进行model selection和model params selection；也会引入一些live data来离线评价模型，待选择出合理的model和对应的params后，会对线上数据来一些相关的线上测试，例如本人所在公司会按流量对新旧model来进行A/B testing，利用最终的kpi指标来作为model的评判标准</p><h2><b>2-模型评估标准</b></h2><p>ML中，有多重不同考量的model，不同的目标有不同的评估标准，本节主要介绍Classification Metrics、Regression Metrics、Ranking Metrics</p><h2><b>2.1-Classification Metrics</b></h2><h2>Accuracy</h2><p>分类Accuracy就是指在分类方法中，被正确分类的样本数据占所有样本数量的比例。</p><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic2.zhimg.com/v2-976346c986cbef853a287db4f77e9e95_b.jpg" data-caption="" data-rawwidth="398" data-rawheight="66" class="content_image" width="398"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-976346c986cbef853a287db4f77e9e95_hd.jpg" data-caption="" data-rawwidth="398" data-rawheight="66" class="content_image lazy" width="398" data-actualsrc="https://pic2.zhimg.com/v2-976346c986cbef853a287db4f77e9e95_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><h2>Confusion Matrix</h2><p>Accuracy的计算十分简便，但是类别之间是等价的，很多时候，由于判断为某类的代价不一致，我们不能简单地利用Accuracy来说明某个分类器的好坏。比如一个医生将患病病人评价为没有患病的情况比将未患病用户判定为患病用户的代价要大得多，后者可以通过其他检测来继续验证，而前者则很难；另外当本身训练数据中各样本数量分布极度不均衡的时候，比如#0/#1=9:1，即使是一个分类器将所有样本全部判断为0时，这个分类的accuracy也达到了90%，很显然这里是有问题的。</p><p>假定某样本有100个正样本与200个负样本，confusion table如下：</p><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic4.zhimg.com/v2-cb8c94e9509e9e005d8ac629f1809a03_b.jpg" data-caption="" data-rawwidth="894" data-rawheight="148" class="origin_image zh-lightbox-thumb" width="894" data-original="https://pic4.zhimg.com/v2-cb8c94e9509e9e005d8ac629f1809a03_r.jpg"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-cb8c94e9509e9e005d8ac629f1809a03_hd.jpg" data-caption="" data-rawwidth="894" data-rawheight="148" class="origin_image zh-lightbox-thumb lazy" width="894" data-original="https://pic4.zhimg.com/v2-cb8c94e9509e9e005d8ac629f1809a03_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-cb8c94e9509e9e005d8ac629f1809a03_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><p>从这张图表中，我们可以很明显的看出在正分类中，我们的分类器有较低的准确率：(80/(20+80)=80%)，负分类中准确率为(195/(195+5)=97.5%)，如果仅仅考虑全局的accuracy，(80+195)/(100+200)=91.7%，丢失了很多信息。</p><h2>Per-Class Accuracy</h2><p>在上面例子中，对每类的accuracy做一个平均：(80%+97.5%)/2=88.75%，和之前的准确率相差较大，尤其是在分布极度不均的正负样本数量时，9+1-判断为10+,accuracy为90%，(100%+0)/2=50%</p><h2>Log-Loss</h2><p>在Logisitic Regression分类器中，最终的分类是指定阈值，然后对predict的值来进行判断进行分类，假定指定阈值0.5，model计算得到属于class 1的概率为0.51，这里有一个错误，但是这里有余概率与分类阈值相差很少，Log-Loss就是一个将此类因素考虑的标准：</p><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic1.zhimg.com/v2-e025588625fb004bc25c3039f2254ee8_b.jpg" data-caption="" data-rawwidth="704" data-rawheight="52" class="origin_image zh-lightbox-thumb" width="704" data-original="https://pic1.zhimg.com/v2-e025588625fb004bc25c3039f2254ee8_r.jpg"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-e025588625fb004bc25c3039f2254ee8_hd.jpg" data-caption="" data-rawwidth="704" data-rawheight="52" class="origin_image zh-lightbox-thumb lazy" width="704" data-original="https://pic1.zhimg.com/v2-e025588625fb004bc25c3039f2254ee8_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-e025588625fb004bc25c3039f2254ee8_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><p>pi是属于ith class的概率，yi是第ith的真实label，如果数据功底较强的人可能一眼就可以看出，这里其实就是y和p分布的Cross-Entropy，即真实label与预测的y的分布之间的差异。最小化Log-Loss即为最大化分类器的性能。</p><h2>AUC</h2><p>AUC即Area Under the Curve，这里的Curve就是ROC曲线，ROC的横坐标为Flase positive rate，纵坐标为Ture Positive Rate，用分类器的FP和TP来衡量分类器的性能好坏。而这里ROC是一个曲线而非一个值，AUC就是将该ROC用一个数值表示，这个数值就是曲线之下的面积。</p><p class="ztext-empty-paragraph"><br></p><h2><b>2.2-Ranking Metrics</b></h2><p>Ranking Metrics和前面的分类的merics，有很多相似的地方，例如，用户给定一个query，然后搜索引擎会反馈一个item list， 这个item list会按照与用户query的相关性来进行排序，其本质就是一个0/1的二元分类器，其中score是分类为1的概率，以此为标准来进行相关性的判定。当然Ranking Metrics很多时候也使用Regression的Metrics，例如在个性化推荐系统中，会通过各种数据的feature来进行一个score的计算，并以此为标准对推荐结果进行排序。</p><p>这里，我们首先介绍下Precision-Recall，也就是在分类中经常使用的来作为Ranking Metrics</p><h2>Precision Recall</h2><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic2.zhimg.com/v2-58da2d74cf160b96f49ad6574dcfe171_b.jpg" data-caption="" data-rawwidth="766" data-rawheight="496" class="origin_image zh-lightbox-thumb" width="766" data-original="https://pic2.zhimg.com/v2-58da2d74cf160b96f49ad6574dcfe171_r.jpg"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-58da2d74cf160b96f49ad6574dcfe171_hd.jpg" data-caption="" data-rawwidth="766" data-rawheight="496" class="origin_image zh-lightbox-thumb lazy" width="766" data-original="https://pic2.zhimg.com/v2-58da2d74cf160b96f49ad6574dcfe171_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-58da2d74cf160b96f49ad6574dcfe171_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic4.zhimg.com/v2-4d31ad9683e05e0e380c4589138ff51b_b.jpg" data-caption="" data-rawwidth="554" data-rawheight="136" class="origin_image zh-lightbox-thumb" width="554" data-original="https://pic4.zhimg.com/v2-4d31ad9683e05e0e380c4589138ff51b_r.jpg"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-4d31ad9683e05e0e380c4589138ff51b_hd.jpg" data-caption="" data-rawwidth="554" data-rawheight="136" class="origin_image zh-lightbox-thumb lazy" width="554" data-original="https://pic4.zhimg.com/v2-4d31ad9683e05e0e380c4589138ff51b_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-4d31ad9683e05e0e380c4589138ff51b_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><p>通常，我们在ranking中只对top K来进行计算，就是所谓的precision@k,recall@k，precision和recall之间的关系有点类似于True Postive 和False Postive之间的关系，单独谈其中一样是没有意义的，通常我们使用F1 score来表明其好坏：</p><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic2.zhimg.com/v2-b42d87f76ff7235dd5f6d022eeda3075_b.jpg" data-caption="" data-rawwidth="318" data-rawheight="70" class="content_image" width="318"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-b42d87f76ff7235dd5f6d022eeda3075_hd.jpg" data-caption="" data-rawwidth="318" data-rawheight="70" class="content_image lazy" width="318" data-actualsrc="https://pic2.zhimg.com/v2-b42d87f76ff7235dd5f6d022eeda3075_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><h2>NDCG</h2><p>NDCG是另一种很有效地排序标准，这里不对其做详细概念说明，只举一个例子就明白了，如想详细了解，请阅读<u><a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Discounted_cumulative_gain" class=" external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043"><span class="invisible">https://</span><span class="visible">en.wikipedia.org/wiki/D</span><span class="invisible">iscounted_cumulative_gain</span><span class="ellipsis"></span></a></u></p><p>假定某一个排序方法，给出的结果为D1,D2,D3,D4,D5,D6,而用户的相关得分（比如通过用户对其点击率来计算）为3,2,3,0,1,2。</p><p>则这个搜索的累积的熵为：</p><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic3.zhimg.com/v2-a990a86c731453186fa8f2155fe6ed4e_b.jpg" data-caption="" data-rawwidth="758" data-rawheight="118" class="origin_image zh-lightbox-thumb" width="758" data-original="https://pic3.zhimg.com/v2-a990a86c731453186fa8f2155fe6ed4e_r.jpg"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-a990a86c731453186fa8f2155fe6ed4e_hd.jpg" data-caption="" data-rawwidth="758" data-rawheight="118" class="origin_image zh-lightbox-thumb lazy" width="758" data-original="https://pic3.zhimg.com/v2-a990a86c731453186fa8f2155fe6ed4e_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-a990a86c731453186fa8f2155fe6ed4e_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><p>明显可知，CG对排序间item的相对位置不敏感，改变item彼此间的位置不影响CG的值，这是不合理的，这里我们添加一个Discounted信息：</p><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic2.zhimg.com/v2-b5918aa19b93041b0f3a49a5fd82ae45_b.jpg" data-caption="" data-rawwidth="350" data-rawheight="434" class="content_image" width="350"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-b5918aa19b93041b0f3a49a5fd82ae45_hd.jpg" data-caption="" data-rawwidth="350" data-rawheight="434" class="content_image lazy" width="350" data-actualsrc="https://pic2.zhimg.com/v2-b5918aa19b93041b0f3a49a5fd82ae45_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><p>这个ranking的DCG计算如下：</p><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic4.zhimg.com/v2-77b442036fde895638d451ba82a52173_b.jpg" data-caption="" data-rawwidth="1188" data-rawheight="132" class="origin_image zh-lightbox-thumb" width="1188" data-original="https://pic4.zhimg.com/v2-77b442036fde895638d451ba82a52173_r.jpg"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-77b442036fde895638d451ba82a52173_hd.jpg" data-caption="" data-rawwidth="1188" data-rawheight="132" class="origin_image zh-lightbox-thumb lazy" width="1188" data-original="https://pic4.zhimg.com/v2-77b442036fde895638d451ba82a52173_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-77b442036fde895638d451ba82a52173_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><p>同理，我们做一个最佳的排序的计算，这里最佳的排序是按照用户相关得分的排序：</p><p>3,3,2,2,1,0</p><p>此时，最佳的DCG = 8.69</p><p>最终的Normalize DCG=8.10/8.69=0.932</p><h2><b>2.3-Regression Metrics</b></h2><p>在回归任务中，我们一般需要去预测数值型的得分，例如我们会预测未来一段时间股票的价格，另外个性化系统预测用户对某个item的得分，类似的这些任务我们都会用到回归方法。</p><h2>RMSE</h2><p>在回归任务中，最普通的评估标准是RMSE（root-mean-square error)：</p><p class="ztext-empty-paragraph"><br></p><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic4.zhimg.com/v2-4b18bec63d9623b4ed5debba2ff8d007_b.jpg" data-caption="" data-rawwidth="288" data-rawheight="96" class="content_image" width="288"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-4b18bec63d9623b4ed5debba2ff8d007_hd.jpg" data-caption="" data-rawwidth="288" data-rawheight="96" class="content_image lazy" width="288" data-actualsrc="https://pic4.zhimg.com/v2-4b18bec63d9623b4ed5debba2ff8d007_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><h2>Quantiles of Errors</h2><p>RMSE有个比较严重的问题，它对large outliers比较敏感，通常一个比较大的离群值会很大地影响最终的RMSE值。Quantiles在某一方面来说，相对于RMSE来说鲁棒性比较高。</p><p>Median Absolute Percentage一般能够有效地减少离群值的影响：</p><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic1.zhimg.com/v2-b203dbef81f86269aaf545b9b576c89c_b.jpg" data-caption="" data-rawwidth="420" data-rawheight="52" class="content_image" width="420"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-b203dbef81f86269aaf545b9b576c89c_hd.jpg" data-caption="" data-rawwidth="420" data-rawheight="52" class="content_image lazy" width="420" data-actualsrc="https://pic1.zhimg.com/v2-b203dbef81f86269aaf545b9b576c89c_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><p>当然，我们也可以使用第&gt;90%的数据来找到数据当中的worst case，或者用&lt;0.1来表示数据当中的best case。</p><p class="ztext-empty-paragraph"><br></p><h2><b>2.4-Cautions</b></h2><h2>Training Metrics 和Evaluation Metrics的差异</h2><p>很多时候，Evaluation Metrics 和Training Metrics可以通用，我们可以直接选定Evaluation Metrics为目标函数来对其优化，例如RMSE，但是也有很多Evaluation Metrics 不能直接作为目标函数来优化。</p><h2>Skewed Datasets：Imbalanced classes，outliers， and Rare Data</h2><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic3.zhimg.com/v2-4b7ebe11879da830fbd9e947573db3a6_b.jpg" data-caption="" data-rawwidth="900" data-rawheight="430" class="origin_image zh-lightbox-thumb" width="900" data-original="https://pic3.zhimg.com/v2-4b7ebe11879da830fbd9e947573db3a6_r.jpg"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-4b7ebe11879da830fbd9e947573db3a6_hd.jpg" data-caption="" data-rawwidth="900" data-rawheight="430" class="origin_image zh-lightbox-thumb lazy" width="900" data-original="https://pic3.zhimg.com/v2-4b7ebe11879da830fbd9e947573db3a6_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-4b7ebe11879da830fbd9e947573db3a6_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><p>如果在datasets中，正负样本数相差很大，比如99/1，这样我们的分类器很容易全1，来达到accuracy达到99%，ROC也很好看，但此时其实算法的泛化能力很差，应该是无效的。</p><h2><b>3-线下评估机制</b></h2><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic4.zhimg.com/v2-ae5c8c9ef3389cc079899fa07d3ed67f_b.jpg" data-caption="" data-rawwidth="888" data-rawheight="554" class="origin_image zh-lightbox-thumb" width="888" data-original="https://pic4.zhimg.com/v2-ae5c8c9ef3389cc079899fa07d3ed67f_r.jpg"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-ae5c8c9ef3389cc079899fa07d3ed67f_hd.jpg" data-caption="" data-rawwidth="888" data-rawheight="554" class="origin_image zh-lightbox-thumb lazy" width="888" data-original="https://pic4.zhimg.com/v2-ae5c8c9ef3389cc079899fa07d3ed67f_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-ae5c8c9ef3389cc079899fa07d3ed67f_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><p>一般，我们将历史数据按某种策略分为训练数据和验证数据，以此我们做Model Training，根据相关评估标准来做Model Selection，选定好model方法之后，利用Validation data去做Hyperparameter tuner，选择出在验证集数据中性能最好的Hyperparameter sets。</p><p>很多时候，获取一个有效地历史训练数据集代价很大，我们通常只能获取到相对于真实数据很小的一部分数据，为了保证model的泛化能力，我们通常会采用很多其他的方法来充分验证，例如Hold-Out Validation,Cross-Validation,Bootstrap and Jackknife，这三种基本思想都相同，其中Hold-Out实现最简单，只是简单地将整个训练集分为训练集和验证集，然后用验证集的数据对训练集生成的model验证model有效性，Cross-Validation是将整个训练数据集划分为k-fold，多次取其中某一个fold做验证数据集，相对于Hold-Out Validation来说，相当于多次操作；前面两种可能大部分人都听说过，而Bootstrap很少有人了解，相对于Cross-Validation,其实质我们可以理解为，每次取K-fold里面的某部分做验证集，这其实是一种不放回的采样，而Bootstrap则恰好相反，它实质是一种由放回的采样原理：每次取其中某些数据做验证数据，然后放回重新选取，为什么要选择放回呢？统计学家们认为训练数据本身就有一种潜在的分布信息，我们称为”经验分布”，每次随机选取，然后不放回能够保证每次的经验分布都为原始的训练数据本身的分布信息，那么如此一来，bootstrap set中有很多数据是重复的（即为我们的经验分布），有个文档<u><a href="https://link.zhihu.com/?target=https%3A//lagunita.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/cv_boot.pdf" class=" external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043"><span class="invisible">https://</span><span class="visible">lagunita.stanford.edu/c</span><span class="invisible">4x/HumanitiesScience/StatLearning/asset/cv_boot.pdf</span><span class="ellipsis"></span></a></u> <u><a href="https://link.zhihu.com/?target=http%3A//www.americanscientist.org/issues/pub/2010/3/the-bootstrap/1" class=" external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043"><span class="invisible">http://www.</span><span class="visible">americanscientist.org/i</span><span class="invisible">ssues/pub/2010/3/the-bootstrap/1</span><span class="ellipsis"></span></a></u>里面有详细的说明。如果想试试具体效果，可以去sklearn里面尝试下：<u><a href="https://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/modules/grid_search.html%23out-of-bag-estimates" class=" external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043"><span class="invisible">http://</span><span class="visible">scikit-learn.org/stable</span><span class="invisible">/modules/grid_search.html#out-of-bag-estimates</span><span class="ellipsis"></span></a></u></p><h2><b>4-Hyperparameter Tuning</b></h2><p>首先，明白下Hyperparameter是个啥，和模型参数有啥区别</p><h2>4.1-Model Parameter vs Hyperparameter</h2><p>这里举个例子：我们有一个线性回归的模型来表示features和target之间关系：</p><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic3.zhimg.com/v2-ac7c74e7818bc4574d7365f6e946333e_b.jpg" data-caption="" data-rawwidth="122" data-rawheight="52" class="content_image" width="122"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-ac7c74e7818bc4574d7365f6e946333e_hd.jpg" data-caption="" data-rawwidth="122" data-rawheight="52" class="content_image lazy" width="122" data-actualsrc="https://pic3.zhimg.com/v2-ac7c74e7818bc4574d7365f6e946333e_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><p>x是特征向量，y为对应的score值，而w则是我们通过训练得到的参数模型，我们所说的训练model其实就是最是采取优化策略来得到w产生最fit数据的预测数据。这里的w使我们常说的Model Parameter而Hyperparameter通常在原始的linear regression中不需要，但是在lasso、ridge 这些里面会增加一些正则化的考虑来惩罚复杂度较高的模型，而这里的惩罚系数就是我们这里提到的Hyperparameter。</p><p>在很多复杂的模型，例如Dt，SVM，GBDT中有很多复杂的Hyperparameter对最终的预测有很重要的影响。</p><h2>4.2-Hyperparameter Tuning Mechanism</h2><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic4.zhimg.com/v2-df3bbe9b78d133463da84c24bc5d2383_b.jpg" data-caption="" data-rawwidth="888" data-rawheight="800" class="origin_image zh-lightbox-thumb" width="888" data-original="https://pic4.zhimg.com/v2-df3bbe9b78d133463da84c24bc5d2383_r.jpg"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-df3bbe9b78d133463da84c24bc5d2383_hd.jpg" data-caption="" data-rawwidth="888" data-rawheight="800" class="origin_image zh-lightbox-thumb lazy" width="888" data-original="https://pic4.zhimg.com/v2-df3bbe9b78d133463da84c24bc5d2383_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-df3bbe9b78d133463da84c24bc5d2383_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><p>如何去选择Hyperparameter，我们提供四种方法：Grid Search，Random Search，Smart Hyperparameter Tuning，Nested Cross-Validation</p><ul><li>Grid Search就是把所有Hyperparameter做组合，然后贪婪去训练模型，选择效果最好的模型和对应的Hyperparameter</li><li>Random Search就是采用随机的策略，和grid search的关系有点类似于随机梯度下降和批梯度下降的关系</li><li>Smart Hyperparameter Tuning：计算下次参数选择，来更快速地收敛到最优参数 </li></ul><p class="ztext-empty-paragraph"><br></p><p>x是特征向量，y为对应的score值，而w则是我们通过训练得到的参数模型，我们所说的训练model其实就是最是采取优化策略来得到w产生最fit数据的预测数据。这里的w使我们常说的Model Parameter而Hyperparameter通常在原始的linear regression中不需要，但是在lasso、ridge 这些里面会增加一些正则化的考虑来惩罚复杂度较高的模型，而这里的惩罚系数就是我们这里提到的Hyperparameter。</p><p>在很多复杂的模型，例如Dt，SVM，GBDT中有很多复杂的Hyperparameter对最终的预测有很重要的影响。</p><h2>4.2-Hyperparameter Tuning Mechanism</h2><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic4.zhimg.com/v2-df3bbe9b78d133463da84c24bc5d2383_b.jpg" data-caption="" data-rawwidth="888" data-rawheight="800" class="origin_image zh-lightbox-thumb" width="888" data-original="https://pic4.zhimg.com/v2-df3bbe9b78d133463da84c24bc5d2383_r.jpg"/></noscript><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-df3bbe9b78d133463da84c24bc5d2383_hd.jpg" data-caption="" data-rawwidth="888" data-rawheight="800" class="origin_image zh-lightbox-thumb lazy" width="888" data-original="https://pic4.zhimg.com/v2-df3bbe9b78d133463da84c24bc5d2383_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-df3bbe9b78d133463da84c24bc5d2383_b.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><p>如何去选择Hyperparameter，我们提供四种方法：Grid Search，Random Search，Smart Hyperparameter Tuning，Nested Cross-Validation</p><ul><li>Grid Search就是把所有Hyperparameter做组合，然后贪婪去训练模型，选择效果最好的模型和对应的Hyperparameter</li><li>Random Search就是采用随机的策略，和grid search的关系有点类似于随机梯度下降和批梯度下降的关系</li><li>Smart Hyperparameter Tuning：计算下次参数选择，来更快速地收敛到最优参数</li></ul></div></div><div class="ContentItem-time">编辑于 2017-11-03</div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><div class="Tag Topic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19559450&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/19559450" target="_blank"><div class="Popover"><div id="Popover3-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover3-content">机器学习</div></div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-fixed is-bottom" style="width: 690px; bottom: 0px; left: 606.5px;"><div class="ContentItem-actions" data-za-detail-view-path-module="BottomBar" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Post&quot;,&quot;id&quot;:&quot;30721429&quot;}}}"><span><button aria-label="赞同 18" type="button" class="Button VoteButton VoteButton--up"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 18</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button BottomActions-CommentBtn Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>1 条评论</button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="Popover4-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover4-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><div class="Post-ActionMenuButton"><div class="Popover"><div id="Popover5-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover5-content"><button type="button" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div><div class="Sticky--holder" style="position: static; top: auto; right: auto; bottom: 0px; left: 0px; display: block; float: none; margin: 0px 0px 10px; height: 54px;"></div></div></article><div class="Post-Sub Post-NormalSub"><div class="PostIndex-Contributions" data-za-detail-view-path-module="ColumnList" data-za-detail-view-path-module_name="文章被以下专栏收录" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null}}"><h3 class="BlockTitle">文章被以下专栏收录</h3><ul><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="https://zhuanlan.zhihu.com/burness-DL"><div class="Popover"><div id="Popover6-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover6-content"><img class="Avatar Avatar--medium Avatar--round" width="40" height="40" src="./【博客存档】机器学习模型评估 - 知乎_files/v2-464976646760cff7f02930d5fa850d56_xs.jpg" srcset="https://pic3.zhimg.com/v2-464976646760cff7f02930d5fa850d56_l.jpg 2x" alt="小石头的码疯窝"></div></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><a class="ColumnLink ColumnItem-Title" href="https://zhuanlan.zhihu.com/burness-DL"><div class="Popover"><div id="Popover7-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover7-content">小石头的码疯窝</div></div></a></h2><div class="ContentItem-meta">尽量硬核干货，不定期学习笔记</div></div><div class="ContentItem-extra"><a href="https://zhuanlan.zhihu.com/burness-DL" type="button" class="Button">进入专栏</a></div></div></div></ul></div><div class="Recommendations-Main" style="width: 1903px;"><h3 class="BlockTitle Recommendations-BlockTitle">推荐阅读</h3><ul class="Recommendations-List"><button class="PagingButton PagingButton-Previous" disabled=""><svg class="Zi Zi--ArrowLeft" fill="#d3d3d3" viewBox="0 0 24 24" width="40" height="40"><path d="M14.782 16.78a.737.737 0 0 1-1.052 0L9.218 12.53a.758.758 0 0 1 0-1.063L13.73 7.22a.737.737 0 0 1 1.052 0c.29.294.29.77.001 1.063L11 12l3.782 3.716c.29.294.29.77 0 1.063z" fill-rule="evenodd"></path></svg></button><a href="https://zhuanlan.zhihu.com/p/53088201" class="PostItem"><div><h1 class="PostItem-Title">机器学习超参数优化算法-Hyperband</h1><p class="PostItem-Summary">参考文献:Hyperband: Bandit-Based Configuration Evaluation for Hyperparameter OptimizationI. 传统优化算法机器学习中模型性能的好坏往往与超参数(如batch size,filter size等)有密切的…</p><div class="PostItem-Footer"><span>marsggbo</span><span class="PostItem-FooterTitle"></span></div></div></a><a href="https://zhuanlan.zhihu.com/p/38328593" class="PostItem"><div><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-5a7add3dd86d0bf4acb10824174eab33_250x0.jpg" srcset="https://pic2.zhimg.com/v2-5a7add3dd86d0bf4acb10824174eab33_qhd.jpg 2x" class="PostItem-TitleImage" alt="机器学习大牛最常用的5个回归损失函数，你知道几个？"><h1 class="PostItem-Title">机器学习大牛最常用的5个回归损失函数，你知道几个？</h1><div class="PostItem-Footer"><span>数据汪</span><span class="PostItem-FooterTitle">发表于数据汪</span></div></div></a><a href="https://zhuanlan.zhihu.com/p/30394026" class="PostItem"><div><h1 class="PostItem-Title">机器学习中正则化项L1和L2的直观理解</h1><p class="PostItem-Summary">正则化（Regularization）机器学习中几乎都可以看到损失函数后面会添加一个额外项，常用的额外项一般有两种，一般英文称作ℓ1-norm和ℓ2-norm，中文称作L1正则化和L2正则化，或者L1范数和L2…</p><div class="PostItem-Footer"><span>lijia...</span><span class="PostItem-FooterTitle">发表于计算机视觉...</span></div></div></a><a href="https://zhuanlan.zhihu.com/p/32156992" class="PostItem"><div><img src="./【博客存档】机器学习模型评估 - 知乎_files/v2-5935be4d938bb12529e0236ae863fa0b_250x0.jpg" srcset="https://pic4.zhimg.com/v2-5935be4d938bb12529e0236ae863fa0b_qhd.jpg 2x" class="PostItem-TitleImage" alt="大话机器学习之GBDT"><h1 class="PostItem-Title">大话机器学习之GBDT</h1><div class="PostItem-Footer"><span>程序</span><span class="PostItem-FooterTitle"></span></div></div></a><button class="PagingButton PagingButton-Next"><svg class="Zi Zi--ArrowRight" fill="#d3d3d3" viewBox="0 0 24 24" width="40" height="40"><path d="M9.218 16.78a.737.737 0 0 0 1.052 0l4.512-4.249a.758.758 0 0 0 0-1.063L10.27 7.22a.737.737 0 0 0-1.052 0 .759.759 0 0 0-.001 1.063L13 12l-3.782 3.716a.758.758 0 0 0 0 1.063z" fill-rule="evenodd"></path></svg></button></ul></div><div class="Comments-container" data-za-detail-view-path-module="CommentList" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null}}"><div class="CommentsV2 CommentsV2--withEditor CommentsV2-withPagination"><div class="Topbar CommentTopbar"><div class="Topbar-title"><h2 class="CommentTopbar-title">1 条评论</h2></div><div class="Topbar-options"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Switch Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M13.004 7V4.232c0-.405.35-.733.781-.733.183 0 .36.06.501.17l6.437 5.033c.331.26.376.722.1 1.033a.803.803 0 0 1-.601.264H2.75a.75.75 0 0 1-.75-.75V7.75A.75.75 0 0 1 2.75 7h10.254zm-1.997 9.999v2.768c0 .405-.35.733-.782.733a.814.814 0 0 1-.5-.17l-6.437-5.034a.702.702 0 0 1-.1-1.032.803.803 0 0 1 .6-.264H21.25a.75.75 0 0 1 .75.75v1.499a.75.75 0 0 1-.75.75H11.007z" fill-rule="evenodd"></path></svg></span>切换为时间排序</button></div></div><div class="CommentsV2-footer CommentEditorV2--normal"><div class="CommentEditorV2-inputWrap"><div class="CommentEditorV2-input Input-wrapper Input-wrapper--spread Input-wrapper--large Input-wrapper--noPadding"><div class="Input Editable"><div class="Dropzone RichText RichText--editable RichText--clearBoth ztext" style="min-height: 198px;"><div class="DraftEditor-root"><div class="public-DraftEditorPlaceholder-root"><div class="public-DraftEditorPlaceholder-inner" id="placeholder-4hug0" style="white-space: pre-wrap;">写下你的评论...</div></div><div class="DraftEditor-editorContainer"><div aria-describedby="placeholder-4hug0" class="notranslate public-DraftEditor-content" contenteditable="true" role="textbox" spellcheck="true" tabindex="0" style="outline: none; user-select: text; white-space: pre-wrap; overflow-wrap: break-word;"><div data-contents="true"><div class="Editable-unstyled" data-block="true" data-editor="4hug0" data-offset-key="2eqjl-0-0"><div data-offset-key="2eqjl-0-0" class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr"><span data-offset-key="2eqjl-0-0"><br data-text="true"></span></div></div></div></div></div></div></div><input multiple="" type="file" accept="image/jpg,image/jpeg,image/png,image/gif" style="display: none;"><div></div></div></div><div class="CommentEditorV2-inputUpload"><div class="CommentEditorV2-popoverWrap"><div class="Popover CommentEditorV2-inputUpLoad-Icon"><button aria-label="插入表情" data-tooltip="插入表情" data-tooltip-position="bottom" data-tooltip-will-hide-on-click="true" id="Popover8-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover8-content" type="button" class="Button Editable-control Button--plain"><svg class="Zi Zi--Emotion" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M7.523 13.5h8.954c-.228 2.47-2.145 4-4.477 4-2.332 0-4.25-1.53-4.477-4zM12 21a9 9 0 1 1 0-18 9 9 0 0 1 0 18zm0-1.5a7.5 7.5 0 1 0 0-15 7.5 7.5 0 0 0 0 15zm-3-8a1.5 1.5 0 1 1 0-3 1.5 1.5 0 0 1 0 3zm6 0a1.5 1.5 0 1 1 0-3 1.5 1.5 0 0 1 0 3z"></path></svg></button></div></div></div></div><button disabled="" type="button" class="Button CommentEditorV2-singleButton Button--primary Button--blue">发布</button></div><div><div class="CommentListV2"><ul class="NestComment"><li class="NestComment--rootCommentNoChild"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover11-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover11-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/fan-fan-39-81-54"><img class="Avatar UserLink-avatar" width="24" height="24" src="./【博客存档】机器学习模型评估 - 知乎_files/v2-2a94f875b5ba0788888a36750cb851e8_s.jpg" srcset="https://pic3.zhimg.com/v2-2a94f875b5ba0788888a36750cb851e8_xs.jpg 2x" alt="饭饭"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/fan-fan-39-81-54">饭饭</a></span><span class="CommentItemV2-time">1 年前</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext"><p>Median Absolute Percentage  能解释详细一些吗？</p></div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="margin-right: 5px;"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>赞</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="margin-right: 5px;"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="transform: rotate(180deg); margin-right: 5px;"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="margin-right: 5px;"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li></ul></div></div></div></div></div></div></main><div class="CornerButtons"><div class="CornerAnimayedFlex"><button data-tooltip="建议反馈" data-tooltip-position="left" data-tooltip-will-hide-on-click="true" aria-label="建议反馈" type="button" class="Button CornerButton Button--plain"><svg class="Zi Zi--Feedback" title="建议反馈" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M19.99 6.99L18 5s-1-1-2-1H8C7 4 6 5 6 5L4 7S3 8 3 9v9s0 2 2.002 2H19c2 0 2-2 2-2V9c0-1-1.01-2.01-1.01-2.01zM16.5 5.5L19 8H5l2.5-2.5h9zm-2 5.5s.5 0 .5.5-.5.5-.5.5h-5s-.5 0-.5-.5.5-.5.5-.5h5z"></path></svg></button></div><div class="CornerAnimayedFlex CornerAnimayedFlex--hidden"><button data-tooltip="回到顶部" data-tooltip-position="left" data-tooltip-will-hide-on-click="true" aria-label="回到顶部" type="button" class="Button CornerButton Button--plain"><svg class="Zi Zi--BackToTop" title="回到顶部" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M16.036 19.59a1 1 0 0 1-.997.995H9.032a.996.996 0 0 1-.997-.996v-7.005H5.03c-1.1 0-1.36-.633-.578-1.416L11.33 4.29a1.003 1.003 0 0 1 1.412 0l6.878 6.88c.782.78.523 1.415-.58 1.415h-3.004v7.005z"></path></svg></button></div></div></div></div><script id="js-clientConfig" type="text/json">{"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","zhuanlanHost":"zhuanlan.zhihu.com","apiHost":"api.zhihu.com"}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"env\u002FgetIpinfo\u002F":false,"article\u002Fget\u002F":false,"brand\u002FgetUrl\u002F":false}},"entities":{"users":{"duan-shi-shi-68":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-0f6e8c23103c91a60ae2d9a9216b835f_{size}.jpg","uid":"30949685329920","userType":"people","isFollowing":false,"urlToken":"duan-shi-shi-68","id":"822e01c87076f4db8bee226402ae39e9","description":"数据挖掘\n推荐\n炼丹\n炼丹炉开发\n“无量”工程师\n业务造航母，工作拧螺丝\n我已加入“维权骑士”(http:\u002F\u002Frightknights.com)的版权保护计划。","name":"想飞的石头","isAdvertiser":false,"headline":"追求极致，拒绝不完美","gender":1,"url":"\u002Fpeople\u002F822e01c87076f4db8bee226402ae39e9","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-0f6e8c23103c91a60ae2d9a9216b835f_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"网易 资深算法工程师"}],"exposedMedal":{"medalId":"972478580856496128","medalName":"专栏作家","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c6395b66f4bf91be3056666c4ae80867_r.png","miniAvatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-2675240add1d7e06c796232ba0f0ba64_is.png","description":"开通专栏并收获 1000 关注"}}},"questions":{},"answers":{},"articles":{"30721429":{"id":30721429,"title":"【博客存档】机器学习模型评估","type":"article","excerptTitle":"","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F30721429","imageUrl":"","titleImage":"","excerpt":"\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-1285da230b6a3e31534ee334dfb5a1ac_200x112.jpg\" data-caption=\"\" data-rawwidth=\"898\" data-rawheight=\"786\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-1285da230b6a3e31534ee334dfb5a1ac_r.jpg\" class=\"origin_image inline-img zh-lightbox-thumb\"\u002F\u003E\u003Cb\u003E前言\u003C\u002Fb\u003E当数据好了之后，你所需的只是调下开源包，然后一个模型就出来了，但是，好与不好？谁来界定？这篇文章，主要针对模型的评估，系统介绍下各种不同的模型的各种评测标准，主要参考Alice Zhang的这篇文章\u003Cu\u003E\u003Ca href=\"http:\u002F\u002Fwww.oreilly.com\u002Fdata\u002Ffree\u002Fevaluating-machine-learning-models.csp\"\u003Ehttp:\u002F\u002Fwww.oreilly.com\u002Fdata\u002Ffree\u002Fevaluating-machine-learning-models.csp\u003C\u002Fa\u003E\u003C\u002Fu\u003E。 \u003Cb\u003E1-基础理解\u003C\u002Fb\u003E Figure1-1是一…","created":1509712800,"updated":1509712824,"author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-0f6e8c23103c91a60ae2d9a9216b835f_{size}.jpg","uid":"30949685329920","userType":"people","isFollowing":false,"urlToken":"duan-shi-shi-68","id":"822e01c87076f4db8bee226402ae39e9","description":"数据挖掘\n推荐\n炼丹\n炼丹炉开发\n“无量”工程师\n业务造航母，工作拧螺丝\n我已加入“维权骑士”(http:\u002F\u002Frightknights.com)的版权保护计划。","name":"想飞的石头","isAdvertiser":false,"headline":"追求极致，拒绝不完美","gender":1,"url":"\u002Fpeople\u002F822e01c87076f4db8bee226402ae39e9","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-0f6e8c23103c91a60ae2d9a9216b835f_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"网易 资深算法工程师"}],"exposedMedal":{"medalId":"972478580856496128","medalName":"专栏作家","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c6395b66f4bf91be3056666c4ae80867_r.png","miniAvatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-2675240add1d7e06c796232ba0f0ba64_is.png","description":"开通专栏并收获 1000 关注"}},"commentPermission":"all","state":"published","imageWidth":0,"imageHeight":0,"content":"\u003Ch2\u003E\u003Cb\u003E前言\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E当数据好了之后，你所需的只是调下开源包，然后一个模型就出来了，但是，好与不好？谁来界定？\u003C\u002Fp\u003E\u003Cp\u003E这篇文章，主要针对模型的评估，系统介绍下各种不同的模型的各种评测标准，主要参考Alice Zhang的这篇文章\u003Cu\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fwww.oreilly.com\u002Fdata\u002Ffree\u002Fevaluating-machine-learning-models.csp\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttp:\u002F\u002Fwww.\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Eoreilly.com\u002Fdata\u002Ffree\u002Fe\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003Evaluating-machine-learning-models.csp\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fu\u003E。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E1-基础理解\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-1285da230b6a3e31534ee334dfb5a1ac_b.jpg\" data-caption=\"\" data-rawwidth=\"898\" data-rawheight=\"786\" class=\"origin_image zh-lightbox-thumb\" width=\"898\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-1285da230b6a3e31534ee334dfb5a1ac_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;898&#39; height=&#39;786&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"898\" data-rawheight=\"786\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"898\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-1285da230b6a3e31534ee334dfb5a1ac_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-1285da230b6a3e31534ee334dfb5a1ac_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003EFigure1-1是一个比较合理的产生机器学习模型的workflow，首先，我们拿到Historical data 然后应用到我们选择的model，然后对数据进行离线评测，离线评测一般我们会从Historical data中，通过一些策略选择出一些数据作为Validation，用来离线评测我们的模型，进行model selection和model params selection；也会引入一些live data来离线评价模型，待选择出合理的model和对应的params后，会对线上数据来一些相关的线上测试，例如本人所在公司会按流量对新旧model来进行A\u002FB testing，利用最终的kpi指标来作为model的评判标准\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E2-模型评估标准\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003EML中，有多重不同考量的model，不同的目标有不同的评估标准，本节主要介绍Classification Metrics、Regression Metrics、Ranking Metrics\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E2.1-Classification Metrics\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Ch2\u003EAccuracy\u003C\u002Fh2\u003E\u003Cp\u003E分类Accuracy就是指在分类方法中，被正确分类的样本数据占所有样本数量的比例。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-976346c986cbef853a287db4f77e9e95_b.jpg\" data-caption=\"\" data-rawwidth=\"398\" data-rawheight=\"66\" class=\"content_image\" width=\"398\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;398&#39; height=&#39;66&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"398\" data-rawheight=\"66\" class=\"content_image lazy\" width=\"398\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-976346c986cbef853a287db4f77e9e95_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch2\u003EConfusion Matrix\u003C\u002Fh2\u003E\u003Cp\u003EAccuracy的计算十分简便，但是类别之间是等价的，很多时候，由于判断为某类的代价不一致，我们不能简单地利用Accuracy来说明某个分类器的好坏。比如一个医生将患病病人评价为没有患病的情况比将未患病用户判定为患病用户的代价要大得多，后者可以通过其他检测来继续验证，而前者则很难；另外当本身训练数据中各样本数量分布极度不均衡的时候，比如#0\u002F#1=9:1，即使是一个分类器将所有样本全部判断为0时，这个分类的accuracy也达到了90%，很显然这里是有问题的。\u003C\u002Fp\u003E\u003Cp\u003E假定某样本有100个正样本与200个负样本，confusion table如下：\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-cb8c94e9509e9e005d8ac629f1809a03_b.jpg\" data-caption=\"\" data-rawwidth=\"894\" data-rawheight=\"148\" class=\"origin_image zh-lightbox-thumb\" width=\"894\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-cb8c94e9509e9e005d8ac629f1809a03_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;894&#39; height=&#39;148&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"894\" data-rawheight=\"148\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"894\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-cb8c94e9509e9e005d8ac629f1809a03_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-cb8c94e9509e9e005d8ac629f1809a03_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E从这张图表中，我们可以很明显的看出在正分类中，我们的分类器有较低的准确率：(80\u002F(20+80)=80%)，负分类中准确率为(195\u002F(195+5)=97.5%)，如果仅仅考虑全局的accuracy，(80+195)\u002F(100+200)=91.7%，丢失了很多信息。\u003C\u002Fp\u003E\u003Ch2\u003EPer-Class Accuracy\u003C\u002Fh2\u003E\u003Cp\u003E在上面例子中，对每类的accuracy做一个平均：(80%+97.5%)\u002F2=88.75%，和之前的准确率相差较大，尤其是在分布极度不均的正负样本数量时，9+1-判断为10+,accuracy为90%，(100%+0)\u002F2=50%\u003C\u002Fp\u003E\u003Ch2\u003ELog-Loss\u003C\u002Fh2\u003E\u003Cp\u003E在Logisitic Regression分类器中，最终的分类是指定阈值，然后对predict的值来进行判断进行分类，假定指定阈值0.5，model计算得到属于class 1的概率为0.51，这里有一个错误，但是这里有余概率与分类阈值相差很少，Log-Loss就是一个将此类因素考虑的标准：\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e025588625fb004bc25c3039f2254ee8_b.jpg\" data-caption=\"\" data-rawwidth=\"704\" data-rawheight=\"52\" class=\"origin_image zh-lightbox-thumb\" width=\"704\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e025588625fb004bc25c3039f2254ee8_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;704&#39; height=&#39;52&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"704\" data-rawheight=\"52\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"704\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e025588625fb004bc25c3039f2254ee8_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e025588625fb004bc25c3039f2254ee8_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003Epi是属于ith class的概率，yi是第ith的真实label，如果数据功底较强的人可能一眼就可以看出，这里其实就是y和p分布的Cross-Entropy，即真实label与预测的y的分布之间的差异。最小化Log-Loss即为最大化分类器的性能。\u003C\u002Fp\u003E\u003Ch2\u003EAUC\u003C\u002Fh2\u003E\u003Cp\u003EAUC即Area Under the Curve，这里的Curve就是ROC曲线，ROC的横坐标为Flase positive rate，纵坐标为Ture Positive Rate，用分类器的FP和TP来衡量分类器的性能好坏。而这里ROC是一个曲线而非一个值，AUC就是将该ROC用一个数值表示，这个数值就是曲线之下的面积。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E2.2-Ranking Metrics\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003ERanking Metrics和前面的分类的merics，有很多相似的地方，例如，用户给定一个query，然后搜索引擎会反馈一个item list， 这个item list会按照与用户query的相关性来进行排序，其本质就是一个0\u002F1的二元分类器，其中score是分类为1的概率，以此为标准来进行相关性的判定。当然Ranking Metrics很多时候也使用Regression的Metrics，例如在个性化推荐系统中，会通过各种数据的feature来进行一个score的计算，并以此为标准对推荐结果进行排序。\u003C\u002Fp\u003E\u003Cp\u003E这里，我们首先介绍下Precision-Recall，也就是在分类中经常使用的来作为Ranking Metrics\u003C\u002Fp\u003E\u003Ch2\u003EPrecision Recall\u003C\u002Fh2\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-58da2d74cf160b96f49ad6574dcfe171_b.jpg\" data-caption=\"\" data-rawwidth=\"766\" data-rawheight=\"496\" class=\"origin_image zh-lightbox-thumb\" width=\"766\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-58da2d74cf160b96f49ad6574dcfe171_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;766&#39; height=&#39;496&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"766\" data-rawheight=\"496\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"766\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-58da2d74cf160b96f49ad6574dcfe171_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-58da2d74cf160b96f49ad6574dcfe171_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4d31ad9683e05e0e380c4589138ff51b_b.jpg\" data-caption=\"\" data-rawwidth=\"554\" data-rawheight=\"136\" class=\"origin_image zh-lightbox-thumb\" width=\"554\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4d31ad9683e05e0e380c4589138ff51b_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;554&#39; height=&#39;136&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"554\" data-rawheight=\"136\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"554\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4d31ad9683e05e0e380c4589138ff51b_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4d31ad9683e05e0e380c4589138ff51b_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E通常，我们在ranking中只对top K来进行计算，就是所谓的precision@k,recall@k，precision和recall之间的关系有点类似于True Postive 和False Postive之间的关系，单独谈其中一样是没有意义的，通常我们使用F1 score来表明其好坏：\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-b42d87f76ff7235dd5f6d022eeda3075_b.jpg\" data-caption=\"\" data-rawwidth=\"318\" data-rawheight=\"70\" class=\"content_image\" width=\"318\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;318&#39; height=&#39;70&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"318\" data-rawheight=\"70\" class=\"content_image lazy\" width=\"318\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-b42d87f76ff7235dd5f6d022eeda3075_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch2\u003ENDCG\u003C\u002Fh2\u003E\u003Cp\u003ENDCG是另一种很有效地排序标准，这里不对其做详细概念说明，只举一个例子就明白了，如想详细了解，请阅读\u003Cu\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FDiscounted_cumulative_gain\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttps:\u002F\u002F\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Een.wikipedia.org\u002Fwiki\u002FD\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003Eiscounted_cumulative_gain\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fu\u003E\u003C\u002Fp\u003E\u003Cp\u003E假定某一个排序方法，给出的结果为D1,D2,D3,D4,D5,D6,而用户的相关得分（比如通过用户对其点击率来计算）为3,2,3,0,1,2。\u003C\u002Fp\u003E\u003Cp\u003E则这个搜索的累积的熵为：\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-a990a86c731453186fa8f2155fe6ed4e_b.jpg\" data-caption=\"\" data-rawwidth=\"758\" data-rawheight=\"118\" class=\"origin_image zh-lightbox-thumb\" width=\"758\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-a990a86c731453186fa8f2155fe6ed4e_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;758&#39; height=&#39;118&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"758\" data-rawheight=\"118\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"758\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-a990a86c731453186fa8f2155fe6ed4e_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-a990a86c731453186fa8f2155fe6ed4e_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E明显可知，CG对排序间item的相对位置不敏感，改变item彼此间的位置不影响CG的值，这是不合理的，这里我们添加一个Discounted信息：\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-b5918aa19b93041b0f3a49a5fd82ae45_b.jpg\" data-caption=\"\" data-rawwidth=\"350\" data-rawheight=\"434\" class=\"content_image\" width=\"350\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;350&#39; height=&#39;434&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"350\" data-rawheight=\"434\" class=\"content_image lazy\" width=\"350\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-b5918aa19b93041b0f3a49a5fd82ae45_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E这个ranking的DCG计算如下：\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-77b442036fde895638d451ba82a52173_b.jpg\" data-caption=\"\" data-rawwidth=\"1188\" data-rawheight=\"132\" class=\"origin_image zh-lightbox-thumb\" width=\"1188\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-77b442036fde895638d451ba82a52173_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1188&#39; height=&#39;132&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"1188\" data-rawheight=\"132\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1188\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-77b442036fde895638d451ba82a52173_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-77b442036fde895638d451ba82a52173_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E同理，我们做一个最佳的排序的计算，这里最佳的排序是按照用户相关得分的排序：\u003C\u002Fp\u003E\u003Cp\u003E3,3,2,2,1,0\u003C\u002Fp\u003E\u003Cp\u003E此时，最佳的DCG = 8.69\u003C\u002Fp\u003E\u003Cp\u003E最终的Normalize DCG=8.10\u002F8.69=0.932\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E2.3-Regression Metrics\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E在回归任务中，我们一般需要去预测数值型的得分，例如我们会预测未来一段时间股票的价格，另外个性化系统预测用户对某个item的得分，类似的这些任务我们都会用到回归方法。\u003C\u002Fp\u003E\u003Ch2\u003ERMSE\u003C\u002Fh2\u003E\u003Cp\u003E在回归任务中，最普通的评估标准是RMSE（root-mean-square error)：\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4b18bec63d9623b4ed5debba2ff8d007_b.jpg\" data-caption=\"\" data-rawwidth=\"288\" data-rawheight=\"96\" class=\"content_image\" width=\"288\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;288&#39; height=&#39;96&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"288\" data-rawheight=\"96\" class=\"content_image lazy\" width=\"288\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4b18bec63d9623b4ed5debba2ff8d007_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch2\u003EQuantiles of Errors\u003C\u002Fh2\u003E\u003Cp\u003ERMSE有个比较严重的问题，它对large outliers比较敏感，通常一个比较大的离群值会很大地影响最终的RMSE值。Quantiles在某一方面来说，相对于RMSE来说鲁棒性比较高。\u003C\u002Fp\u003E\u003Cp\u003EMedian Absolute Percentage一般能够有效地减少离群值的影响：\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-b203dbef81f86269aaf545b9b576c89c_b.jpg\" data-caption=\"\" data-rawwidth=\"420\" data-rawheight=\"52\" class=\"content_image\" width=\"420\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;420&#39; height=&#39;52&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"420\" data-rawheight=\"52\" class=\"content_image lazy\" width=\"420\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-b203dbef81f86269aaf545b9b576c89c_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E当然，我们也可以使用第&gt;90%的数据来找到数据当中的worst case，或者用&lt;0.1来表示数据当中的best case。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E2.4-Cautions\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Ch2\u003ETraining Metrics 和Evaluation Metrics的差异\u003C\u002Fh2\u003E\u003Cp\u003E很多时候，Evaluation Metrics 和Training Metrics可以通用，我们可以直接选定Evaluation Metrics为目标函数来对其优化，例如RMSE，但是也有很多Evaluation Metrics 不能直接作为目标函数来优化。\u003C\u002Fp\u003E\u003Ch2\u003ESkewed Datasets：Imbalanced classes，outliers， and Rare Data\u003C\u002Fh2\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-4b7ebe11879da830fbd9e947573db3a6_b.jpg\" data-caption=\"\" data-rawwidth=\"900\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb\" width=\"900\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-4b7ebe11879da830fbd9e947573db3a6_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;900&#39; height=&#39;430&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"900\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"900\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-4b7ebe11879da830fbd9e947573db3a6_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-4b7ebe11879da830fbd9e947573db3a6_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E如果在datasets中，正负样本数相差很大，比如99\u002F1，这样我们的分类器很容易全1，来达到accuracy达到99%，ROC也很好看，但此时其实算法的泛化能力很差，应该是无效的。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E3-线下评估机制\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ae5c8c9ef3389cc079899fa07d3ed67f_b.jpg\" data-caption=\"\" data-rawwidth=\"888\" data-rawheight=\"554\" class=\"origin_image zh-lightbox-thumb\" width=\"888\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ae5c8c9ef3389cc079899fa07d3ed67f_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;888&#39; height=&#39;554&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"888\" data-rawheight=\"554\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"888\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ae5c8c9ef3389cc079899fa07d3ed67f_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ae5c8c9ef3389cc079899fa07d3ed67f_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E一般，我们将历史数据按某种策略分为训练数据和验证数据，以此我们做Model Training，根据相关评估标准来做Model Selection，选定好model方法之后，利用Validation data去做Hyperparameter tuner，选择出在验证集数据中性能最好的Hyperparameter sets。\u003C\u002Fp\u003E\u003Cp\u003E很多时候，获取一个有效地历史训练数据集代价很大，我们通常只能获取到相对于真实数据很小的一部分数据，为了保证model的泛化能力，我们通常会采用很多其他的方法来充分验证，例如Hold-Out Validation,Cross-Validation,Bootstrap and Jackknife，这三种基本思想都相同，其中Hold-Out实现最简单，只是简单地将整个训练集分为训练集和验证集，然后用验证集的数据对训练集生成的model验证model有效性，Cross-Validation是将整个训练数据集划分为k-fold，多次取其中某一个fold做验证数据集，相对于Hold-Out Validation来说，相当于多次操作；前面两种可能大部分人都听说过，而Bootstrap很少有人了解，相对于Cross-Validation,其实质我们可以理解为，每次取K-fold里面的某部分做验证集，这其实是一种不放回的采样，而Bootstrap则恰好相反，它实质是一种由放回的采样原理：每次取其中某些数据做验证数据，然后放回重新选取，为什么要选择放回呢？统计学家们认为训练数据本身就有一种潜在的分布信息，我们称为”经验分布”，每次随机选取，然后不放回能够保证每次的经验分布都为原始的训练数据本身的分布信息，那么如此一来，bootstrap set中有很多数据是重复的（即为我们的经验分布），有个文档\u003Cu\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Flagunita.stanford.edu\u002Fc4x\u002FHumanitiesScience\u002FStatLearning\u002Fasset\u002Fcv_boot.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttps:\u002F\u002F\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Elagunita.stanford.edu\u002Fc\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003E4x\u002FHumanitiesScience\u002FStatLearning\u002Fasset\u002Fcv_boot.pdf\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fu\u003E \u003Cu\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fwww.americanscientist.org\u002Fissues\u002Fpub\u002F2010\u002F3\u002Fthe-bootstrap\u002F1\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttp:\u002F\u002Fwww.\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Eamericanscientist.org\u002Fi\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003Essues\u002Fpub\u002F2010\u002F3\u002Fthe-bootstrap\u002F1\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fu\u003E里面有详细的说明。如果想试试具体效果，可以去sklearn里面尝试下：\u003Cu\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fscikit-learn.org\u002Fstable\u002Fmodules\u002Fgrid_search.html%23out-of-bag-estimates\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttp:\u002F\u002F\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Escikit-learn.org\u002Fstable\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003E\u002Fmodules\u002Fgrid_search.html#out-of-bag-estimates\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fu\u003E\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E4-Hyperparameter Tuning\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E首先，明白下Hyperparameter是个啥，和模型参数有啥区别\u003C\u002Fp\u003E\u003Ch2\u003E4.1-Model Parameter vs Hyperparameter\u003C\u002Fh2\u003E\u003Cp\u003E这里举个例子：我们有一个线性回归的模型来表示features和target之间关系：\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E前言\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E当数据好了之后，你所需的只是调下开源包，然后一个模型就出来了，但是，好与不好？谁来界定？\u003C\u002Fp\u003E\u003Cp\u003E这篇文章，主要针对模型的评估，系统介绍下各种不同的模型的各种评测标准，主要参考Alice Zhang的这篇文章\u003Cu\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fwww.oreilly.com\u002Fdata\u002Ffree\u002Fevaluating-machine-learning-models.csp\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttp:\u002F\u002Fwww.\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Eoreilly.com\u002Fdata\u002Ffree\u002Fe\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003Evaluating-machine-learning-models.csp\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fu\u003E。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E1-基础理解\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-1285da230b6a3e31534ee334dfb5a1ac_b.jpg\" data-caption=\"\" data-rawwidth=\"898\" data-rawheight=\"786\" class=\"origin_image zh-lightbox-thumb\" width=\"898\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-1285da230b6a3e31534ee334dfb5a1ac_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;898&#39; height=&#39;786&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"898\" data-rawheight=\"786\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"898\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-1285da230b6a3e31534ee334dfb5a1ac_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-1285da230b6a3e31534ee334dfb5a1ac_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003EFigure1-1是一个比较合理的产生机器学习模型的workflow，首先，我们拿到Historical data 然后应用到我们选择的model，然后对数据进行离线评测，离线评测一般我们会从Historical data中，通过一些策略选择出一些数据作为Validation，用来离线评测我们的模型，进行model selection和model params selection；也会引入一些live data来离线评价模型，待选择出合理的model和对应的params后，会对线上数据来一些相关的线上测试，例如本人所在公司会按流量对新旧model来进行A\u002FB testing，利用最终的kpi指标来作为model的评判标准\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E2-模型评估标准\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003EML中，有多重不同考量的model，不同的目标有不同的评估标准，本节主要介绍Classification Metrics、Regression Metrics、Ranking Metrics\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E2.1-Classification Metrics\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Ch2\u003EAccuracy\u003C\u002Fh2\u003E\u003Cp\u003E分类Accuracy就是指在分类方法中，被正确分类的样本数据占所有样本数量的比例。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-976346c986cbef853a287db4f77e9e95_b.jpg\" data-caption=\"\" data-rawwidth=\"398\" data-rawheight=\"66\" class=\"content_image\" width=\"398\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;398&#39; height=&#39;66&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"398\" data-rawheight=\"66\" class=\"content_image lazy\" width=\"398\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-976346c986cbef853a287db4f77e9e95_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch2\u003EConfusion Matrix\u003C\u002Fh2\u003E\u003Cp\u003EAccuracy的计算十分简便，但是类别之间是等价的，很多时候，由于判断为某类的代价不一致，我们不能简单地利用Accuracy来说明某个分类器的好坏。比如一个医生将患病病人评价为没有患病的情况比将未患病用户判定为患病用户的代价要大得多，后者可以通过其他检测来继续验证，而前者则很难；另外当本身训练数据中各样本数量分布极度不均衡的时候，比如#0\u002F#1=9:1，即使是一个分类器将所有样本全部判断为0时，这个分类的accuracy也达到了90%，很显然这里是有问题的。\u003C\u002Fp\u003E\u003Cp\u003E假定某样本有100个正样本与200个负样本，confusion table如下：\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-cb8c94e9509e9e005d8ac629f1809a03_b.jpg\" data-caption=\"\" data-rawwidth=\"894\" data-rawheight=\"148\" class=\"origin_image zh-lightbox-thumb\" width=\"894\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-cb8c94e9509e9e005d8ac629f1809a03_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;894&#39; height=&#39;148&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"894\" data-rawheight=\"148\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"894\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-cb8c94e9509e9e005d8ac629f1809a03_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-cb8c94e9509e9e005d8ac629f1809a03_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E从这张图表中，我们可以很明显的看出在正分类中，我们的分类器有较低的准确率：(80\u002F(20+80)=80%)，负分类中准确率为(195\u002F(195+5)=97.5%)，如果仅仅考虑全局的accuracy，(80+195)\u002F(100+200)=91.7%，丢失了很多信息。\u003C\u002Fp\u003E\u003Ch2\u003EPer-Class Accuracy\u003C\u002Fh2\u003E\u003Cp\u003E在上面例子中，对每类的accuracy做一个平均：(80%+97.5%)\u002F2=88.75%，和之前的准确率相差较大，尤其是在分布极度不均的正负样本数量时，9+1-判断为10+,accuracy为90%，(100%+0)\u002F2=50%\u003C\u002Fp\u003E\u003Ch2\u003ELog-Loss\u003C\u002Fh2\u003E\u003Cp\u003E在Logisitic Regression分类器中，最终的分类是指定阈值，然后对predict的值来进行判断进行分类，假定指定阈值0.5，model计算得到属于class 1的概率为0.51，这里有一个错误，但是这里有余概率与分类阈值相差很少，Log-Loss就是一个将此类因素考虑的标准：\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e025588625fb004bc25c3039f2254ee8_b.jpg\" data-caption=\"\" data-rawwidth=\"704\" data-rawheight=\"52\" class=\"origin_image zh-lightbox-thumb\" width=\"704\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e025588625fb004bc25c3039f2254ee8_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;704&#39; height=&#39;52&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"704\" data-rawheight=\"52\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"704\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e025588625fb004bc25c3039f2254ee8_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e025588625fb004bc25c3039f2254ee8_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003Epi是属于ith class的概率，yi是第ith的真实label，如果数据功底较强的人可能一眼就可以看出，这里其实就是y和p分布的Cross-Entropy，即真实label与预测的y的分布之间的差异。最小化Log-Loss即为最大化分类器的性能。\u003C\u002Fp\u003E\u003Ch2\u003EAUC\u003C\u002Fh2\u003E\u003Cp\u003EAUC即Area Under the Curve，这里的Curve就是ROC曲线，ROC的横坐标为Flase positive rate，纵坐标为Ture Positive Rate，用分类器的FP和TP来衡量分类器的性能好坏。而这里ROC是一个曲线而非一个值，AUC就是将该ROC用一个数值表示，这个数值就是曲线之下的面积。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E2.2-Ranking Metrics\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003ERanking Metrics和前面的分类的merics，有很多相似的地方，例如，用户给定一个query，然后搜索引擎会反馈一个item list， 这个item list会按照与用户query的相关性来进行排序，其本质就是一个0\u002F1的二元分类器，其中score是分类为1的概率，以此为标准来进行相关性的判定。当然Ranking Metrics很多时候也使用Regression的Metrics，例如在个性化推荐系统中，会通过各种数据的feature来进行一个score的计算，并以此为标准对推荐结果进行排序。\u003C\u002Fp\u003E\u003Cp\u003E这里，我们首先介绍下Precision-Recall，也就是在分类中经常使用的来作为Ranking Metrics\u003C\u002Fp\u003E\u003Ch2\u003EPrecision Recall\u003C\u002Fh2\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-58da2d74cf160b96f49ad6574dcfe171_b.jpg\" data-caption=\"\" data-rawwidth=\"766\" data-rawheight=\"496\" class=\"origin_image zh-lightbox-thumb\" width=\"766\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-58da2d74cf160b96f49ad6574dcfe171_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;766&#39; height=&#39;496&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"766\" data-rawheight=\"496\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"766\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-58da2d74cf160b96f49ad6574dcfe171_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-58da2d74cf160b96f49ad6574dcfe171_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4d31ad9683e05e0e380c4589138ff51b_b.jpg\" data-caption=\"\" data-rawwidth=\"554\" data-rawheight=\"136\" class=\"origin_image zh-lightbox-thumb\" width=\"554\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4d31ad9683e05e0e380c4589138ff51b_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;554&#39; height=&#39;136&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"554\" data-rawheight=\"136\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"554\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4d31ad9683e05e0e380c4589138ff51b_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4d31ad9683e05e0e380c4589138ff51b_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E通常，我们在ranking中只对top K来进行计算，就是所谓的precision@k,recall@k，precision和recall之间的关系有点类似于True Postive 和False Postive之间的关系，单独谈其中一样是没有意义的，通常我们使用F1 score来表明其好坏：\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-b42d87f76ff7235dd5f6d022eeda3075_b.jpg\" data-caption=\"\" data-rawwidth=\"318\" data-rawheight=\"70\" class=\"content_image\" width=\"318\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;318&#39; height=&#39;70&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"318\" data-rawheight=\"70\" class=\"content_image lazy\" width=\"318\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-b42d87f76ff7235dd5f6d022eeda3075_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch2\u003ENDCG\u003C\u002Fh2\u003E\u003Cp\u003ENDCG是另一种很有效地排序标准，这里不对其做详细概念说明，只举一个例子就明白了，如想详细了解，请阅读\u003Cu\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FDiscounted_cumulative_gain\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttps:\u002F\u002F\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Een.wikipedia.org\u002Fwiki\u002FD\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003Eiscounted_cumulative_gain\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fu\u003E\u003C\u002Fp\u003E\u003Cp\u003E假定某一个排序方法，给出的结果为D1,D2,D3,D4,D5,D6,而用户的相关得分（比如通过用户对其点击率来计算）为3,2,3,0,1,2。\u003C\u002Fp\u003E\u003Cp\u003E则这个搜索的累积的熵为：\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-a990a86c731453186fa8f2155fe6ed4e_b.jpg\" data-caption=\"\" data-rawwidth=\"758\" data-rawheight=\"118\" class=\"origin_image zh-lightbox-thumb\" width=\"758\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-a990a86c731453186fa8f2155fe6ed4e_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;758&#39; height=&#39;118&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"758\" data-rawheight=\"118\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"758\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-a990a86c731453186fa8f2155fe6ed4e_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-a990a86c731453186fa8f2155fe6ed4e_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E明显可知，CG对排序间item的相对位置不敏感，改变item彼此间的位置不影响CG的值，这是不合理的，这里我们添加一个Discounted信息：\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-b5918aa19b93041b0f3a49a5fd82ae45_b.jpg\" data-caption=\"\" data-rawwidth=\"350\" data-rawheight=\"434\" class=\"content_image\" width=\"350\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;350&#39; height=&#39;434&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"350\" data-rawheight=\"434\" class=\"content_image lazy\" width=\"350\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-b5918aa19b93041b0f3a49a5fd82ae45_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E这个ranking的DCG计算如下：\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-77b442036fde895638d451ba82a52173_b.jpg\" data-caption=\"\" data-rawwidth=\"1188\" data-rawheight=\"132\" class=\"origin_image zh-lightbox-thumb\" width=\"1188\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-77b442036fde895638d451ba82a52173_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1188&#39; height=&#39;132&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"1188\" data-rawheight=\"132\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1188\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-77b442036fde895638d451ba82a52173_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-77b442036fde895638d451ba82a52173_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E同理，我们做一个最佳的排序的计算，这里最佳的排序是按照用户相关得分的排序：\u003C\u002Fp\u003E\u003Cp\u003E3,3,2,2,1,0\u003C\u002Fp\u003E\u003Cp\u003E此时，最佳的DCG = 8.69\u003C\u002Fp\u003E\u003Cp\u003E最终的Normalize DCG=8.10\u002F8.69=0.932\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E2.3-Regression Metrics\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E在回归任务中，我们一般需要去预测数值型的得分，例如我们会预测未来一段时间股票的价格，另外个性化系统预测用户对某个item的得分，类似的这些任务我们都会用到回归方法。\u003C\u002Fp\u003E\u003Ch2\u003ERMSE\u003C\u002Fh2\u003E\u003Cp\u003E在回归任务中，最普通的评估标准是RMSE（root-mean-square error)：\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4b18bec63d9623b4ed5debba2ff8d007_b.jpg\" data-caption=\"\" data-rawwidth=\"288\" data-rawheight=\"96\" class=\"content_image\" width=\"288\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;288&#39; height=&#39;96&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"288\" data-rawheight=\"96\" class=\"content_image lazy\" width=\"288\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4b18bec63d9623b4ed5debba2ff8d007_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch2\u003EQuantiles of Errors\u003C\u002Fh2\u003E\u003Cp\u003ERMSE有个比较严重的问题，它对large outliers比较敏感，通常一个比较大的离群值会很大地影响最终的RMSE值。Quantiles在某一方面来说，相对于RMSE来说鲁棒性比较高。\u003C\u002Fp\u003E\u003Cp\u003EMedian Absolute Percentage一般能够有效地减少离群值的影响：\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-b203dbef81f86269aaf545b9b576c89c_b.jpg\" data-caption=\"\" data-rawwidth=\"420\" data-rawheight=\"52\" class=\"content_image\" width=\"420\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;420&#39; height=&#39;52&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"420\" data-rawheight=\"52\" class=\"content_image lazy\" width=\"420\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-b203dbef81f86269aaf545b9b576c89c_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E当然，我们也可以使用第&gt;90%的数据来找到数据当中的worst case，或者用&lt;0.1来表示数据当中的best case。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E2.4-Cautions\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Ch2\u003ETraining Metrics 和Evaluation Metrics的差异\u003C\u002Fh2\u003E\u003Cp\u003E很多时候，Evaluation Metrics 和Training Metrics可以通用，我们可以直接选定Evaluation Metrics为目标函数来对其优化，例如RMSE，但是也有很多Evaluation Metrics 不能直接作为目标函数来优化。\u003C\u002Fp\u003E\u003Ch2\u003ESkewed Datasets：Imbalanced classes，outliers， and Rare Data\u003C\u002Fh2\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-4b7ebe11879da830fbd9e947573db3a6_b.jpg\" data-caption=\"\" data-rawwidth=\"900\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb\" width=\"900\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-4b7ebe11879da830fbd9e947573db3a6_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;900&#39; height=&#39;430&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"900\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"900\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-4b7ebe11879da830fbd9e947573db3a6_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-4b7ebe11879da830fbd9e947573db3a6_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E如果在datasets中，正负样本数相差很大，比如99\u002F1，这样我们的分类器很容易全1，来达到accuracy达到99%，ROC也很好看，但此时其实算法的泛化能力很差，应该是无效的。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E3-线下评估机制\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ae5c8c9ef3389cc079899fa07d3ed67f_b.jpg\" data-caption=\"\" data-rawwidth=\"888\" data-rawheight=\"554\" class=\"origin_image zh-lightbox-thumb\" width=\"888\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ae5c8c9ef3389cc079899fa07d3ed67f_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;888&#39; height=&#39;554&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"888\" data-rawheight=\"554\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"888\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ae5c8c9ef3389cc079899fa07d3ed67f_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ae5c8c9ef3389cc079899fa07d3ed67f_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E一般，我们将历史数据按某种策略分为训练数据和验证数据，以此我们做Model Training，根据相关评估标准来做Model Selection，选定好model方法之后，利用Validation data去做Hyperparameter tuner，选择出在验证集数据中性能最好的Hyperparameter sets。\u003C\u002Fp\u003E\u003Cp\u003E很多时候，获取一个有效地历史训练数据集代价很大，我们通常只能获取到相对于真实数据很小的一部分数据，为了保证model的泛化能力，我们通常会采用很多其他的方法来充分验证，例如Hold-Out Validation,Cross-Validation,Bootstrap and Jackknife，这三种基本思想都相同，其中Hold-Out实现最简单，只是简单地将整个训练集分为训练集和验证集，然后用验证集的数据对训练集生成的model验证model有效性，Cross-Validation是将整个训练数据集划分为k-fold，多次取其中某一个fold做验证数据集，相对于Hold-Out Validation来说，相当于多次操作；前面两种可能大部分人都听说过，而Bootstrap很少有人了解，相对于Cross-Validation,其实质我们可以理解为，每次取K-fold里面的某部分做验证集，这其实是一种不放回的采样，而Bootstrap则恰好相反，它实质是一种由放回的采样原理：每次取其中某些数据做验证数据，然后放回重新选取，为什么要选择放回呢？统计学家们认为训练数据本身就有一种潜在的分布信息，我们称为”经验分布”，每次随机选取，然后不放回能够保证每次的经验分布都为原始的训练数据本身的分布信息，那么如此一来，bootstrap set中有很多数据是重复的（即为我们的经验分布），有个文档\u003Cu\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Flagunita.stanford.edu\u002Fc4x\u002FHumanitiesScience\u002FStatLearning\u002Fasset\u002Fcv_boot.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttps:\u002F\u002F\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Elagunita.stanford.edu\u002Fc\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003E4x\u002FHumanitiesScience\u002FStatLearning\u002Fasset\u002Fcv_boot.pdf\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fu\u003E \u003Cu\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fwww.americanscientist.org\u002Fissues\u002Fpub\u002F2010\u002F3\u002Fthe-bootstrap\u002F1\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttp:\u002F\u002Fwww.\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Eamericanscientist.org\u002Fi\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003Essues\u002Fpub\u002F2010\u002F3\u002Fthe-bootstrap\u002F1\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fu\u003E里面有详细的说明。如果想试试具体效果，可以去sklearn里面尝试下：\u003Cu\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fscikit-learn.org\u002Fstable\u002Fmodules\u002Fgrid_search.html%23out-of-bag-estimates\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttp:\u002F\u002F\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Escikit-learn.org\u002Fstable\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003E\u002Fmodules\u002Fgrid_search.html#out-of-bag-estimates\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fu\u003E\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E4-Hyperparameter Tuning\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E首先，明白下Hyperparameter是个啥，和模型参数有啥区别\u003C\u002Fp\u003E\u003Ch2\u003E4.1-Model Parameter vs Hyperparameter\u003C\u002Fh2\u003E\u003Cp\u003E这里举个例子：我们有一个线性回归的模型来表示features和target之间关系：\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ac7c74e7818bc4574d7365f6e946333e_b.jpg\" data-caption=\"\" data-rawwidth=\"122\" data-rawheight=\"52\" class=\"content_image\" width=\"122\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;122&#39; height=&#39;52&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"122\" data-rawheight=\"52\" class=\"content_image lazy\" width=\"122\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ac7c74e7818bc4574d7365f6e946333e_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003Ex是特征向量，y为对应的score值，而w则是我们通过训练得到的参数模型，我们所说的训练model其实就是最是采取优化策略来得到w产生最fit数据的预测数据。这里的w使我们常说的Model Parameter而Hyperparameter通常在原始的linear regression中不需要，但是在lasso、ridge 这些里面会增加一些正则化的考虑来惩罚复杂度较高的模型，而这里的惩罚系数就是我们这里提到的Hyperparameter。\u003C\u002Fp\u003E\u003Cp\u003E在很多复杂的模型，例如Dt，SVM，GBDT中有很多复杂的Hyperparameter对最终的预测有很重要的影响。\u003C\u002Fp\u003E\u003Ch2\u003E4.2-Hyperparameter Tuning Mechanism\u003C\u002Fh2\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-df3bbe9b78d133463da84c24bc5d2383_b.jpg\" data-caption=\"\" data-rawwidth=\"888\" data-rawheight=\"800\" class=\"origin_image zh-lightbox-thumb\" width=\"888\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-df3bbe9b78d133463da84c24bc5d2383_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;888&#39; height=&#39;800&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"888\" data-rawheight=\"800\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"888\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-df3bbe9b78d133463da84c24bc5d2383_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-df3bbe9b78d133463da84c24bc5d2383_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E如何去选择Hyperparameter，我们提供四种方法：Grid Search，Random Search，Smart Hyperparameter Tuning，Nested Cross-Validation\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003EGrid Search就是把所有Hyperparameter做组合，然后贪婪去训练模型，选择效果最好的模型和对应的Hyperparameter\u003C\u002Fli\u003E\u003Cli\u003ERandom Search就是采用随机的策略，和grid search的关系有点类似于随机梯度下降和批梯度下降的关系\u003C\u002Fli\u003E\u003Cli\u003ESmart Hyperparameter Tuning：计算下次参数选择，来更快速地收敛到最优参数 \u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003Ex是特征向量，y为对应的score值，而w则是我们通过训练得到的参数模型，我们所说的训练model其实就是最是采取优化策略来得到w产生最fit数据的预测数据。这里的w使我们常说的Model Parameter而Hyperparameter通常在原始的linear regression中不需要，但是在lasso、ridge 这些里面会增加一些正则化的考虑来惩罚复杂度较高的模型，而这里的惩罚系数就是我们这里提到的Hyperparameter。\u003C\u002Fp\u003E\u003Cp\u003E在很多复杂的模型，例如Dt，SVM，GBDT中有很多复杂的Hyperparameter对最终的预测有很重要的影响。\u003C\u002Fp\u003E\u003Ch2\u003E4.2-Hyperparameter Tuning Mechanism\u003C\u002Fh2\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-df3bbe9b78d133463da84c24bc5d2383_b.jpg\" data-caption=\"\" data-rawwidth=\"888\" data-rawheight=\"800\" class=\"origin_image zh-lightbox-thumb\" width=\"888\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-df3bbe9b78d133463da84c24bc5d2383_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;888&#39; height=&#39;800&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-rawwidth=\"888\" data-rawheight=\"800\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"888\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-df3bbe9b78d133463da84c24bc5d2383_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-df3bbe9b78d133463da84c24bc5d2383_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E如何去选择Hyperparameter，我们提供四种方法：Grid Search，Random Search，Smart Hyperparameter Tuning，Nested Cross-Validation\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003EGrid Search就是把所有Hyperparameter做组合，然后贪婪去训练模型，选择效果最好的模型和对应的Hyperparameter\u003C\u002Fli\u003E\u003Cli\u003ERandom Search就是采用随机的策略，和grid search的关系有点类似于随机梯度下降和批梯度下降的关系\u003C\u002Fli\u003E\u003Cli\u003ESmart Hyperparameter Tuning：计算下次参数选择，来更快速地收敛到最优参数\u003C\u002Fli\u003E\u003C\u002Ful\u003E","adminClosedComment":false,"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19559450","type":"topic","id":"19559450","name":"机器学习"}],"voteupCount":18,"voting":0,"column":{"description":"尽量硬核干货，不定期学习笔记","canManage":false,"intro":"也欢迎关注同名公众号小石头的码疯窝，不定期资料分享","isFollowing":false,"urlToken":"burness-DL","id":"burness-DL","articlesCount":63,"acceptSubmission":true,"title":"小石头的码疯窝","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fburness-DL","commentPermission":"all","created":1483449242,"updated":1566007786,"imageUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-464976646760cff7f02930d5fa850d56_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-0f6e8c23103c91a60ae2d9a9216b835f_{size}.jpg","uid":"30949685329920","userType":"people","isFollowing":false,"urlToken":"duan-shi-shi-68","id":"822e01c87076f4db8bee226402ae39e9","description":"数据挖掘\n推荐\n炼丹\n炼丹炉开发\n“无量”工程师\n业务造航母，工作拧螺丝\n我已加入“维权骑士”(http:\u002F\u002Frightknights.com)的版权保护计划。","name":"想飞的石头","isAdvertiser":false,"headline":"追求极致，拒绝不完美","gender":1,"url":"\u002Fpeople\u002F822e01c87076f4db8bee226402ae39e9","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-0f6e8c23103c91a60ae2d9a9216b835f_l.jpg","isOrg":false,"type":"people"},"followers":4637,"type":"column"},"commentCount":1,"contributions":[{"id":903665,"state":"accepted","type":"first_publish","column":{"description":"尽量硬核干货，不定期学习笔记","canManage":false,"intro":"也欢迎关注同名公众号小石头的码疯窝，不定期资料分享","isFollowing":false,"urlToken":"burness-DL","id":"burness-DL","articlesCount":63,"acceptSubmission":true,"title":"小石头的码疯窝","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fburness-DL","commentPermission":"all","created":1483449242,"updated":1566007786,"imageUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-464976646760cff7f02930d5fa850d56_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-0f6e8c23103c91a60ae2d9a9216b835f_{size}.jpg","uid":"30949685329920","userType":"people","isFollowing":false,"urlToken":"duan-shi-shi-68","id":"822e01c87076f4db8bee226402ae39e9","description":"数据挖掘\n推荐\n炼丹\n炼丹炉开发\n“无量”工程师\n业务造航母，工作拧螺丝\n我已加入“维权骑士”(http:\u002F\u002Frightknights.com)的版权保护计划。","name":"想飞的石头","isAdvertiser":false,"headline":"追求极致，拒绝不完美","gender":1,"url":"\u002Fpeople\u002F822e01c87076f4db8bee226402ae39e9","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-0f6e8c23103c91a60ae2d9a9216b835f_l.jpg","isOrg":false,"type":"people"},"followers":4637,"type":"column"}}],"isTitleImageFullScreen":false,"upvotedFollowees":[],"commercialInfo":{"isCommercial":false,"plugin":{}},"suggestEdit":{"status":false,"reason":"","tip":"","url":"","title":""},"reason":"","annotationAction":[],"canTip":false,"tipjarorsCount":0,"isLabeled":false,"hasPublishingDraft":false,"isFavorited":false,"isNormal":true,"status":0,"shareText":"【博客存档】机器学习模型评估 - 来自知乎专栏「小石头的码疯窝」，作者: 想飞的石头 https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F30721429 （想看更多？下载 @知乎 App：http:\u002F\u002Fweibo.com\u002Fp\u002F100404711598 ）","canComment":{"status":true,"reason":""},"trackUrl":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper\u002Fpage_monitor_log?si=__SESSIONID__&ti=__ATOKEN__&at=view&pf=__OS__&ed=__MEMBERID__&idfa=__IDFA__&imei=__IMEI__&androidid=__ANDROIDID__"]}},"columns":{"burness-DL":{"description":"尽量硬核干货，不定期学习笔记","canManage":false,"intro":"也欢迎关注同名公众号小石头的码疯窝，不定期资料分享","isFollowing":false,"urlToken":"burness-DL","id":"burness-DL","articlesCount":63,"acceptSubmission":true,"title":"小石头的码疯窝","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fburness-DL","commentPermission":"all","created":1483449242,"updated":1566007786,"imageUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-464976646760cff7f02930d5fa850d56_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-0f6e8c23103c91a60ae2d9a9216b835f_{size}.jpg","uid":"30949685329920","userType":"people","isFollowing":false,"urlToken":"duan-shi-shi-68","id":"822e01c87076f4db8bee226402ae39e9","description":"数据挖掘\n推荐\n炼丹\n炼丹炉开发\n“无量”工程师\n业务造航母，工作拧螺丝\n我已加入“维权骑士”(http:\u002F\u002Frightknights.com)的版权保护计划。","name":"想飞的石头","isAdvertiser":false,"headline":"追求极致，拒绝不完美","gender":1,"url":"\u002Fpeople\u002F822e01c87076f4db8bee226402ae39e9","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-0f6e8c23103c91a60ae2d9a9216b835f_l.jpg","isOrg":false,"type":"people"},"followers":4637,"type":"column"}},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"clubs":{}},"currentUser":"","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false}},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{}},"env":{"ab":{"config":{"experiments":[{"expId":"launch-us_foltopic_user-10","expPrefix":"us_foltopic_user","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false}],"params":[{"id":"li_back","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_slot_style","type":"String","value":"event_card","chainId":"_all_"},{"id":"se_famous","type":"String","value":"1","chainId":"_all_"},{"id":"tp_topic_head","type":"String","value":"1","chainId":"_all_"},{"id":"top_recall_exp_v1","type":"String","value":"1","chainId":"_all_"},{"id":"soc_bignew","type":"String","value":"1","chainId":"_all_"},{"id":"li_qa_new_cover","type":"String","value":"0","chainId":"_all_"},{"id":"gue_zhuantikapian","type":"String","value":"0"},{"id":"zr_intervene","type":"String","value":"0","chainId":"_all_"},{"id":"zr_video_recall","type":"String","value":"current_recall","chainId":"_all_"},{"id":"se_go_ztext","type":"String","value":"0","chainId":"_all_"},{"id":"ug_zero_follow","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_item_cf","type":"String","value":"close","chainId":"_all_"},{"id":"zr_test_aa1","type":"String","value":"0","chainId":"_all_"},{"id":"ug_follow_answerer","type":"String","value":"0","chainId":"_all_"},{"id":"ls_new_upload","type":"String","value":"0","chainId":"_all_"},{"id":"zr_answer_rec_cp","type":"String","value":"open","chainId":"_all_"},{"id":"se_limit","type":"String","value":"0","chainId":"_all_"},{"id":"se_webtimebox","type":"String","value":"0","chainId":"_all_"},{"id":"se_billboardsearch","type":"String","value":"0","chainId":"_all_"},{"id":"pf_creator_card","type":"String","value":"1","chainId":"_all_"},{"id":"li_android_vip","type":"String","value":"0","chainId":"_all_"},{"id":"se_ios_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"se_ltr_ck","type":"String","value":"0","chainId":"_all_"},{"id":"se_amovietab","type":"String","value":"1","chainId":"_all_"},{"id":"top_universalebook","type":"String","value":"1","chainId":"_all_"},{"id":"se_whitelist","type":"String","value":"0","chainId":"_all_"},{"id":"se_mclick1","type":"String","value":"0","chainId":"_all_"},{"id":"soc_notification","type":"String","value":"0","chainId":"_all_"},{"id":"li_album_liutongab","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_xgb","type":"String","value":"0","chainId":"_all_"},{"id":"zr_art_rec","type":"String","value":"base","chainId":"_all_"},{"id":"se_backsearch","type":"String","value":"0","chainId":"_all_"},{"id":"se_new_topic","type":"String","value":"0","chainId":"_all_"},{"id":"top_recall_exp_v2","type":"String","value":"1","chainId":"_all_"},{"id":"li_tjys_ec_ab","type":"String","value":"0","chainId":"_all_"},{"id":"zw_sameq_sorce","type":"String","value":"999","chainId":"_all_"},{"id":"top_test_4_liguangyi","type":"String","value":"1","chainId":"_all_"},{"id":"web_n_web_msg","type":"String","value":"0"},{"id":"li_pay_banner_type","type":"String","value":"0","chainId":"_all_"},{"id":"gue_new_special_page","type":"String","value":"0"},{"id":"se_colorfultab","type":"String","value":"1","chainId":"_all_"},{"id":"se_wannasearch","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_metacard_top","type":"String","value":"top","chainId":"_all_"},{"id":"pf_noti_entry_num","type":"String","value":"0","chainId":"_all_"},{"id":"zr_ans_rec","type":"String","value":"gbrank","chainId":"_all_"},{"id":"top_v_album","type":"String","value":"1","chainId":"_all_"},{"id":"web_sem_ab","type":"String","value":"0"},{"id":"web_question_invite","type":"String","value":"B"},{"id":"web_answer_list_ad","type":"String","value":"1"},{"id":"se_likebutton","type":"String","value":"0","chainId":"_all_"},{"id":"se_p_slideshow","type":"String","value":"0","chainId":"_all_"},{"id":"soc_update","type":"String","value":"1","chainId":"_all_"},{"id":"li_search_answer","type":"String","value":"0","chainId":"_all_"},{"id":"zr_rel_search","type":"String","value":"base","chainId":"_all_"},{"id":"se_ri","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sft","type":"String","value":"a","chainId":"_all_"},{"id":"top_quality","type":"String","value":"0","chainId":"_all_"},{"id":"top_gr_ab","type":"String","value":"0","chainId":"_all_"},{"id":"web_answerlist_ad","type":"String","value":"0"},{"id":"zr_km_answer","type":"String","value":"open_cvr","chainId":"_all_"},{"id":"zr_man_intervene","type":"String","value":"0","chainId":"_all_"},{"id":"se_site_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"se_zu_recommend","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_toast","type":"String","value":"1","chainId":"_all_"},{"id":"ug_newtag","type":"String","value":"0","chainId":"_all_"},{"id":"web_heifetz_grow_ad","type":"String","value":"1"},{"id":"zr_search_xgb","type":"String","value":"0","chainId":"_all_"},{"id":"tp_meta_card","type":"String","value":"0","chainId":"_all_"},{"id":"tp_m_intro_re_topic","type":"String","value":"1","chainId":"_all_"},{"id":"tp_sft_v2","type":"String","value":"d","chainId":"_all_"},{"id":"top_new_feed","type":"String","value":"5","chainId":"_all_"},{"id":"se_waterfall","type":"String","value":"0","chainId":"_all_"},{"id":"ug_zero_follow_0","type":"String","value":"0","chainId":"_all_"},{"id":"ug_follow_topic_1","type":"String","value":"2","chainId":"_all_"},{"id":"se_preset_tech","type":"String","value":"0","chainId":"_all_"},{"id":"se_websearch","type":"String","value":"3","chainId":"_all_"},{"id":"li_se_album_card","type":"String","value":"0","chainId":"_all_"},{"id":"web_column_auto_invite","type":"String","value":"0"},{"id":"zr_video_rank","type":"String","value":"new_rank","chainId":"_all_"},{"id":"zr_video_rank_nn","type":"String","value":"new_rank","chainId":"_all_"},{"id":"se_rr","type":"String","value":"0","chainId":"_all_"},{"id":"se_webmajorob","type":"String","value":"0","chainId":"_all_"},{"id":"se_lottery","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_metacard","type":"String","value":"1","chainId":"_all_"},{"id":"zr_km_style","type":"String","value":"base","chainId":"_all_"},{"id":"tp_sticky_android","type":"String","value":"0","chainId":"_all_"},{"id":"pf_fuceng","type":"String","value":"1","chainId":"_all_"},{"id":"ls_videoad","type":"String","value":"2","chainId":"_all_"},{"id":"gue_anonymous","type":"String","value":"show"},{"id":"se_col_boost","type":"String","value":"0","chainId":"_all_"},{"id":"se_topicdirect","type":"String","value":"2","chainId":"_all_"},{"id":"pf_foltopic_usernum","type":"String","value":"50","chainId":"_all_"},{"id":"li_qa_cover","type":"String","value":"old","chainId":"_all_"},{"id":"li_hot_score_ab","type":"String","value":"0","chainId":"_all_"},{"id":"se_ltr_gc","type":"String","value":"0","chainId":"_all_"},{"id":"ug_goodcomment_0","type":"String","value":"1","chainId":"_all_"},{"id":"li_ts_sample","type":"String","value":"old","chainId":"_all_"},{"id":"se_dnn_slabel","type":"String","value":"0","chainId":"_all_"},{"id":"se_college","type":"String","value":"default","chainId":"_all_"},{"id":"se_featured","type":"String","value":"1","chainId":"_all_"},{"id":"tsp_childbillboard","type":"String","value":"1","chainId":"_all_"},{"id":"tsp_newchild","type":"String","value":"1","chainId":"_all_"},{"id":"web_answer_update","type":"String","value":"0"},{"id":"se_zu_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_hotctr","type":"String","value":"1","chainId":"_all_"},{"id":"se_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"pf_newguide_vertical","type":"String","value":"0","chainId":"_all_"},{"id":"tp_header_style","type":"String","value":"1","chainId":"_all_"},{"id":"top_ebook","type":"String","value":"0","chainId":"_all_"},{"id":"li_price_test","type":"String","value":"1","chainId":"_all_"},{"id":"zr_km_tag","type":"String","value":"open","chainId":"_all_"},{"id":"se_dnn_muli_task","type":"String","value":"0","chainId":"_all_"},{"id":"se_search_feed","type":"String","value":"N","chainId":"_all_"},{"id":"se_mobileweb","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_qa","type":"String","value":"0","chainId":"_all_"},{"id":"top_recall_deep_user","type":"String","value":"1","chainId":"_all_"},{"id":"top_ydyq","type":"String","value":"X","chainId":"_all_"},{"id":"li_se_paid_answer","type":"String","value":"0","chainId":"_all_"},{"id":"se_subtext","type":"String","value":"0","chainId":"_all_"},{"id":"ug_fw_answ_aut_1","type":"String","value":"0","chainId":"_all_"},{"id":"ls_fmp4","type":"String","value":"0","chainId":"_all_"},{"id":"soc_bigone","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_kv","type":"String","value":"0","chainId":"_all_"},{"id":"se_dnn_cpyramid","type":"String","value":"0","chainId":"_all_"},{"id":"se_topiclabel","type":"String","value":"1","chainId":"_all_"},{"id":"se_movietab","type":"String","value":"1","chainId":"_all_"},{"id":"top_rank","type":"String","value":"0","chainId":"_all_"},{"id":"soc_special","type":"String","value":"0","chainId":"_all_"},{"id":"qa_answerlist_ad","type":"String","value":"0","chainId":"_all_"},{"id":"zr_rec_answer_cp","type":"String","value":"close","chainId":"_all_"},{"id":"zr_km_xgb_model","type":"String","value":"new_xgb","chainId":"_all_"},{"id":"se_webrs","type":"String","value":"1","chainId":"_all_"},{"id":"se_ad_index","type":"String","value":"10","chainId":"_all_"},{"id":"ug_follow_answerer_0","type":"String","value":"0","chainId":"_all_"},{"id":"se_expired_ob","type":"String","value":"0","chainId":"_all_"},{"id":"se_ltr_nn","type":"String","value":"1","chainId":"_all_"},{"id":"top_vipconsume","type":"String","value":"1","chainId":"_all_"},{"id":"zr_article_rec_rank","type":"String","value":"close","chainId":"_all_"},{"id":"se_mclick","type":"String","value":"0","chainId":"_all_"},{"id":"se_payconsult","type":"String","value":"0","chainId":"_all_"},{"id":"top_root","type":"String","value":"0","chainId":"_all_"},{"id":"se_auto_syn","type":"String","value":"0","chainId":"_all_"},{"id":"se_college_cm","type":"String","value":"0","chainId":"_all_"},{"id":"se_time_threshold","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_lastread","type":"String","value":"0","chainId":"_all_"},{"id":"zr_infinity_small","type":"String","value":"256","chainId":"_all_"},{"id":"se_agency","type":"String","value":" 0","chainId":"_all_"},{"id":"top_hotcommerce","type":"String","value":"1","chainId":"_all_"},{"id":"tsp_vote","type":"String","value":"1","chainId":"_all_"},{"id":"qa_test","type":"String","value":"0","chainId":"_all_"},{"id":"zr_slot_cold_start","type":"String","value":"default","chainId":"_all_"},{"id":"zr_km_paid_answer","type":"String","value":"0","chainId":"_all_"},{"id":"top_native_answer","type":"String","value":"1","chainId":"_all_"}],"chains":[{"chainId":"_all_"}]},"triggers":{}},"userAgent":{"Edge":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":false,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"isWebView":false,"origin":"Mozilla\u002F5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F76.0.3809.100 Safari\u002F537.36"},"ctx":{"path":"\u002Fp\u002F30721429"},"trafficSource":"production","edition":{"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false},"theme":"light","enableShortcut":true,"referer":"https:\u002F\u002Fai100-2.cupoy.com\u002Fmission\u002FD36","conf":{},"ipInfo":{"cityName":"Taipei City","countryName":"China","regionName":"Taiwan","countryCode":"TW"},"logged":false,"tdkInfo":{}},"me":{"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"active":{"sendDigitsError":null,"activeConfirmSucceeded":null,"activeConfirmError":null},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false,"captchaBase64String":null,"captchaValidationMessage":null,"loginCaptchaExpires":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"currentCreatorUrlToken":null,"homeData":{"recommendQuestions":[]},"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0},"goodatTopics":[]},"customPromotion":{"itemLists":{}},"recommend":{"recommendTimes":{}}},"explore":{"academy":{"tabs":[],"article":{}}},"rights":[],"rightsStatus":{},"levelUpperLimit":10,"account":{"growthLevel":{}},"applyStatus":{}},"recommendation":{"homeRecommendations":[]},"shareTexts":{},"articles":{"voters":{}},"previewPost":{},"favlists":{"relations":{}},"columns":{"voters":{}},"reward":{"answer":{},"article":{},"question":{}},"video":{"data":{},"shareVideoDetail":{},"last":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotListCategories":[],"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]}},"readStatus":{},"column":{},"requestColumn":{"categories":[],"error":null},"articleContribution":{"contributeRequests":[],"deleteContributeIdList":[],"handledContributeIdList":[],"recommendedColumns":[],"pinnedColumns":[],"sentContributeRequestsIdList":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"burness-DL",null]},"columnContribution":{"contributeRequests":[],"autoInviteEnabled":false,"recommendedContributors":[],"contributionInvitation":null},"draftHistory":{"history":{},"drafts":{}},"upload":{},"articleDraft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"updating":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null},"deleteFail":{"fail":false},"recommendTopics":[],"selectedColumn":0},"articleDrafts":{"isDrained":false,"isLoading":false,"items":[]},"columnAutocomplete":{"users":[],"friends":[]}},"fetchHost":"www.zhihu.com","subAppName":"column"}</script><script src="./【博客存档】机器学习模型评估 - 知乎_files/vendor.7842b402f56b92d57f3e.js.下載"></script><script src="./【博客存档】机器学习模型评估 - 知乎_files/column.app.271d003f1086546b5513.js.下載"></script><script></script><div><div style="display: none;">想来知乎工作？请发送邮件到 jobs@zhihu.com</div></div><script src="./【博客存档】机器学习模型评估 - 知乎_files/zap.js.下載"></script><div><div><div class="Editable-languageSuggestions" style="left: -1179px; top: -999px;"><div><div class="Popover"><div class="Editable-languageSuggestionsInput Input-wrapper"><input autocomplete="off" role="combobox" aria-expanded="false" aria-autocomplete="list" aria-activedescendant="AutoComplete10-0" id="Popover9-toggle" aria-haspopup="true" aria-owns="Popover9-content" class="Input" placeholder="选择语言" value=""><div class="Input-after"><svg class="Zi Zi--Select" fill="#afbdcf" viewBox="0 0 24 24" width="24" height="24"><path d="M12 16.183l2.716-2.966a.757.757 0 0 1 1.064.001.738.738 0 0 1 0 1.052l-3.247 3.512a.758.758 0 0 1-1.064 0L8.22 14.27a.738.738 0 0 1 0-1.052.758.758 0 0 1 1.063 0L12 16.183zm0-9.365L9.284 9.782a.758.758 0 0 1-1.064 0 .738.738 0 0 1 0-1.052l3.248-3.512a.758.758 0 0 1 1.065 0L15.78 8.73a.738.738 0 0 1 0 1.052.757.757 0 0 1-1.063.001L12 6.818z" fill-rule="evenodd"></path></svg></div></div></div></div></div></div></div></body></html>