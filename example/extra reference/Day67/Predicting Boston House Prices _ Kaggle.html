<!DOCTYPE html>
<!-- saved from url=(0066)https://www.kaggle.com/sagarnildass/predicting-boston-house-prices -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><style id="react-tooltip">.__react_component_tooltip{border-radius:3px;display:inline-block;font-size:13px;left:-999em;opacity:0;padding:8px 21px;position:fixed;pointer-events:none;transition:opacity 0.3s ease-out;top:-999em;visibility:hidden;z-index:999}.__react_component_tooltip.allow_hover,.__react_component_tooltip.allow_click{pointer-events:auto}.__react_component_tooltip:before,.__react_component_tooltip:after{content:"";width:0;height:0;position:absolute}.__react_component_tooltip.show{opacity:0.9;margin-top:0px;margin-left:0px;visibility:visible}.__react_component_tooltip.type-dark{color:#fff;background-color:#222}.__react_component_tooltip.type-dark.place-top:after{border-top-color:#222;border-top-style:solid;border-top-width:6px}.__react_component_tooltip.type-dark.place-bottom:after{border-bottom-color:#222;border-bottom-style:solid;border-bottom-width:6px}.__react_component_tooltip.type-dark.place-left:after{border-left-color:#222;border-left-style:solid;border-left-width:6px}.__react_component_tooltip.type-dark.place-right:after{border-right-color:#222;border-right-style:solid;border-right-width:6px}.__react_component_tooltip.type-dark.border{border:1px solid #fff}.__react_component_tooltip.type-dark.border.place-top:before{border-top:8px solid #fff}.__react_component_tooltip.type-dark.border.place-bottom:before{border-bottom:8px solid #fff}.__react_component_tooltip.type-dark.border.place-left:before{border-left:8px solid #fff}.__react_component_tooltip.type-dark.border.place-right:before{border-right:8px solid #fff}.__react_component_tooltip.type-success{color:#fff;background-color:#8DC572}.__react_component_tooltip.type-success.place-top:after{border-top-color:#8DC572;border-top-style:solid;border-top-width:6px}.__react_component_tooltip.type-success.place-bottom:after{border-bottom-color:#8DC572;border-bottom-style:solid;border-bottom-width:6px}.__react_component_tooltip.type-success.place-left:after{border-left-color:#8DC572;border-left-style:solid;border-left-width:6px}.__react_component_tooltip.type-success.place-right:after{border-right-color:#8DC572;border-right-style:solid;border-right-width:6px}.__react_component_tooltip.type-success.border{border:1px solid #fff}.__react_component_tooltip.type-success.border.place-top:before{border-top:8px solid #fff}.__react_component_tooltip.type-success.border.place-bottom:before{border-bottom:8px solid #fff}.__react_component_tooltip.type-success.border.place-left:before{border-left:8px solid #fff}.__react_component_tooltip.type-success.border.place-right:before{border-right:8px solid #fff}.__react_component_tooltip.type-warning{color:#fff;background-color:#F0AD4E}.__react_component_tooltip.type-warning.place-top:after{border-top-color:#F0AD4E;border-top-style:solid;border-top-width:6px}.__react_component_tooltip.type-warning.place-bottom:after{border-bottom-color:#F0AD4E;border-bottom-style:solid;border-bottom-width:6px}.__react_component_tooltip.type-warning.place-left:after{border-left-color:#F0AD4E;border-left-style:solid;border-left-width:6px}.__react_component_tooltip.type-warning.place-right:after{border-right-color:#F0AD4E;border-right-style:solid;border-right-width:6px}.__react_component_tooltip.type-warning.border{border:1px solid #fff}.__react_component_tooltip.type-warning.border.place-top:before{border-top:8px solid #fff}.__react_component_tooltip.type-warning.border.place-bottom:before{border-bottom:8px solid #fff}.__react_component_tooltip.type-warning.border.place-left:before{border-left:8px solid #fff}.__react_component_tooltip.type-warning.border.place-right:before{border-right:8px solid #fff}.__react_component_tooltip.type-error{color:#fff;background-color:#BE6464}.__react_component_tooltip.type-error.place-top:after{border-top-color:#BE6464;border-top-style:solid;border-top-width:6px}.__react_component_tooltip.type-error.place-bottom:after{border-bottom-color:#BE6464;border-bottom-style:solid;border-bottom-width:6px}.__react_component_tooltip.type-error.place-left:after{border-left-color:#BE6464;border-left-style:solid;border-left-width:6px}.__react_component_tooltip.type-error.place-right:after{border-right-color:#BE6464;border-right-style:solid;border-right-width:6px}.__react_component_tooltip.type-error.border{border:1px solid #fff}.__react_component_tooltip.type-error.border.place-top:before{border-top:8px solid #fff}.__react_component_tooltip.type-error.border.place-bottom:before{border-bottom:8px solid #fff}.__react_component_tooltip.type-error.border.place-left:before{border-left:8px solid #fff}.__react_component_tooltip.type-error.border.place-right:before{border-right:8px solid #fff}.__react_component_tooltip.type-info{color:#fff;background-color:#337AB7}.__react_component_tooltip.type-info.place-top:after{border-top-color:#337AB7;border-top-style:solid;border-top-width:6px}.__react_component_tooltip.type-info.place-bottom:after{border-bottom-color:#337AB7;border-bottom-style:solid;border-bottom-width:6px}.__react_component_tooltip.type-info.place-left:after{border-left-color:#337AB7;border-left-style:solid;border-left-width:6px}.__react_component_tooltip.type-info.place-right:after{border-right-color:#337AB7;border-right-style:solid;border-right-width:6px}.__react_component_tooltip.type-info.border{border:1px solid #fff}.__react_component_tooltip.type-info.border.place-top:before{border-top:8px solid #fff}.__react_component_tooltip.type-info.border.place-bottom:before{border-bottom:8px solid #fff}.__react_component_tooltip.type-info.border.place-left:before{border-left:8px solid #fff}.__react_component_tooltip.type-info.border.place-right:before{border-right:8px solid #fff}.__react_component_tooltip.type-light{color:#222;background-color:#fff}.__react_component_tooltip.type-light.place-top:after{border-top-color:#fff;border-top-style:solid;border-top-width:6px}.__react_component_tooltip.type-light.place-bottom:after{border-bottom-color:#fff;border-bottom-style:solid;border-bottom-width:6px}.__react_component_tooltip.type-light.place-left:after{border-left-color:#fff;border-left-style:solid;border-left-width:6px}.__react_component_tooltip.type-light.place-right:after{border-right-color:#fff;border-right-style:solid;border-right-width:6px}.__react_component_tooltip.type-light.border{border:1px solid #222}.__react_component_tooltip.type-light.border.place-top:before{border-top:8px solid #222}.__react_component_tooltip.type-light.border.place-bottom:before{border-bottom:8px solid #222}.__react_component_tooltip.type-light.border.place-left:before{border-left:8px solid #222}.__react_component_tooltip.type-light.border.place-right:before{border-right:8px solid #222}.__react_component_tooltip.place-top{margin-top:-10px}.__react_component_tooltip.place-top:before{border-left:10px solid transparent;border-right:10px solid transparent;bottom:-8px;left:50%;margin-left:-10px}.__react_component_tooltip.place-top:after{border-left:8px solid transparent;border-right:8px solid transparent;bottom:-6px;left:50%;margin-left:-8px}.__react_component_tooltip.place-bottom{margin-top:10px}.__react_component_tooltip.place-bottom:before{border-left:10px solid transparent;border-right:10px solid transparent;top:-8px;left:50%;margin-left:-10px}.__react_component_tooltip.place-bottom:after{border-left:8px solid transparent;border-right:8px solid transparent;top:-6px;left:50%;margin-left:-8px}.__react_component_tooltip.place-left{margin-left:-10px}.__react_component_tooltip.place-left:before{border-top:6px solid transparent;border-bottom:6px solid transparent;right:-8px;top:50%;margin-top:-5px}.__react_component_tooltip.place-left:after{border-top:5px solid transparent;border-bottom:5px solid transparent;right:-6px;top:50%;margin-top:-4px}.__react_component_tooltip.place-right{margin-left:10px}.__react_component_tooltip.place-right:before{border-top:6px solid transparent;border-bottom:6px solid transparent;left:-8px;top:50%;margin-top:-5px}.__react_component_tooltip.place-right:after{border-top:5px solid transparent;border-bottom:5px solid transparent;left:-6px;top:50%;margin-top:-4px}.__react_component_tooltip .multi-line{display:block;padding:2px 0px;text-align:center}</style>
    <title>Predicting Boston House Prices | Kaggle</title>
    
    <meta name="robots" content="index, follow">
    <meta name="description" content="Download Open Datasets on 1000s of Projects + Share Projects on One Platform. Explore Popular Topics Like Government, Sports, Medicine, Fintech, Food, More. Flexible Data Ingestion.">
    <meta name="turbolinks-cache-control" content="no-cache">
                <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, minimum-scale=1.0">    <meta name="theme-color" content="#008ABC">
    <script type="text/javascript" async="" src="./Predicting Boston House Prices _ Kaggle_files/koj6gxx6"></script><script type="text/javascript" async="" src="./Predicting Boston House Prices _ Kaggle_files/js"></script><script src="./Predicting Boston House Prices _ Kaggle_files/cb=gapi.loaded_0" async=""></script><script type="text/javascript" async="" src="./Predicting Boston House Prices _ Kaggle_files/analytics.js.下載"></script><script async="" src="./Predicting Boston House Prices _ Kaggle_files/fbevents.js.下載"></script><script type="text/javascript">
        window["initialPageLoadStartTime"] = new Date().getTime();
    </script>
    <link rel="dns-prefetch" href="https://www.google-analytics.com/"><link rel="dns-prefetch" href="https://stats.g.doubleclick.net/"><link rel="dns-prefetch" href="https://js.intercomcdn.com/"><link rel="dns-prefetch" href="https://storage.googleapis.com/">
    <link href="https://www.kaggle.com/static/images/favicon.ico" rel="shortcut icon" type="image/x-icon">
    <link rel="manifest" href="https://www.kaggle.com/static/json/manifest.json">
    <link href="./Predicting Boston House Prices _ Kaggle_files/css" rel="stylesheet" type="text/css">
    <link href="./Predicting Boston House Prices _ Kaggle_files/icon" rel="stylesheet" type="text/css">
        <link rel="canonical" href="https://www.kaggle.com/sagarnildass/predicting-boston-house-prices">                    <link rel="stylesheet" type="text/css" href="./Predicting Boston House Prices _ Kaggle_files/vendor.css">
        <link rel="stylesheet" type="text/css" href="./Predicting Boston House Prices _ Kaggle_files/app.css">
    
    
 
        <script>
        try{(function(a,s,y,n,c,h,i,d,e){d=s.createElement("style");
        d.appendChild(s.createTextNode(""));s.head.appendChild(d);d=d.sheet;
        y=y.map(x => d.insertRule(x + "{ opacity: 0 !important }"));
        h.start=1*new Date;h.end=i=function(){y.forEach(x => x<d.cssRules.length ? d.deleteRule(x) : {})};
        (a[n]=a[n]||[]).hide=h;setTimeout(function(){i();h.end=null},c);h.timeout=c;
        })(window,document,['.site-header-react__nav'],'dataLayer',2000,{'GTM-52LNT9S':true});}catch{}
    </script><style></style>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());
        gtag('config', 'UA-12629138-1', {
            'optimize_id': 'GTM-52LNT9S',
            'displayFeaturesTask': null,
            'send_page_view': false
        });
    </script>
    <script async="" src="./Predicting Boston House Prices _ Kaggle_files/js(1)"></script>

    
<script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
            n.callMethod.apply(n,arguments):n.queue.push(arguments)};
        if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
        n.queue=[];t=b.createElement(e);t.async=!0;
        t.src=v;s=b.getElementsByTagName(e)[0];
        s.parentNode.insertBefore(t,s)}(window,document,'script',
        'https://connect.facebook.net/en_US/fbevents.js');
    fbq("set", "autoConfig", "false", "136809193586742");
    fbq('init', '136809193586742'); 
    fbq('track', 'PageView');
</script>
<noscript>
    <img height="1" width="1" src="https://www.facebook.com/tr?id=136809193586742&ev=PageView&noscript=1"/>
</noscript>

<script>window.intercomSettings = {"app_id":"koj6gxx6","name":"KentHsieh","email":"kent1206@gmail.com","user_hash":"8172293af08a846725f3508819f89e92ff063a38d5f248e9d6b6c730bbf3a446","created_at":1493533395,"last_visit_date_at":1566320556,"status_id":2,"performance_tier":0,"user_name":"insightseeker","display_name":"KentHsieh","is_admin":false,"experiment_group":1,"newsletter_subscriber":true,"competition_mailing_list_subscriber":true,"block_emails":false,"competitions_tier":0,"competitions_tier_attained_at":1493537034,"kernels_tier":0,"kernels_tier_attained_at":1493537034,"discussion_tier":0,"discussion_tier_attained_at":1493537034,"datasets_count":0,"last_new_dataset_visit_at":1493534871,"host_page_visits":0,"in_class_competitions_hosted_count":0,"last_hour_searches_count":0,"api_has_token":false,"api_has_requests":false};</script>        <script>(function () { var w = window; var ic = w.Intercom; if (typeof ic === "function") { ic('reattach_activator'); ic('update', intercomSettings); } else { var d = document; var i = function () { i.c(arguments) }; i.q = []; i.c = function (args) { i.q.push(args) }; w.Intercom = i; function l() { var s = d.createElement('script'); s.type = 'text/javascript'; s.async = true; s.src = 'https://widget.intercom.io/widget/koj6gxx6'; var x = d.getElementsByTagName('script')[0]; x.parentNode.insertBefore(s, x); } if (w.attachEvent) { w.attachEvent('onload', l); } else { w.addEventListener('load', l, false); } } })()</script>
    
    
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@kaggledatasets">
    <meta name="og:url" content="https://kaggle.com/sagarnildass/predicting-boston-house-prices">
    <meta name="og:title" content="Predicting Boston House Prices">
    <meta name="og:description" content="Using data from Boston Housing">
    <meta name="og:image" content="https://storage.googleapis.com/kaggle-avatars/thumbnails/1122229-gp.jpg">


    
    

    
    
    
<script type="text/javascript">
    var Kaggle = Kaggle || {};

    Kaggle.Current = {
        userId: 1050034,
        userProfileUrl: '/insightseeker',
        userDisplayNameEscaped: 'KentHsieh',
        userThumbnailUrl: 'https://storage.googleapis.com/kaggle-avatars/thumbnails/1050034-gp.jpg',
        userEmail: 'kent1206@gmail.com',
        userIsPhoneVerified: true,
        userName: 'insightseeker',
        tier: 'Novice',
        antiForgeryToken: 'CfDJ8LdUzqlsSWBPr4Ce3rb9VL9yu6tM7X7dz4JrC4CPpFlmrG3zdNFffn4kjkmzi2tYrcOK9Kcd5ELyH-7rux49L7gel9Rwnf9QTNL6rYxXT_KGZqn_3aqC-5UBIsgXJaQMcROnih5EutLrT1OUaEP7a4XpNFKgQXZg0Zs4_R0c7eGgb9QgZOL_RUYrty3JL5MRMA',
        isAnonymous: false,
        analyticsToken: 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1NjYzMjE0NTYsIlVzZXJJZCI6MTA1MDAzNH0.IwW6reoift0rdt0RprduoYYpaCCNzLbSw6Rf0roCmIQ',
        analyticsTokenExpiry: 15,
        internetKernelsEnabled: true,
        
        
        
        
        
        
        mdeImageUploader: true,
        
        
        
    }
        Kaggle.Current.log = function(){};
        Kaggle.Current.warn = function(){};

    var decodeUserDisplayName = function () {
        var escapedUserDisplayName = Kaggle.Current.userDisplayNameEscaped || "";
        try {
            var textVersion = new DOMParser().parseFromString(escapedUserDisplayName, "text/html").documentElement.textContent;
            if (textVersion) {
                return textVersion;
            }
        } catch(ex) {}
        return escapedUserDisplayName;
    }
    Kaggle.Current.userDisplayName = decodeUserDisplayName();
</script>

    

<script type="text/javascript">
    var Kaggle = Kaggle || {};
    Kaggle.PageMessages = [];
</script>

    
<script type="text/javascript">
/* <![CDATA[ */
goog_snippet_vars = function() {
    var w = window;
    w.google_conversion_id = 955616553;
    w.google_conversion_label = "QSjvCKDksHMQqZrWxwM";
    w.google_conversion_value = 0.00;
    w.google_conversion_currency = "USD";
    w.google_remarketing_only = false;
    w.google_conversion_language = "en";
    w.google_conversion_format = "3";
    w.google_conversion_color = "ffffff";
}
// DO NOT CHANGE THE CODE BELOW.
goog_report_conversion = function(url) {
    goog_snippet_vars();
    window.google_conversion_format = "3";
    var opt = new Object();
    opt.onload_callback = function() {
        if (typeof(url) != 'undefined') {
            window.location = url;
        }
    }
    var conv_handler = window['google_trackConversion'];
    if (typeof(conv_handler) == 'function') {
        conv_handler(opt);
    }
}
/* ]]> */
</script>
<script type="text/javascript" src="./Predicting Boston House Prices _ Kaggle_files/f.txt">
</script>



        <script>window['useKaggleAnalytics'] = true;</script>

    <script src="./Predicting Boston House Prices _ Kaggle_files/vendor.js.下載" data-turbolinks-track="reload"></script>
    <script src="./Predicting Boston House Prices _ Kaggle_files/app.js.下載" data-turbolinks-track="reload"></script><style data-styled="" data-styled-version="4.3.2" id="kaggle-sc-1" data-turbolinks-permanent="true"></style><style data-styled="" data-styled-version="4.3.2" id="kaggle-sc-2" data-turbolinks-permanent="true"></style><style data-styled="" data-styled-version="4.3.2" id="kaggle-sc-3" data-turbolinks-permanent="true"></style><style data-emotion=""></style>
        <script>
            (function() {
                if ('serviceWorker' in navigator) {
                    navigator.serviceWorker.register("/static/assets/service-worker.js").then(function(reg) {
                        reg.onupdatefound = function() {
                            var installingWorker = reg.installing;
                            installingWorker.onstatechange = function() {
                                switch (installingWorker.state) {
                                case 'installed':
                                    if (navigator.serviceWorker.controller) {
                                        console.log('New or updated content is available.');
                                    } else {
                                        console.log('Content is now available offline!');
                                    }
                                    break;
                                case 'redundant':
                                    console.error('The installing service worker became redundant.');
                                    break;
                                }
                            };
                        };
                    }).catch(function(e) {
                      console.error('Error during service worker registration:', e);
                    });
                }
            })();
        </script>
    <script>
        function handleClientLoad() {
            try {
                gapi.load('client:auth2');
            } catch (e) {
                // In Opera, readystatechange is an unreliable detection of script load, causing
                // this function to be called before gapi exists on the window. The onload callback
                // is still called at the correct time, so the feature works as expected - it's
                // just generating noisy errors.
            }
        }
    </script>
    <script async="" defer="" src="./Predicting Boston House Prices _ Kaggle_files/api.js.下載" onload="this.googleApiOnLoad=function(){};handleClientLoad()" onreadystatechange="if (this.readyState === &#39;complete&#39;) this.googleApiOnLoad()" gapi_processed="true">
    </script>
            <script defer="" src="./Predicting Boston House Prices _ Kaggle_files/stackdriver-errors-concat.min.js.下載"></script>
        <script type="text/javascript">
            window.addEventListener('DOMContentLoaded', function () {
                var errorHandler = new StackdriverErrorReporter();
                errorHandler.start({
                    key: 'AIzaSyDANGXFHtSIVc51MIdGwg4mQFgm3oNrKoo',
                    projectId: 'kaggle-161607',
                    service: 'web-fe',
                    version: '1badb6831f33dee2527b45f186403e6ee8ae957d',
                    context: { user: '1050034' }
                });
            });
        </script>
<script charset="utf-8" src="./Predicting Boston House Prices _ Kaggle_files/4.js.下載"></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style></head>
<body data-turbolinks="true"><div id="MathJax_Message" style="display: none;"></div>
    <main>
        






<div class="site-layout">
        <div class="site-layout__header">
            <div data-component-name="SiteHeaderContainer" style="display: flex; flex-direction: column; flex: 1 0 auto;"><div style="--mdc-theme-on-primary:#fff; --mdc-theme-on-surface:rgba(0, 0, 0, 0.87); --mdc-theme-text-primary-on-background:rgba(0, 0, 0, 0.87); --mdc-theme-text-secondary-on-background:rgba(0, 0, 0, 0.54); --mdc-theme-text-hint-on-background:rgba(0, 0, 0, 0.38); --mdc-theme-text-disabled-on-background:rgba(0, 0, 0, 0.38); --mdc-theme-text-icon-on-background:rgba(0, 0, 0, 0.38); --mdc-theme-primary:#20BEFF; --mdc-theme-error:#F58B8A; --mdc-theme-background:#F8F8F8; --mdc-theme-surface:#F8F8F8; --mdc-theme-primary-bg:#20BEFF; --mdc-theme-secondary-bg:#919294;"><div class="site-header-react"><div class="site-header-react__wrapper-cookies-header"><div class="site-header-react__container"><div class="site-header-react__content"><div class="site-header-react__logo"><a href="https://www.kaggle.com/"><img alt="Kaggle" src="./Predicting Boston House Prices _ Kaggle_files/site-logo.png"></a></div><div class="site-header-react__quick-search"><div class="quick-search undefined quick-search--dark false undefined"><div class="quick-search__search-box-container"><input aria-label="Site search" class="quick-search__search-box " type="text" placeholder="Search"><div class="quick-search__button"><i class="fa fa-search"></i></div></div></div></div><nav class="site-header-react__nav"><ol class="SiteHeader_SiteHeaderNavList-sc-fru52l btMbfT"><li class="site-header-react__nav-item site-header-react__nav-item--first"><a id="site-header-competitions__a" href="https://www.kaggle.com/competitions">Competitions</a></li><li class="site-header-react__nav-item "><a id="site-header-datasets__a" href="https://www.kaggle.com/datasets">Datasets</a></li><li class="site-header-react__nav-item "><a id="site-header-notebooks__a" href="https://www.kaggle.com/kernels">Notebooks</a></li><li class="site-header-react__nav-item "><a id="site-header-discussion__a" href="https://www.kaggle.com/discussion">Discussion</a></li><li class="site-header-react__nav-item "><a id="site-header-courses__a" href="https://www.kaggle.com/learn">Courses</a></li><li class="site-header-react__nav-item"><a class="site-header-react__extra-link" title="More"><svg class="site-header-react__nav-ellipsis" width="18px" height="4px" viewBox="0 0 18 4"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><path d="M16,4 C14.8954305,4 14,3.1045695 14,2 C14,0.8954305 14.8954305,0 16,0 C17.1045695,0 18,0.8954305 18,2 C18,3.1045695 17.1045695,4 16,4 Z M9,4 C7.8954305,4 7,3.1045695 7,2 C7,0.8954305 7.8954305,0 9,0 C10.1045695,0 11,0.8954305 11,2 C11,3.1045695 10.1045695,4 9,4 Z M2,4 C0.8954305,4 0,3.1045695 0,2 C0,0.8954305 0.8954305,0 2,0 C3.1045695,0 4,0.8954305 4,2 C4,3.1045695 3.1045695,4 2,4 Z" fill="#FFFFFF"></path></g></svg></a></li></ol></nav><div class="NotificationContainer_NotificationContainerWrapper-sc-bb4fan kMXxed"><span class="tooltip-container NotificationContainer_NotificationTooltipWrapper-sc-1jdjwer TZodN" data-tooltip="Notifications"><div class="NotificationIcon_NotificationIconWrapper-sc-3rbk7d dBubSX"><div class="NotificationIcon_NotificationIconIcon-sc-wpy2fb bdBygo"><span name="bell" class="fa fa-bell"></span></div></div></span><div></div></div><div class="site-header-react__user"><span class="tooltip-container" data-tooltip="Your profile"><div class="site-header-react__user--logged-in"><ul><li class="site-header-react__user-avatar"><img src="./Predicting Boston House Prices _ Kaggle_files/1050034-gp.jpg"></li></ul></div></span></div></div></div></div></div></div></div><script>var Kaggle=window.Kaggle||{};Kaggle.State=Kaggle.State||[];Kaggle.State.push({});performance && performance.mark && performance.mark("SiteHeaderContainer.componentCouldBootstrap");</script>
        </div>

    <div class="site-layout__main-content">
        

<div data-component-name="KernelViewer" style="display: flex; flex-direction: column; flex: 1 0 auto;"><div style="--mdc-theme-on-primary:#fff; --mdc-theme-on-surface:rgba(0, 0, 0, 0.87); --mdc-theme-text-primary-on-background:rgba(0, 0, 0, 0.87); --mdc-theme-text-secondary-on-background:rgba(0, 0, 0, 0.54); --mdc-theme-text-hint-on-background:rgba(0, 0, 0, 0.38); --mdc-theme-text-disabled-on-background:rgba(0, 0, 0, 0.38); --mdc-theme-text-icon-on-background:rgba(0, 0, 0, 0.38); --mdc-theme-primary:#20BEFF; --mdc-theme-error:#F58B8A; --mdc-theme-background:#F8F8F8; --mdc-theme-surface:#F8F8F8; --mdc-theme-primary-bg:#20BEFF; --mdc-theme-secondary-bg:#919294;"><div><div id="kernel-header-wrapper" class="KernelViewer_HeaderWrapper-sc-ohc1yt sDEXm" style="top: 0px;"><div class="KernelViewerContext_ViewerHeader-sc-rb9dne gsvLRR"><span class="KernelViewerContext_PrimaryInfoWrapper-sc-ia2f3h iJGizJ"><span class="KernelViewerContext_AuthorThumbnailWrapper-sc-16ybk5w gKyUfk"><span class="tooltip-container tooltip-container--left" data-tooltip="Sagarnil Das"><a class="avatar" href="https://www.kaggle.com/sagarnildass" style="width: 37px;"><img class="avatar__thumbnail" src="./Predicting Boston House Prices _ Kaggle_files/1122229-gp.jpg" alt="Sagarnil Das" width="37" height="37" style="border-radius: 3.7px;"><img class="avatar__tier" src="./Predicting Boston House Prices _ Kaggle_files/avatier-contributor@2x.png" alt="contributor tier" width="37" style="margin-top: 1.85px;"></a></span><span class="KernelViewerContext_TruncatedCollaboratorsWrapper-sc-t3tbmb gucpcS"><span><span class="TruncatedCollaborators_CollaboratorThumbnailWrapper-sc-soe92h eyJHdD"></span></span></span></span><span class="KernelViewerContext_TitleBlock-sc-jekzgk iwNYec"><img src="./Predicting Boston House Prices _ Kaggle_files/bronzel@1x.png" alt="bronze medal" class="KernelViewerContext_KernelMedal-sc-1dzga2t kEdeN"><a class="KernelViewerContext_KernelTitle-sc-jsmupn GtGFN">Predicting Boston House Prices</a><br><span class="KernelViewerContext_KernelSubtitle-sc-1joh4f8 hwbMpd"><span class="KernelViewerContext_KernelTypeInfo-sc-1l6fza6 cCZXYa">Python notebook using data from</span><a href="https://www.kaggle.com/schirmerchad/bostonhoustingmlnd" class="KernelViewerContext_DataSourceUrl-sc-1dm3ij9 jFvyqr"> Boston Housing</a><span> · <span>17,336</span> views</span><span> · <span title="Tue Jul 11 2017 23:27:57 GMT+0800 (台北標準時間)">2y ago</span></span><span class="KernelViewerContext_CategoriesWrapper-sc-8yrjj dLfERf"><span></span></span></span></span></span><span class="HeaderOptionsContext_CommunityWrapper-sc-1xsfcpi kzxDmz"><span class="KernelVoteButton_VoteButtonWrapper-sc-2bmt8m dMnHdH"><div class="vote-button__container "><div class="vote-button vote-button--compact vote-button--enabled"><div class="vote-button__button vote-button__button--up vb-upvote"><span class="fa fa-angle-up"></span></div><div class="vote-button__button vote-button__button--up vote-button__vote-count-container"><span class="vote-button__vote-count">24</span></div><div class="vote-button__button-placeholder"></div></div></div></span><span class="fork-button ForkButton_JoinedForkContainer-sc-c3nw8f drKHMd"><a class="copy-and-edit-link ForkButton_ForkButtonContainer-sc-l7qw5r kBdRw" href="https://www.kaggle.com/kernels/fork/307985"><span class="ForkButton_ForkTextContainer-sc-ij05fn cLUeWe"><span class="fa fa-code-fork ForkButton_CodeForkIcon-sc-1q1gn6s cjcKpm"></span> Copy and Edit</span></a><a class="ForkButton_ForkButtonContainer-sc-l7qw5r ForkButton_ForkCountContainer-sc-11dafe6 ibzcfR">77</a></span><span class="HeaderOptionsContext_InlineDropdown-sc-183b1ux eVsYcS"><div class="NewsfeedDropdown_DropdownContainer-sc-15xb820 jnulvx"><div class="NewsfeedDropdown_DropdownIconContainer-sc-xh44ey kkVeZz"><span name="ellipsis-h" class="fa fa-ellipsis-h"></span> </div></div></span></span></div><div class="sc-buGlAa crIovC"></div></div><div class="KernelViewer_ViewerContent-sc-c2nrua fxgEVS"><div class="kernel-viewer"><span class="KernelViewer_NavigationSidebarWrapper-sc-56cp8f jJbHsV"><div class="KernelViewerContext_FullNavigationMenu-sc-1n0pzuj PexAa"><span><div class="VersionsInfoBox_VersionTitle-sc-1a29ina bRHlaX">Version 1</div><div class="VersionsInfoBox_VersionTitle-sc-1a29ina VersionsInfoBox_VersionSubtitle-sc-so54uw hPSNEW"><span class="fa fa-history VersionsInfoBox_RecentCommitsIcon-sc-1gnl07a cXSqyw"><span> <span>1 commit</span></span></span></div></span><div class="navbox__container"><div class="navbox__nav-wrapper"><nav class="navbox__nav"><a class="navbox__nav-item--selected" href="https://www.kaggle.com/sagarnildass/predicting-boston-house-prices/notebook" title="Notebook">Notebook</a><div class="navbox__page"><span class="KernelViewerContext_ContentsMenu-sc-8en2wv igjpFC"><div class="navbox__container"><div class="navbox__nav-wrapper"><nav class="navbox__nav"><a class="navbox__nav-item--selected-with-bar" href="https://www.kaggle.com/sagarnildass/predicting-boston-house-prices#Getting-Started" title="">Getting Started</a><div class="navbox__page"></div><a class="navbox__nav-item" href="https://www.kaggle.com/sagarnildass/predicting-boston-house-prices#Data-Exploration" title="">Data Exploration</a><a class="navbox__nav-item" href="https://www.kaggle.com/sagarnildass/predicting-boston-house-prices#Initial-Visualization" title="">Initial Visualization</a><a class="navbox__nav-item" href="https://www.kaggle.com/sagarnildass/predicting-boston-house-prices#Developing-a-Model" title="">Developing a Model</a><a class="navbox__nav-item" href="https://www.kaggle.com/sagarnildass/predicting-boston-house-prices#Analyzing-Model-Performance" title="">Analyzing Model Performance</a><a class="navbox__nav-item" href="https://www.kaggle.com/sagarnildass/predicting-boston-house-prices#Evaluating-Model-Performance" title="">Evaluating Model Performance</a></nav></div></div></span></div><a class="navbox__nav-item" href="https://www.kaggle.com/sagarnildass/predicting-boston-house-prices/data" title="Data">Data</a><a class="navbox__nav-item" href="https://www.kaggle.com/sagarnildass/predicting-boston-house-prices/comments" title="Comments">Comments</a></nav></div></div></div></span><div class="kernel-viewer__container"><div class="kernel-viewer__pane-container" id="kernel-viewer__pane-container"><div class="KernelViewer_SubmissionWrapper-sc-setng8 jYMQsD"></div><div id="notebook"><div><div class="kernel-notebook-pane" style="display: block;"><div class="content-box"><div><div class="content-box__title-bar"><div class="ContentBox_Title-sc-6fbrxj iCvEyH" style="line-height: 46px;">Notebook</div></div></div><div class="content-box__content-section"><div class="kernel-notebook-pane__container"><iframe id="rendered-kernel-content" src="./Predicting Boston House Prices _ Kaggle_files/__results__.html" scrolling="no" title="Main Kernel Content" style="height: 24641px; display: block;"></iframe><div class="kernel-notebook-pane__loading" style="display: none;"><div class="Spinner_SpinnerContainer-sc-fllvzl cBVAFE"><i class="fa fa-circle-o-notch fa-spin fa-2x"></i></div></div></div></div></div></div><div class="kernel-code-pane__subtitle" id="notebook-pane-license-info">This kernel has been released under the <a href="http://www.apache.org/licenses/LICENSE-2.0">Apache 2.0</a> open source license.</div><div class="upvote-suggestion upvote-suggestion__kernel"><div class="upvote-suggestion__text"><div class="upvote-suggestion__main-text">Did you find this Kernel useful?</div><div class="upvote-suggestion__sub-text">Show your appreciation with an upvote</div></div><div class="upvote-suggestion__button"><div class="vote-button__container "><div class="vote-button vote-button--enabled"><div class="vote-button__button vote-button__button--up vb-upvote"><span class="fa fa-caret-up"></span></div><div class="vote-button__button vote-button__button--up vote-button__vote-count-container"><span class="vote-button__vote-count">24</span></div><div class="vote-button__button-placeholder"></div></div></div></div><div class="upvote-suggestion__voters"><div class="upvote-suggestion__voter upvote-suggestion__voter-top"><span class="tooltip-container" data-tooltip="Albert Anthony D. Gavino" data-tooltip-size="small"><a href="https://www.kaggle.com/bertmanila" class="upvote-suggestion__voter-link"><img src="./Predicting Boston House Prices _ Kaggle_files/149680-fb.jpg" alt="Albert Anthony D. Gavino" class="upvote-suggestion__voter-img"></a></span></div><div class="upvote-suggestion__voter upvote-suggestion__voter-top"><span class="tooltip-container" data-tooltip="Ciao" data-tooltip-size="small"><a href="https://www.kaggle.com/asd956458817" class="upvote-suggestion__voter-link"><img src="./Predicting Boston House Prices _ Kaggle_files/default-thumb.png" alt="Ciao" class="upvote-suggestion__voter-img"></a></span></div><div class="upvote-suggestion__voter upvote-suggestion__voter-top"><span class="tooltip-container" data-tooltip="Brandon Yong" data-tooltip-size="small"><a href="https://www.kaggle.com/brandonyongys" class="upvote-suggestion__voter-link"><img src="./Predicting Boston House Prices _ Kaggle_files/default-thumb.png" alt="Brandon Yong" class="upvote-suggestion__voter-img"></a></span></div><div class="upvote-suggestion__voter upvote-suggestion__voter-more"><span class="tooltip-container" data-tooltip="Karthik Ravi" data-tooltip-size="small"><a href="https://www.kaggle.com/karthik28" class="upvote-suggestion__voter-link"><img src="./Predicting Boston House Prices _ Kaggle_files/983908-kg.jpg" alt="Karthik Ravi" class="upvote-suggestion__voter-img"></a></span></div><div class="upvote-suggestion__voter upvote-suggestion__voter-more"><span class="tooltip-container" data-tooltip="Sagarnil Das" data-tooltip-size="small"><a href="https://www.kaggle.com/sagarnildass" class="upvote-suggestion__voter-link"><img src="./Predicting Boston House Prices _ Kaggle_files/1122229-gp.jpg" alt="Sagarnil Das" class="upvote-suggestion__voter-img"></a></span></div><div class="upvote-suggestion__voter upvote-suggestion__voter-more"><span class="tooltip-container" data-tooltip="Shawn_Chen" data-tooltip-size="small"><a href="https://www.kaggle.com/eric455265" class="upvote-suggestion__voter-link"><img src="./Predicting Boston House Prices _ Kaggle_files/default-thumb.png" alt="Shawn_Chen" class="upvote-suggestion__voter-img"></a></span></div><div class="upvote-suggestion__voter upvote-suggestion__voter-more"><span class="tooltip-container" data-tooltip="Vanessa Ozogu" data-tooltip-size="small"><a href="https://www.kaggle.com/vanessao" class="upvote-suggestion__voter-link"><img src="./Predicting Boston House Prices _ Kaggle_files/1365260-kg.jpg" alt="Vanessa Ozogu" class="upvote-suggestion__voter-img"></a></span></div><div class="upvote-suggestion__voter upvote-suggestion__voter-more"><span class="tooltip-container" data-tooltip="Junaid" data-tooltip-size="small"><a href="https://www.kaggle.com/junaid388" class="upvote-suggestion__voter-link"><img src="./Predicting Boston House Prices _ Kaggle_files/1427255-gp.jpg" alt="Junaid" class="upvote-suggestion__voter-img"></a></span></div><div class="upvote-suggestion__voter upvote-suggestion__voter-more"><span class="tooltip-container" data-tooltip="Harsha Rathi" data-tooltip-size="small"><a href="https://www.kaggle.com/harsharathi10" class="upvote-suggestion__voter-link"><img src="./Predicting Boston House Prices _ Kaggle_files/1649435-gp.jpg" alt="Harsha Rathi" class="upvote-suggestion__voter-img"></a></span></div><div class="upvote-suggestion__voter upvote-suggestion__voter-more"><span class="tooltip-container" data-tooltip="Pranab Sarkar" data-tooltip-size="small"><a href="https://www.kaggle.com/sarkarpranab66" class="upvote-suggestion__voter-link"><img src="./Predicting Boston House Prices _ Kaggle_files/1860361-kg.png" alt="Pranab Sarkar" class="upvote-suggestion__voter-img"></a></span></div><div class="upvote-suggestion__voter upvote-suggestion__voter-other"><span class="tooltip-container" data-tooltip="ODosari" data-tooltip-size="small"><a href="https://www.kaggle.com/odosari" class="upvote-suggestion__voter-link"><img src="./Predicting Boston House Prices _ Kaggle_files/default-thumb.png" alt="ODosari" class="upvote-suggestion__voter-img"></a></span></div><div class="upvote-suggestion__voter upvote-suggestion__voter-other"><span class="tooltip-container" data-tooltip="Venu" data-tooltip-size="small"><a href="https://www.kaggle.com/venupannala" class="upvote-suggestion__voter-link"><img src="./Predicting Boston House Prices _ Kaggle_files/2087405-gp.jpg" alt="Venu" class="upvote-suggestion__voter-img"></a></span></div><div class="upvote-suggestion__voter upvote-suggestion__voter-other"><span class="tooltip-container" data-tooltip="roboblob" data-tooltip-size="small"><a href="https://www.kaggle.com/roboblob" class="upvote-suggestion__voter-link"><img src="./Predicting Boston House Prices _ Kaggle_files/2119905-gr.jpg" alt="roboblob" class="upvote-suggestion__voter-img"></a></span></div><div class="upvote-suggestion__voter upvote-suggestion__voter-other"><span class="tooltip-container" data-tooltip="niraj" data-tooltip-size="small"><a href="https://www.kaggle.com/nirajsinh" class="upvote-suggestion__voter-link"><img src="./Predicting Boston House Prices _ Kaggle_files/default-thumb.png" alt="niraj" class="upvote-suggestion__voter-img"></a></span></div><div class="upvote-suggestion__voter upvote-suggestion__voter-other"><span class="tooltip-container" data-tooltip="murali" data-tooltip-size="small"><a href="https://www.kaggle.com/murali2020" class="upvote-suggestion__voter-link"><img src="./Predicting Boston House Prices _ Kaggle_files/default-thumb.png" alt="murali" class="upvote-suggestion__voter-img"></a></span></div></div></div></div></div><div id="data"><div class="DataExplorer_Container-sc-3uk7qa koHlGt"><div class="content-box"><div><div class="content-box__title-bar"><div class="ContentBox_Title-sc-6fbrxj iCvEyH" style="line-height: 46px;">Data</div><div class="content-box__right-side"><div class="DataExplorerOverview_TitleBarRight-sc-41u5ex fQnYEh"><div class="DataExplorerOverview_TitleBarItem-sc-7z0gmy jyhbDO"><span name="arrows-alt" class="DataExplorerOverview_ModalButton-sc-16rlqid gECrJu DataExplorerPreview_TitleButtonInactive-sc-bzj2yc dDfMPl fa fa-arrows-alt"></span></div></div></div></div></div><div class="content-box__content-section"><div class="DataExplorerOverview_MainContentFlexHeight-sc-mttnn1 lnvRMd"><div class="DataExplorerOverview_Column-sc-1xt9894 DataExplorerOverview_ListColumn-sc-1njtlh9 yQpPz"><div class="DataExplorerList_ExplorerContainer-sc-1a88ctb cTVrfV"><div class="data-explorer-list__header DataExplorerList_Header-sc-kx3nd2 iKHbXp"><div class="DataExplorerList_HeaderTitle-sc-b3zndu eydspt">Data Sources</div><div class="DataExplorerList_HeaderRight-sc-17seut1 gAngP"></div></div><div class="DataExplorerList_Content-sc-t0yrr2 fXPaho"><div class="DataExplorerList_Entry-sc-1uffrzs khXEaG"><div class="DataExplorerList_UnselectedEntry-sc-1lv5br0 DataExplorerList_SelectedEntry-sc-tsuri7 cWTFrR"><div style="width: 16px; min-width: 16px; max-width: 16px;"></div><span name="chevron-down" class="DataExplorerList_CollapsedIcon-sc-1qjwt1i jUrboE fa fa-chevron-down"></span><span name="cube" class="DataExplorerList_TypeIcon-sc-mjsupj kUeJgO fa fa-cube"></span><div class="DataExplorerList_EntryNameToolTip2-sc-kc4oz cMQbPv ToolTip_ToolTipContainer-sc-f0vhmk eHUYTV"><div data-tip="true" data-for="tooltip_0" class="DataExplorerList_EntryName-sc-12cja4h eKEqwz" currentitem="false">Boston Housing</div><div class="__react_component_tooltip place-top type-dark " id="tooltip_0" data-id="tooltip"><div class="ToolTip_ToolTipView-sc-1ci7zcv iiWQyY"><span>Boston Housing</span></div></div></div><div class="DataExplorerList_EntryInfoSpacer-sc-6qxfsx Hmlgy"></div></div><div class="DataExplorerList_EntryChildren-sc-1x9zgty fGsVOx"><div class="DataExplorerList_Entry-sc-1uffrzs khXEaG"><div class="DataExplorerList_UnselectedEntry-sc-1lv5br0 bgskLU"><div style="width: 32px; min-width: 32px; max-width: 32px;"></div><span name="chevron-right" class="DataExplorerList_CollapsedIcon-sc-1qjwt1i DataExplorerList_DisabledCollapsedIcon-sc-1qkqjxo fZbCWh fa fa-chevron-right"></span><span name="table" class="DataExplorerList_TypeIcon-sc-mjsupj kUeJgO fa fa-table"></span><div class="DataExplorerList_EntryNameToolTip2-sc-kc4oz cMQbPv ToolTip_ToolTipContainer-sc-f0vhmk eHUYTV"><div data-tip="true" data-for="tooltip_1" class="DataExplorerList_EntryName-sc-12cja4h eKEqwz" currentitem="false">housing.csv</div><div class="__react_component_tooltip place-top type-dark " id="tooltip_1" data-id="tooltip"><div class="ToolTip_ToolTipView-sc-1ci7zcv iiWQyY"><span>housing.csv</span></div></div></div><div class="ToolTip_ToolTipContainer-sc-f0vhmk eHUYTV"><div data-tip="true" data-for="tooltip_2" class="DataExplorerList_EntryInfo-sc-lll5vk gqOAgi" currentitem="false">489 x 4</div><div class="__react_component_tooltip place-top type-dark " id="tooltip_2" data-id="tooltip"><div class="ToolTip_ToolTipView-sc-1ci7zcv iiWQyY">489 rows x 4 columns</div></div></div></div><a href="https://www.kaggle.com/schirmerchad/bostonhoustingmlnd/downloads/housing.csv/1" class="DataExplorerList_EntryAction-sc-1hkyq01 kJGSCc"><span name="download" class="DataExplorerList_EntryActionIconBackground-sc-aljf47 gfXnRe fa fa-download"></span></a></div></div><a href="https://www.kaggle.com/schirmerchad/bostonhoustingmlnd" target="_blank" class="DataExplorerList_EntryAction-sc-1hkyq01 kJGSCc"><span name="external-link" class="DataExplorerList_EntryActionIconBackground-sc-aljf47 gfXnRe fa fa-external-link"></span></a></div></div></div></div><div class="DataExplorerOverview_Column-sc-1xt9894 DataExplorerOverview_ObjectColumn-sc-v8kz4s ihDqUe"><div class="DataExplorerDescription_Container-sc-rtvgew bNXSrs"><div class="DataExplorerDescription_DatasourceHeader-sc-1jjl64y hYvtkD"><img src="./Predicting Boston House Prices _ Kaggle_files/dataset-thumbnail.jpg" alt="Boston Housing source image" class="DataExplorerDescription_DatasourceImage-sc-124hjw6 faQqwa"><div class="DataExplorerDescription_DatasourceDetails-sc-6z9az5 cgraVO"><div class="DataExplorerDescription_DatasourceName-sc-16246cx dGdqfj"><a href="https://www.kaggle.com/schirmerchad/bostonhoustingmlnd" class="DataExplorerDescription_DatasourceLink-sc-40is1k cdJMFj">Boston Housing</a></div><div class="DataExplorerDescription_DatasourceOverview-sc-64u3xx dthRbg">Concerns housing values in suburbs of Boston</div><div class="DataExplorerDescription_DatasourceLastUpdated-sc-1bnzx7s gFoPLV">Last Updated: <span title="Sun Jun 11 2017 23:07:11 GMT+0800 (台北標準時間)">2 years ago</span> (Version 1)</div></div></div><div class="DataExplorerDescription_Header-sc-9udzgu kagSZQ"><div class="DataExplorerDescription_HeaderTitle-sc-8yzcy8 kIiVNS">About this Dataset</div><div class="DataExplorerDescription_HeaderRight-sc-m2iwyg fyjBEU"></div></div><div class="DataExplorerDescription_Content-sc-yp9anb eysdMp"><div class="markdown-converter__text--rendered data-explorer-overview-description"><h3>Context</h3>

<p>The dataset for this project originates from the UCI Machine Learning Repository. The Boston housing data was collected in 1978 and each of the 506 entries represent aggregated data about 14 features for homes from various suburbs in Boston, Massachusetts.</p>

<h3>Acknowledgements</h3>

<p><a href="https://github.com/udacity/machine-learning">https://github.com/udacity/machine-learning</a></p>

<p><a href="https://archive.ics.uci.edu/ml/datasets/Housing">https://archive.ics.uci.edu/ml/datasets/Housing</a></p></div></div></div></div></div></div></div></div></div><div id="output"></div><div id="comments"><div class="comment-list"><div class="comment-list__header"><span class="comment-list__title">Comments <span class="comment-list__title-count">(0)</span></span><span class="comment-list__spacer"></span><div class="comment-list__sort-pane-button"></div></div><div class="comment-wrapper comment-wrapper--mini-form"><div class="discussion-comment__author"><a class="avatar" href="https://www.kaggle.com/insightseeker" style="width: 40px;"><img class="avatar__thumbnail" src="./Predicting Boston House Prices _ Kaggle_files/1050034-gp.jpg" alt="KentHsieh" width="40" height="40" style="border-radius: 4px;"></a></div><div class="discussion-comment discussion-comment--parent"><div class="discussion-comment__body discussion-comment__body--mini discussion-comment__body--editing"><div class="Comment_PlaceholderInputWrapper-sc-17xtcz1 koqHBC"><textarea placeholder="Click here to comment..." class="Comment_PlaceholderInput-sc-1wlma0j cDsCxI"></textarea></div></div></div></div><div class="comment-list__comment-linking-spacer"></div></div></div></div></div><div class="KernelViewer_EmptyColumn-sc-bggm6s hnmdWt"></div></div></div><span class="KernelViewer_NavigationFooterWrapper-sc-4ovrl2 kcUtKL"><div class="StickyFooter-sc-17zvw1m fWlDXE"><nav class="NavBoxFooter_NavigationBar-sc-fpq4iw cMYHPq"><a href="https://www.kaggle.com/sagarnildass/predicting-boston-house-prices/notebook" title="Notebook" class="NavBoxFooter_NavigationElement-sc-1a4m5yz NavBoxFooter_SelectedNavigationElement-sc-128c1nx cbSEkc"><div><span name="book" size="20" class="fa fa-book fa-stack-20x"></span></div><div class="NavBoxFooter_NavigationText-sc-p8ascv fWncVd">Notebook</div></a><a href="https://www.kaggle.com/sagarnildass/predicting-boston-house-prices/data" title="Data" class="NavBoxFooter_NavigationElement-sc-1a4m5yz hmfEdB"><div><span name="table" size="20" class="fa fa-table fa-stack-20x"></span></div><div class="NavBoxFooter_NavigationText-sc-p8ascv fWncVd">Data</div></a><a href="https://www.kaggle.com/sagarnildass/predicting-boston-house-prices/comments" title="Comments" class="NavBoxFooter_NavigationElement-sc-1a4m5yz hmfEdB"><div><span name="comment" size="20" class="fa fa-comment fa-stack-20x"></span></div><div class="NavBoxFooter_NavigationText-sc-p8ascv fWncVd">Comments</div></a></nav><div class="navbox__page"></div></div></span></div></div></div><script>var Kaggle=window.Kaggle||{};Kaggle.State=Kaggle.State||[];Kaggle.State.push({"kernel":{"id":307985,"title":"Predicting Boston House Prices","forkParent":null,"currentRunId":1332019,"mostRecentRunId":1332019,"url":"/sagarnildass/predicting-boston-house-prices","tags":[],"commentCount":0,"upvoteCount":24,"viewCount":17336,"forkCount":77,"bestPublicScore":null,"author":{"id":1122229,"displayName":"Sagarnil Das","email":null,"editedEmail":null,"editedEmailCode":null,"userName":"sagarnildass","thumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1122229-gp.jpg","profileUrl":"/sagarnildass","registerDate":"0001-01-01T00:00:00Z","lastVisitDate":"0001-01-01T00:00:00Z","statusId":0,"performanceTier":1,"userRoles":null,"userLogins":null,"groupIds":null,"duplicateUsers":null,"hasPhoneVerifications":false,"failedNerdchas":0,"hasPendingNerdcha":false,"deleteRequests":null,"userAttributes":null,"isAdmin":false,"isTvc":false,"isKaggleBot":false,"isAdminOrTvc":false,"isAnonymous":false,"canAct":false,"canBeSeen":false,"thumbnailName":null,"activationCode":"00000000-0000-0000-0000-000000000000","isPhoneVerified":false},"isPrivate":false,"updatedTime":"2017-07-11T15:27:58.24Z","selfLink":"/kernels/307985","pinnedDockerImageVersionId":null,"isLanguageTemplate":false,"medal":"bronze","topicId":null,"readGroupId":null,"writeGroupId":null,"slug":"predicting-boston-house-prices"},"kernelBlob":{"id":7355833,"settings":{"dockerImageVersionId":null,"dataSources":[{"sourceType":"DatasetVersion","sourceId":2485,"databundleVersionId":null,"mountSlug":null}],"sourceType":"notebook","language":"python","isGpuEnabled":false,"isInternetEnabled":false},"source":"{\u0022cells\u0022: [{\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u002283dcb1a7c0f2d6184105bf5fadfc95b845b1bb71\u0022}, \u0022source\u0022: \u0022## Getting Started\\nIn this project, we will evaluate the performance and predictive power of a model that has been trained and tested on data collected from homes in suburbs of Boston, Massachusetts. A model trained on this data that is seen as a *good fit* could then be used to make certain predictions about a home \\u2014 in particular, its monetary value. This model would prove to be invaluable for someone like a real estate agent who could make use of such information on a daily basis.\\n\\nThe dataset for this project originates from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Housing). The Boston housing data was collected in 1978 and each of the 506 entries represent aggregated data about 14 features for homes from various suburbs in Boston, Massachusetts. For the purposes of this project, the following preprocessing steps have been made to the dataset:\\n- 16 data points have an `\u0027MEDV\u0027` value of 50.0. These data points likely contain **missing or censored values** and have been removed.\\n- 1 data point has an `\u0027RM\u0027` value of 8.78. This data point can be considered an **outlier** and has been removed.\\n- The features `\u0027RM\u0027`, `\u0027LSTAT\u0027`, `\u0027PTRATIO\u0027`, and `\u0027MEDV\u0027` are essential. The remaining **non-relevant features** have been excluded.\\n- The feature `\u0027MEDV\u0027` has been **multiplicatively scaled** to account for 35 years of market inflation.\\n\\nRun the code cell below to load the Boston housing dataset, along with a few of the necessary Python libraries required for this project. You will know the dataset loaded successfully if the size of the dataset is reported.\u0022}, {\u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022_uuid\u0022: \u0022e2af66277939d7011a90826443705ca6c4e44b81\u0022, \u0022trusted\u0022: false, \u0022_cell_guid\u0022: \u00225fff6d9f-e77f-4b02-b79f-f1623cfe4c20\u0022}, \u0022outputs\u0022: [], \u0022cell_type\u0022: \u0022code\u0022, \u0022execution_count\u0022: null, \u0022source\u0022: \u0022# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n# For example, here\u0027s several helpful packages to load in \\n\\nimport numpy as np # linear algebra\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\nfrom sklearn.cross_validation import ShuffleSplit\\n\\n# Pretty display for notebooks\\n%matplotlib inline\\n# Input data files are available in the \\\u0022../input/\\\u0022 directory.\\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\\n\\nfrom subprocess import check_output\\nprint(check_output([\\\u0022ls\\\u0022, \\\u0022../input\\\u0022]).decode(\\\u0022utf8\\\u0022))\\n\\n\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u002263d06b86597d9ca010eaa00f6801220dc435a181\u0022}, \u0022source\u0022: \u0022# Load the Boston housing dataset\\ndata = pd.read_csv(\u0027../input/housing.csv\u0027)\\nprices = data[\u0027MEDV\u0027]\\nfeatures = data.drop(\u0027MEDV\u0027, axis = 1)\\n\\ndata.head()\\n\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u00221578947bd37c23afa5fed31e35e97c06fdb4e574\u0022}, \u0022source\u0022: \u0022## Data Exploration\\nIn this first section of this project, we will make a cursory investigation about the Boston housing data and provide our observations. Familiarizing ourself with the data through an explorative process is a fundamental practice to help us better understand and justify our results.\\n\\nSince the main goal of this project is to construct a working model which has the capability of predicting the value of houses, we will need to separate the dataset into **features** and the **target variable**. The **features**, `\u0027RM\u0027`, `\u0027LSTAT\u0027`, and `\u0027PTRATIO\u0027`, give us quantitative information about each data point. The **target variable**, `\u0027MEDV\u0027`, will be the variable we seek to predict. These are stored in `features` and `prices`, respectively.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022e1160a8ba82ab251623ccda98a130660168ac2cf\u0022}, \u0022source\u0022: \u0022### Implementation: Calculate Statistics\\nFor our very first coding implementation, we will calculate descriptive statistics about the Boston housing prices. Since `numpy` has already been imported for us, use this library to perform the necessary calculations. These statistics will be extremely important later on to analyze various prediction results from the constructed model.\\n\\nIn the code cell below, we will need to implement the following:\\n- Calculate the minimum, maximum, mean, median, and standard deviation of `\u0027MEDV\u0027`, which is stored in `prices`.\\n  - Store each calculation in their respective variable.\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u00229d388fff06110d5428097f2c1d3aa0d4784ed748\u0022}, \u0022source\u0022: \u0022# TODO: Minimum price of the data\\nminimum_price = np.mean(prices)\\n\\n# TODO: Maximum price of the data\\nmaximum_price = np.max(prices)\\n\\n# TODO: Mean price of the data\\nmean_price = np.mean(prices)\\n\\n# TODO: Median price of the data\\nmedian_price = np.median(prices)\\n\\n# TODO: Standard deviation of prices of the data\\nstd_price = np.std(prices)\\n\\n# Show the calculated statistics\\nprint(\\\u0022Statistics for Boston housing dataset:\\\\n\\\u0022)\\nprint(\\\u0022Minimum price: ${:,.2f}\\\u0022.format(minimum_price))\\nprint(\\\u0022Maximum price: ${:,.2f}\\\u0022.format(maximum_price))\\nprint(\\\u0022Mean price: ${:,.2f}\\\u0022.format(mean_price))\\nprint(\\\u0022Median price ${:,.2f}\\\u0022.format(median_price))\\nprint(\\\u0022Standard deviation of prices: ${:,.2f}\\\u0022.format(std_price))\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022e3d80e14179ca40fc011a7fe157e039360d4565f\u0022}, \u0022source\u0022: \u0022### Question 1 - Feature Observation\\nAs a reminder, we are using three features from the Boston housing dataset: `\u0027RM\u0027`, `\u0027LSTAT\u0027`, and `\u0027PTRATIO\u0027`. For each data point (neighborhood):\\n- `\u0027RM\u0027` is the average number of rooms among homes in the neighborhood.\\n- `\u0027LSTAT\u0027` is the percentage of homeowners in the neighborhood considered \\\u0022lower class\\\u0022 (working poor).\\n- `\u0027PTRATIO\u0027` is the ratio of students to teachers in primary and secondary schools in the neighborhood.\\n\\n\\n** Using your intuition, for each of the three features above, do you think that an increase in the value of that feature would lead to an **increase** in the value of `\u0027MEDV\u0027` or a **decrease** in the value of `\u0027MEDV\u0027`? Justify your answer for each.**\\n\\n**Hint:** This problem can phrased using examples like below.  \\n* Would you expect a home that has an `\u0027RM\u0027` value(number of rooms) of 6 be worth more or less than a home that has an `\u0027RM\u0027` value of 7?\\n* Would you expect a neighborhood that has an `\u0027LSTAT\u0027` value(percent of lower class workers) of 15 have home prices be worth more or less than a neighborhood that has an `\u0027LSTAT\u0027` value of 20?\\n* Would you expect a neighborhood that has an `\u0027PTRATIO\u0027` value(ratio of students to teachers) of 10 have home prices be worth more or less than a neighborhood that has an `\u0027PTRATIO\u0027` value of 15?\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u00222aa5e98163f393861e8e0c2901a2aca2623ff295\u0022}, \u0022source\u0022: \u0022**Answer: ** In my opinion, the value of \u0027MEDV\u0027 will be dependent on these 3 features in the following way:\\n\\n1) **RM** - The more the value of RM, the more will be the value of \u0027MEDV\u0027. Because it\u0027s pretty evident that with increase in the number of rooms, the price of the house will increase.\\n\\n2) **LSTAT** - The more the value of LSTAT, the less will be the value of \u0027MEDV\u0027. Because with increase in the percentage of \\\u0022lower class\\\u0022 homeowners in the neighbourhood, the crime rate in the neighbourhood may increase. Even though LSTAT doesn\u0027t have a causal effect on the crime rate in the neighbourhood, they are likely to be positively correlated. One more factor is if there are greater percentages of \\\u0022lower class\\\u0022 homeowners in the neighbourhood, then more likely very expensive real estate owners will not build their housing complexes in that region as most of the people will not be able to afford it. So in average, the houses in that region will be cheaper.\\n\\n3) **PTRATIO** - The lesser the value of PTRATIO, the more will be the value of \u0027MEDV\u0027. Because if the students to teacher ratio is low, then that means individual students gets much more attention from the students as opposed to a region where this ratio is high. Over there, as the number of students will be much higher than the number of teachers, teachers will not be able to attend to students individually everytime and hence this may affect the education of the students. So regions with a low PTRATIO will have higher prices for houses.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022c189a186d89871eda1087ce6c49b079daab3681c\u0022}, \u0022source\u0022: \u0022## Initial Visualization\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u002254dba09fe89619bd0f948c93a621627e46ec5443\u0022}, \u0022source\u0022: \u0022# Using pyplot\\nimport matplotlib.pyplot as plt\\nplt.figure(figsize=(20, 5))\\n\\n# i: index\\nfor i, col in enumerate(features.columns):\\n    # 3 plots here hence 1, 3\\n    plt.subplot(1, 3, i+1)\\n    x = data[col]\\n    y = prices\\n    plt.plot(x, y, \u0027o\u0027)\\n    # Create regression line\\n    plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\\n    plt.title(col)\\n    plt.xlabel(col)\\n    plt.ylabel(\u0027prices\u0027)\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u002213df265ec830bfe1179613c298b54f588e64d953\u0022}, \u0022source\u0022: \u0022----\\n\\n## Developing a Model\\nIn this second section of the project, we will develop the tools and techniques necessary for a model to make a prediction. Being able to make accurate evaluations of each model\u0027s performance through the use of these tools and techniques helps to greatly reinforce the confidence in our predictions.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u002207dbfdb1055a7927dece3449ea6feb8053069eb1\u0022}, \u0022source\u0022: \u0022### Implementation: Define a Performance Metric\\nIt is difficult to measure the quality of a given model without quantifying its performance over training and testing. This is typically done using some type of performance metric, whether it is through calculating some type of error, the goodness of fit, or some other useful measurement. For this project, we will be calculating the [*coefficient of determination*](http://stattrek.com/statistics/dictionary.aspx?definition=coefficient_of_determination), R\u003csup\u003e2\u003c/sup\u003e, to quantify our model\u0027s performance. The coefficient of determination for a model is a useful statistic in regression analysis, as it often describes how \\\u0022good\\\u0022 that model is at making predictions. \\n\\nThe values for R\u003csup\u003e2\u003c/sup\u003e range from 0 to 1, which captures the percentage of squared correlation between the predicted and actual values of the **target variable**. A model with an R\u003csup\u003e2\u003c/sup\u003e of 0 is no better than a model that always predicts the *mean* of the target variable, whereas a model with an R\u003csup\u003e2\u003c/sup\u003e of 1 perfectly predicts the target variable. Any value between 0 and 1 indicates what percentage of the target variable, using this model, can be explained by the **features**. _A model can be given a negative R\u003csup\u003e2\u003c/sup\u003e as well, which indicates that the model is **arbitrarily worse** than one that always predicts the mean of the target variable._\\n\\nFor the `performance_metric` function in the code cell below, we will need to implement the following:\\n- Use `r2_score` from `sklearn.metrics` to perform a performance calculation between `y_true` and `y_predict`.\\n- Assign the performance score to the `score` variable.\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022d268b4e29e08e739f91e0189da76e0cde2081635\u0022}, \u0022source\u0022: \u0022# TODO: Import \u0027r2_score\u0027\\n\\ndef performance_metric(y_true, y_predict):\\n    \\\u0022\\\u0022\\\u0022 Calculates and returns the performance score between \\n        true and predicted values based on the metric chosen. \\\u0022\\\u0022\\\u0022\\n    \\n    # TODO: Calculate the performance score between \u0027y_true\u0027 and \u0027y_predict\u0027\\n    from sklearn.metrics import r2_score\\n    score = r2_score(y_true, y_predict)\\n    \\n    # Return the score\\n    return score\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022b3bd35934d4d70cb869e857a2d53eeafe3614c08\u0022}, \u0022source\u0022: \u0022### Question 2 - Goodness of Fit\\nAssume that a dataset contains five data points and a model made the following predictions for the target variable:\\n\\n| True Value | Prediction |\\n| :----------: | :--------: |\\n| 3.0 | 2.5 |\\n| -0.5 | 0.0 |\\n| 2.0 | 2.1 |\\n| 7.0 | 7.8 |\\n| 4.2 | 5.3 |\\n\\nRun the code cell below to use the `performance_metric` function and calculate this model\u0027s coefficient of determination.\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u002244edc96225a6f25ae990780980fb57e27d7cdc15\u0022}, \u0022source\u0022: \u0022# Calculate the performance of this model\\nscore = performance_metric([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\\nprint(\\\u0022Model has a coefficient of determination, R^2, of {:.3f}.\\\u0022.format(score))\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u002278c19a1ced1b900ccb4c9b09b372d71a6012513c\u0022}, \u0022source\u0022: \u0022### Visualization\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022ba68132b4609872212187e36ce3fbb70725001fd\u0022}, \u0022source\u0022: \u0022import numpy as np\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\ntrue, pred = [3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3]\\n\\n#Plot true values\\ntrue_handle = plt.scatter(true, true, alpha=0.6, color=\u0027blue\u0027, label=\u0027true\u0027)\\n\\n#Reference line\\nfit = np.poly1d(np.polyfit(true,true,1))\\nlims = np.linspace(min(true) - 1, max(true) + 1)\\nplt.plot(lims, fit(lims), alpha=0.3, color=\u0027black\u0027)\\n\\n#Plot predicted values\\npred_handle = plt.scatter(true, pred, alpha=0.6, color=\u0027red\u0027, label=\u0027predicted\u0027)\\n\\n#Legend and show\\nplt.legend(handles=[true_handle,pred_handle], loc=\u0027upper left\u0027)\\nplt.show()\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022e2b8c7d0c0f96621ea60b2b8ab6ce82bb5ddd30f\u0022}, \u0022source\u0022: \u0022* Would you consider this model to have successfully captured the variation of the target variable? \\n* Why or why not?\\n\\n** Hint: **  The R2 score is the proportion of the variance in the dependent variable that is predictable from the independent variable. In other words:\\n* R2 score of 0 means that the dependent variable cannot be predicted from the independent variable.\\n* R2 score of 1 means the dependent variable can be predicted from the independent variable.\\n* R2 score between 0 and 1 indicates the extent to which the dependent variable is predictable. An \\n* R2 score of 0.40 means that 40 percent of the variance in Y is predictable from X.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022dbac254833e0f72dc232e73087e4c29433b4de8b\u0022}, \u0022source\u0022: \u0022**Answer:** Yes, this model has successfully captured the variation of the target variable. This is because we are getting a very high R2 value of 0.923. That means 92.3% of the variance in the True Value is predictable from the Prediction. As this is a very high percentage, we can call this model to be a successful model.\\n\\nThe only drawback is there are only 5 datapoints here. So this might not be statistically significant. Another caveat is that whether the model is successful also depends largely on the application. So for some projects 0.923 is sufficient, whereas for others it could be a low score.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022acd84f7818208f81b8b0c63b129bf0a26a184b58\u0022}, \u0022source\u0022: \u0022### Implementation: Shuffle and Split Data\\nOur next implementation requires that we take the Boston housing dataset and split the data into training and testing subsets. Typically, the data is also shuffled into a random order when creating the training and testing subsets to remove any bias in the ordering of the dataset.\\n\\nFor the code cell below, we will need to implement the following:\\n- Use `train_test_split` from `sklearn.cross_validation` to shuffle and split the `features` and `prices` data into training and testing sets.\\n  - Split the data into 80% training and 20% testing.\\n  - Set the `random_state` for `train_test_split` to a value of your choice. This ensures results are consistent.\\n- Assign the train and testing splits to `X_train`, `X_test`, `y_train`, and `y_test`.\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u00224df0e9e910b51813cd2668282048e638ca0f4350\u0022}, \u0022source\u0022: \u0022# TODO: Import \u0027train_test_split\u0027\\nfrom sklearn import cross_validation\\n\\n# TODO: Shuffle and split the data into training and testing subsets\\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(features, prices, test_size = 0.2, random_state = 42)\\n\\n# Success\\nprint(\\\u0022Training and testing split was successful.\\\u0022)\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u00227d79ec8433e24d85ad0393eeac9aa3353d56d890\u0022}, \u0022source\u0022: \u0022### Question 3 - Training and Testing\\n\\n* What is the benefit to splitting a dataset into some ratio of training and testing subsets for a learning algorithm?\\n\\n**Hint:** Think about how overfitting or underfitting is contingent upon how splits on data is done.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u00220d8459d1d73bbf4d311a5ee2756494230d05abd5\u0022}, \u0022source\u0022: \u0022**Answer: ** A possible alternative to splitting a dataset into training and testing data would be to train and test on the same data. But that creates a problem. Here there is a very high chance of getting a high variance model which may eventually lead to a 100% accuracy rate with addition of new features, but that\u0027s only because it is overfitting the data. It has developed such a complex model that it will have limited or no ability to generalize data and so when we use that model on unknown data, it will give us very very low accuracy. So to avoid that, we can split the data into training and testing sets and train the model on the training data. Then the testing accuracy is a much better estimate than the training accuracy. \\n\\nBut then, the split might create a problem too. If we have a very limited dataset, then even if we take out a small sample of it as testing data, then also , we are losing a portion of the data. So there\u0027s an inherent trade off here which might cause underfitting due to limited datasets. This is where we can take advantage of K-fold cross validation where we divide all the datapoints into k number of bins and then run k separate learning experiments. In each of those, we pick one of those k subsets as our testing set and the remaining k-1 bins as our training sets. This is how we can maximize the machine\u0027s learning experiment.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u00225087a8047d0c16736c01f9df6fd1590889871f55\u0022}, \u0022source\u0022: \u0022----\\n\\n## Analyzing Model Performance\\nIn this third section of the project, we\u0027ll take a look at several models\u0027 learning and testing performances on various subsets of training data. Additionally, we\u0027ll investigate one particular algorithm with an increasing `\u0027max_depth\u0027` parameter on the full training set to observe how model complexity affects performance. Graphing our model\u0027s performance based on varying criteria can be beneficial in the analysis process, such as visualizing behavior that may not have been apparent from the results alone.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022d880c42e3da181c932151496c59f1f4837be5431\u0022}, \u0022source\u0022: \u0022### Learning Curves\\nThe following code cell produces four graphs for a decision tree model with different maximum depths. Each graph visualizes the learning curves of the model for both training and testing as the size of the training set is increased. Note that the shaded region of a learning curve denotes the uncertainty of that curve (measured as the standard deviation). The model is scored on both the training and testing sets using R\u003csup\u003e2\u003c/sup\u003e, the coefficient of determination.  \\n\\nRun the code cell below and use these graphs to answer the following question.\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u00225626afc91763b60b38edf02f634ad2e26a5c07a9\u0022}, \u0022source\u0022: \u0022#Define the necessary functions for plotting\\n###########################################\\n# Suppress matplotlib user warnings\\n# Necessary for newer version of matplotlib\\nimport warnings\\nwarnings.filterwarnings(\\\u0022ignore\\\u0022, category = UserWarning, module = \\\u0022matplotlib\\\u0022)\\n#\\n# Display inline matplotlib plots with IPython\\nfrom IPython import get_ipython\\nget_ipython().run_line_magic(\u0027matplotlib\u0027, \u0027inline\u0027)\\n###########################################\\n\\nimport matplotlib.pyplot as pl\\nimport numpy as np\\nimport sklearn.learning_curve as curves\\nfrom sklearn.tree import DecisionTreeRegressor\\nfrom sklearn.cross_validation import ShuffleSplit, train_test_split\\n\\ndef ModelLearning(X, y):\\n    \\\u0022\\\u0022\\\u0022 Calculates the performance of several models with varying sizes of training data.\\n        The learning and testing scores for each model are then plotted. \\\u0022\\\u0022\\\u0022\\n    \\n    # Create 10 cross-validation sets for training and testing\\n    cv = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.2, random_state = 0)\\n\\n    # Generate the training set sizes increasing by 50\\n    train_sizes = np.rint(np.linspace(1, X.shape[0]*0.8 - 1, 9)).astype(int)\\n\\n    # Create the figure window\\n    fig = pl.figure(figsize=(10,7))\\n\\n    # Create three different models based on max_depth\\n    for k, depth in enumerate([1,3,6,10]):\\n        \\n        # Create a Decision tree regressor at max_depth = depth\\n        regressor = DecisionTreeRegressor(max_depth = depth)\\n\\n        # Calculate the training and testing scores\\n        sizes, train_scores, test_scores = curves.learning_curve(regressor, X, y, \\\\\\n            cv = cv, train_sizes = train_sizes, scoring = \u0027r2\u0027)\\n        \\n        # Find the mean and standard deviation for smoothing\\n        train_std = np.std(train_scores, axis = 1)\\n        train_mean = np.mean(train_scores, axis = 1)\\n        test_std = np.std(test_scores, axis = 1)\\n        test_mean = np.mean(test_scores, axis = 1)\\n\\n        # Subplot the learning curve \\n        ax = fig.add_subplot(2, 2, k+1)\\n        ax.plot(sizes, train_mean, \u0027o-\u0027, color = \u0027r\u0027, label = \u0027Training Score\u0027)\\n        ax.plot(sizes, test_mean, \u0027o-\u0027, color = \u0027g\u0027, label = \u0027Testing Score\u0027)\\n        ax.fill_between(sizes, train_mean - train_std, \\\\\\n            train_mean + train_std, alpha = 0.15, color = \u0027r\u0027)\\n        ax.fill_between(sizes, test_mean - test_std, \\\\\\n            test_mean + test_std, alpha = 0.15, color = \u0027g\u0027)\\n        \\n        # Labels\\n        ax.set_title(\u0027max_depth = %s\u0027%(depth))\\n        ax.set_xlabel(\u0027Number of Training Points\u0027)\\n        ax.set_ylabel(\u0027Score\u0027)\\n        ax.set_xlim([0, X.shape[0]*0.8])\\n        ax.set_ylim([-0.05, 1.05])\\n    \\n    # Visual aesthetics\\n    ax.legend(bbox_to_anchor=(1.05, 2.05), loc=\u0027lower left\u0027, borderaxespad = 0.)\\n    fig.suptitle(\u0027Decision Tree Regressor Learning Performances\u0027, fontsize = 16, y = 1.03)\\n    fig.tight_layout()\\n    fig.show()\\n\\n\\ndef ModelComplexity(X, y):\\n    \\\u0022\\\u0022\\\u0022 Calculates the performance of the model as model complexity increases.\\n        The learning and testing errors rates are then plotted. \\\u0022\\\u0022\\\u0022\\n    \\n    # Create 10 cross-validation sets for training and testing\\n    cv = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.2, random_state = 0)\\n\\n    # Vary the max_depth parameter from 1 to 10\\n    max_depth = np.arange(1,11)\\n\\n    # Calculate the training and testing scores\\n    train_scores, test_scores = curves.validation_curve(DecisionTreeRegressor(), X, y, \\\\\\n        param_name = \\\u0022max_depth\\\u0022, param_range = max_depth, cv = cv, scoring = \u0027r2\u0027)\\n\\n    # Find the mean and standard deviation for smoothing\\n    train_mean = np.mean(train_scores, axis=1)\\n    train_std = np.std(train_scores, axis=1)\\n    test_mean = np.mean(test_scores, axis=1)\\n    test_std = np.std(test_scores, axis=1)\\n\\n    # Plot the validation curve\\n    pl.figure(figsize=(7, 5))\\n    pl.title(\u0027Decision Tree Regressor Complexity Performance\u0027)\\n    pl.plot(max_depth, train_mean, \u0027o-\u0027, color = \u0027r\u0027, label = \u0027Training Score\u0027)\\n    pl.plot(max_depth, test_mean, \u0027o-\u0027, color = \u0027g\u0027, label = \u0027Validation Score\u0027)\\n    pl.fill_between(max_depth, train_mean - train_std, \\\\\\n        train_mean + train_std, alpha = 0.15, color = \u0027r\u0027)\\n    pl.fill_between(max_depth, test_mean - test_std, \\\\\\n        test_mean + test_std, alpha = 0.15, color = \u0027g\u0027)\\n    \\n    # Visual aesthetics\\n    pl.legend(loc = \u0027lower right\u0027)\\n    pl.xlabel(\u0027Maximum Depth\u0027)\\n    pl.ylabel(\u0027Score\u0027)\\n    pl.ylim([-0.05,1.05])\\n    pl.show()\\n\\n\\ndef PredictTrials(X, y, fitter, data):\\n    \\\u0022\\\u0022\\\u0022 Performs trials of fitting and predicting data. \\\u0022\\\u0022\\\u0022\\n\\n    # Store the predicted prices\\n    prices = []\\n\\n    for k in range(10):\\n        # Split the data\\n        X_train, X_test, y_train, y_test = train_test_split(X, y, \\\\\\n            test_size = 0.2, random_state = k)\\n        \\n        # Fit the data\\n        reg = fitter(X_train, y_train)\\n        \\n        # Make a prediction\\n        pred = reg.predict([data[0]])[0]\\n        prices.append(pred)\\n        \\n        # Result\\n        print(\\\u0022Trial {}: ${:,.2f}\\\u0022.format(k+1, pred))\\n\\n    # Display price range\\n    print(\\\u0022\\\\nRange in prices: ${:,.2f}\\\u0022.format(max(prices) - min(prices)))\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u00224be0e269a3ea1d5a71b0831c9ab8652e4a495f6b\u0022}, \u0022source\u0022: \u0022# Produce learning curves for varying training set sizes and maximum depths\\nModelLearning(features, prices)\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022537d6af7237ea9f3f4cf34d9639c11bc6746d6e6\u0022}, \u0022source\u0022: \u0022### Question 4 - Learning the Data\\n* Choose one of the graphs above and state the maximum depth for the model. \\n* What happens to the score of the training curve as more training points are added? What about the testing curve? \\n* Would having more training points benefit the model? \\n\\n**Hint:** Are the learning curves converging to particular scores? Generally speaking, the more data you have, the better. But if your training and testing curves are converging with a score above your benchmark threshold, would this be necessary?\\nThink about the pros and cons of adding more training points based on if the training and testing curves are converging.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u00222bcdd955693b9270603f6a2deb3ccd82b608aa0a\u0022}, \u0022source\u0022: \u0022Answer:\\n\\nA) max_depth = 1 (High Bias Scenario):  We see that initially the Testing Score(green line)  increases with increase in number of training points. But then it plateaus at a very low accuracy score of 0.4 or 40% and increase in number of training points have no effect. This shows that the model does not generalize well on unseen data. On the other hand, the Training Score(red line)  decreases with increase in the number of training points and gets saturated at a score of approximately 0.4 or 40%. This shows that the model is actually underfitting the data and is not complex enough. In this scenario, adding more training points will not benefit the model. Instead, its complexity should be increased for better fitting the dataset.\\n\\nB)max_depth = 3 (Best scenario): Testing Score(green line) increases with increase in training points. It reaches a pretty high score of 0.8 and so we can see the model generalizes well. The Training Score(red line) decreases slightly and reaches 0.8 and stays constant. So we see it fits the model well and reaches a pretty high score. The testing score has two significant phases where the rates of change are different. One is the positive rate of change which goes on uptil approximately 200 training points (within this positive rate of change, we again observe two different rates. One is uptil 50 training points where the rate of increase is very high.The other is between 50 - 200 where the rate of increase is much lower.) and the other is the region where it plateaus with no/very little rate of change which is beyond 200 training points. So if we are below 200 training points, adding more training points will definitely improve the score but beyond that adding more training points will not be very useful as the rate plateaus.\\n\\nC) max_depth = 6 (High Variance Scenario): Testing Score(green line) increases with increase in training points and reaches 0.7. Even though this is not a bad accuracy, it is not generalizing the data as well as max_depth = 3. The Training Score(red line) decrease ever so slightly and stays at 0.9 which is a big sign that it is overfitting the data. It is a High Variance problem. Here also, the testing score show a similar behaviour as the previous one (it plateaus after 200 training points). So once again, we will get an improvement in the testing score by adding more training points when the nuber of training points is less than 200, but after that adding more training points will not benefit us much.\\n\\nD) max_depth = 10 (Higher Variance Scenario): Testing Score(green line) increases with increase in training points and reaches 0.7. So same problem as the previous one. It is not generalizing the data as well as scenario B). The  Training Score(red line) remains constant throughout showing a perfect accuracy of 100% or a score of 1 which tells us it is definitely overfitting the data. This is also a very High Variance problem. Once again the curve show exactly the same behaviour where adding more training points upto 200 will increase the score but not beyond that.\\n\\n\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022af253272e47160270847f617a581ccb6c1b55dd8\u0022}, \u0022source\u0022: \u0022### Complexity Curves\\nThe following code cell produces a graph for a decision tree model that has been trained and validated on the training data using different maximum depths. The graph produces two complexity curves \\u2014 one for training and one for validation. Similar to the **learning curves**, the shaded regions of both the complexity curves denote the uncertainty in those curves, and the model is scored on both the training and validation sets using the `performance_metric` function.  \\n\\n** Run the code cell below and use this graph to answer the following two questions Q5 and Q6. **\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022ce7909fa56ac27f6e4327c4a44156eadfac63218\u0022}, \u0022source\u0022: \u0022ModelComplexity(X_train, y_train)\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u00222b4e1477c3bdeb1afe152d657cecf817a5eca097\u0022}, \u0022source\u0022: \u0022### Question 5 - Bias-Variance Tradeoff\\n* When the model is trained with a maximum depth of 1, does the model suffer from high bias or from high variance? \\n* How about when the model is trained with a maximum depth of 10? What visual cues in the graph justify your conclusions?\\n\\n**Hint:** High bias is a sign of underfitting(model is not complex enough to pick up the nuances in the data) and high variance is a sign of overfitting(model is by-hearting the data and cannot generalize well). Think about which model(depth 1 or 10) aligns with which part of the tradeoff.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u00229ecedaca5241dcfbb0b9c5ea8c1f30a86aef2fbc\u0022}, \u0022source\u0022: \u0022Answer: We can easily recognize a problem related to High Bias or High Variance by simply looking at the graph of training and testing scores.\\n\\nIf there is High Bias, there will be very little gap between Training and Testing Scores. This is because in High Bias scenarios, the model underfits the data and also cannot generalize the data well resulting in both curves converging to a low score.\\n\\nIf there is High Variance, there will be a large gap between the Training and Testing Scores. This is because in High Variance model, even though the model fits well, it does not generalize well as a result of overfitting. This leads to a high Training Score but a relatively low Testing/Validation Score.\\n\\nA) Maximum Depth = 1 (High Bias): Here both Training and Testing Scores are low. So the model is not fitting well and so it is not generalizing well. Thus the two curves are very close to each other and hence this is a High Bias situation.\\n\\nB) Maximum Depth = 10 (High Variance): Here there is a huge gap between Training and Testing Scores. The Training score is almost perfect at 1, but the testing score is much low at around 0.7. So the model is overfitting and hence does not generalize well resulting in a lower Validation Score. So this is a High Variance situation with the curves being far apart. \u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u00221bd38765a62ff32c2c76a663ad4663b80e35124d\u0022}, \u0022source\u0022: \u0022### Question 6 - Best-Guess Optimal Model\\n* Which maximum depth do you think results in a model that best generalizes to unseen data? \\n* What intuition lead you to this answer?\\n\\n** Hint: ** Look at the graph above Question 5 and see where the validation scores lie for the various depths that have been assigned to the model. Does it get better with increased depth? At what point do we get our best validation score without overcomplicating our model? And remember, Occams Razor states \\\u0022Among competing hypotheses, the one with the fewest assumptions should be selected.\\\u0022\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u002212a49303b9a7fa2d6135e389a840675208116f54\u0022}, \u0022source\u0022: \u0022Answer:  Maximum Depth = 4\\n\\nThe validation score seems to plateau here. So this is the highest validation score we can get i.e best generalization of unseen data.\\n\\nThe gap between the Training Score and the Validation Score is not significantly large here too which indicates a High Variance Situation.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022490f25286452dba44fab91dd71764bb788cbe5ea\u0022}, \u0022source\u0022: \u0022-----\\n\\n## Evaluating Model Performance\\nIn this final section of the project, we will construct a model and make a prediction on the client\u0027s feature set using an optimized model from `fit_model`.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u002294519eb50d606e448e8159ec721eb1ccfcbef6d1\u0022}, \u0022source\u0022: \u0022### Question 7 - Grid Search\\n* What is the grid search technique?\\n* How it can be applied to optimize a learning algorithm?\\n\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u002291ad40fe293f9c6caa0fb8cbadf09886f22f4885\u0022}, \u0022source\u0022: \u0022**Answer: ** The Grid search technique allows us to define a grid of the hyperparameters for a specific classifier and then the Grid search technique exhaustively tries out every possible combinations of the hyperparameters values in order to find the best model. After that we can use cross validation techniques like K-fold cross validation or Stratified Shuffle Split to find the highest accuracy by using the hyperparameters suggested by Grid Search technique optimizing the learning algorithm.\\n\\n** Point to Note: ** Due to its exhaustive search nature, grid search can be computationally expensive, especially when data size is large and model is complicated. Sometimes we resort to randomized search in this case to search only some combinations of the parameters. \\n(http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html#sklearn-grid-search-randomizedsearchcv)\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022bcd08529ff2f0047c03073c28b86536624c98859\u0022}, \u0022source\u0022: \u0022### Question 8 - Cross-Validation\\n\\n* What is the k-fold cross-validation training technique? \\n\\n* What benefit does this technique provide for grid search when optimizing a model?\\n\\n**Hint:** When explaining the k-fold cross validation technique, be sure to touch upon what \u0027k\u0027 is, how the dataset is split into different parts for training and testing and the number of times it is run based on the \u0027k\u0027 value.\\n\\nWhen thinking about how k-fold cross validation helps grid search, think about the main drawbacks of grid search which are hinged upon **using a particular subset of data for training or testing** and how k-fold cv could help alleviate that. You can refer to the [docs](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation) for your answer.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022188d1e87e6647bd01915781c6df6badd94f49569\u0022}, \u0022source\u0022: \u0022**Answer: ** In K-fold cross validation technique, we partition the data into k-bins of equal size. After that we run k separate learning experiments. In each of those, we pick one of the k subsets as our testing set. The remaining k-1 bins are put together into the training set. Then we train our machine learning algorithm and just like before test the performance on the testing set. The key thing in cross validation is we run this multiple times (k times) and then we average the k different testing set performances for the k different hold out sets. So we average the test results from those k experiments. So obviously this takes more computation time as now we have to run k separate learning experiments, but the assessment of the learning algorithm will be more accurate.\\n\\nIf we run Grid Search without running a cross validation set, we will have different sets of optimal hyperparameters because without a cross validation set, the estimate of out-of-sample performance would have a high variance. \\n\\nSo in summary, without k-fold cross validation, the Grid Search will select hyper parameter values which works really well on the sample train test split data but there is a high risk that it will work poorly for unknown datasets because of high variance.\\n\\n\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022caa9510684ac27b4e6a4b8a562f72c47ec3e588b\u0022}, \u0022source\u0022: \u0022### Implementation: Fitting a Model\\nOur final implementation requires that we bring everything together and train a model using the **decision tree algorithm**. To ensure that we are producing an optimized model, we will train the model using the grid search technique to optimize the `\u0027max_depth\u0027` parameter for the decision tree. The `\u0027max_depth\u0027` parameter can be thought of as how many questions the decision tree algorithm is allowed to ask about the data before making a prediction. Decision trees are part of a class of algorithms called *supervised learning algorithms*.\\n\\nIn addition, we will find our implementation is using `ShuffleSplit()` for an alternative form of cross-validation (see the `\u0027cv_sets\u0027` variable). While it is not the K-Fold cross-validation technique you describe in **Question 8**, this type of cross-validation technique is just as useful!. The `ShuffleSplit()` implementation below will create 10 (`\u0027n_splits\u0027`) shuffled sets, and for each shuffle, 20% (`\u0027test_size\u0027`) of the data will be used as the *validation set*. While we\u0027re working on our implementation, we\u0027ll think about the contrasts and similarities it has to the K-fold cross-validation technique.\\n\\nPlease note that ShuffleSplit has different parameters in scikit-learn versions 0.17 and 0.18.\\nFor the `fit_model` function in the code cell below, we will need to implement the following:\\n- Use [`DecisionTreeRegressor`](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) from `sklearn.tree` to create a decision tree regressor object.\\n  - Assign this object to the `\u0027regressor\u0027` variable.\\n- Create a dictionary for `\u0027max_depth\u0027` with the values from 1 to 10, and assign this to the `\u0027params\u0027` variable.\\n- Use [`make_scorer`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html) from `sklearn.metrics` to create a scoring function object.\\n  - Pass the `performance_metric` function as a parameter to the object.\\n  - Assign this scoring function to the `\u0027scoring_fnc\u0027` variable.\\n- Use [`GridSearchCV`](http://scikit-learn.org/0.17/modules/generated/sklearn.grid_search.GridSearchCV.html) from `sklearn.grid_search` to create a grid search object.\\n  - Pass the variables `\u0027regressor\u0027`, `\u0027params\u0027`, `\u0027scoring_fnc\u0027`, and `\u0027cv_sets\u0027` as parameters to the object. \\n  - Assign the `GridSearchCV` object to the `\u0027grid\u0027` variable.\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022de5e7d278411c525d71bbfd198f0c310d3dd8f46\u0022}, \u0022source\u0022: \u0022# TODO: Import \u0027make_scorer\u0027, \u0027DecisionTreeRegressor\u0027, and \u0027GridSearchCV\u0027\\nfrom sklearn.tree import DecisionTreeRegressor\\nfrom sklearn.metrics import make_scorer\\nfrom sklearn.grid_search import GridSearchCV\\n\\ndef fit_model(X, y):\\n    \\\u0022\\\u0022\\\u0022 Performs grid search over the \u0027max_depth\u0027 parameter for a \\n        decision tree regressor trained on the input data [X, y]. \\\u0022\\\u0022\\\u0022\\n    \\n    # Create cross-validation sets from the training data\\n    # sklearn version 0.18: ShuffleSplit(n_splits=10, test_size=0.1, train_size=None, random_state=None)\\n    # sklearn versiin 0.17: ShuffleSplit(n, n_iter=10, test_size=0.1, train_size=None, random_state=None)\\n    cv_sets = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.20, random_state = 0)\\n\\n    # TODO: Create a decision tree regressor object\\n    \\n    regressor = DecisionTreeRegressor(random_state = 1001)\\n\\n    # TODO: Create a dictionary for the parameter \u0027max_depth\u0027 with a range from 1 to 10\\n    tree_range = range(1, 11)\\n    params = dict(max_depth=[1,2,3,4,5,6,7,8,9,10])\\n\\n    # TODO: Transform \u0027performance_metric\u0027 into a scoring function using \u0027make_scorer\u0027 \\n    scoring_fnc = make_scorer(performance_metric)\\n\\n    # TODO: Create the grid search cv object --\u003e GridSearchCV()\\n    # Make sure to include the right parameters in the object:\\n    # (estimator, param_grid, scoring, cv) which have values \u0027regressor\u0027, \u0027params\u0027, \u0027scoring_fnc\u0027, and \u0027cv_sets\u0027 respectively.\\n    grid = GridSearchCV(regressor,params,scoring=scoring_fnc,cv=cv_sets)\\n\\n    # Fit the grid search object to the data to compute the optimal model\\n    grid = grid.fit(X, y)\\n\\n    # Return the optimal model after fitting the data\\n    return grid.best_estimator_\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u002236ee52bb6d430084c3f0a2a4e0545fff99916316\u0022}, \u0022source\u0022: \u0022### Making Predictions\\nOnce a model has been trained on a given set of data, it can now be used to make predictions on new sets of input data. In the case of a *decision tree regressor*, the model has learned *what the best questions to ask about the input data are*, and can respond with a prediction for the **target variable**. We can use these predictions to gain information about data where the value of the target variable is unknown \\u2014 such as data the model was not trained on.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u00220aaf4b8bf51630a96abf85b00b467feaf19536a6\u0022}, \u0022source\u0022: \u0022### Question 9 - Optimal Model\\n\\n* What maximum depth does the optimal model have? How does this result compare to your guess in **Question 6**?  \\n\\nRun the code block below to fit the decision tree regressor to the training data and produce an optimal model.\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022e952e6e5557ed258fbfdf0dc5cf83b9a481a779c\u0022}, \u0022source\u0022: \u0022# Fit the training data to the model using grid search\\nreg = fit_model(X_train, y_train)\\n\\n# Produce the value for \u0027max_depth\u0027\\nprint(\\\u0022Parameter \u0027max_depth\u0027 is {} for the optimal model.\\\u0022.format(reg.get_params()[\u0027max_depth\u0027]))\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022c5c8354e3415694e1dfa5a77d45aeabc32a0cc5b\u0022}, \u0022source\u0022: \u0022** Hint: ** The answer comes from the output of the code snipped above.\\n\\n**Answer: ** The optimum model has a maximum depth of 4. This exactly matches our guess from ** Question 6 **. Both results are reliable as in both cases, we did cross validation with Shufflesplit combined with checking against a range of the max_depth hyperparamters to give us the most optimal value of the max_depth. So based on our course of action, there is very little chance that our model will work poorly for unknown datasets because of high variance.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u002279b82a252a87655e7014c3e0d856bd77e0599bc7\u0022}, \u0022source\u0022: \u0022### Question 10 - Predicting Selling Prices\\nImagine that we were a real estate agent in the Boston area looking to use this model to help price homes owned by our clients that they wish to sell. We have collected the following information from three of our clients:\\n\\n| Feature | Client 1 | Client 2 | Client 3 |\\n| :---: | :---: | :---: | :---: |\\n| Total number of rooms in home | 5 rooms | 4 rooms | 8 rooms |\\n| Neighborhood poverty level (as %) | 17% | 32% | 3% |\\n| Student-teacher ratio of nearby schools | 15-to-1 | 22-to-1 | 12-to-1 |\\n\\n* What price would you recommend each client sell his/her home at? \\n* Do these prices seem reasonable given the values for the respective features? \\n\\n**Hint:** Use the statistics you calculated in the **Data Exploration** section to help justify your response.  Of the three clients, client 3 has has the biggest house, in the best public school neighborhood with the lowest poverty level; while client 2 has the smallest house, in a neighborhood with a relatively high poverty rate and not the best public schools.\\n\\nRun the code block below to have your optimized model make predictions for each client\u0027s home.\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022353f3ccdba3e93361baf3a1b13e932f3f15986a1\u0022}, \u0022source\u0022: \u0022# Produce a matrix for client data\\nclient_data = [[5, 17, 15], # Client 1\\n               [4, 32, 22], # Client 2\\n               [8, 3, 12]]  # Client 3\\n\\n# Show predictions\\nfor i, price in enumerate(reg.predict(client_data)):\\n    print(\\\u0022Predicted selling price for Client {}\u0027s home: ${:,.2f}\\\u0022.format(i+1, price))\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022a145c5bf4700f486ac330e20cfbf7917a8f1c6c6\u0022}, \u0022source\u0022: \u0022### Visualization\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022cba2ba3b4c485cfa92132f3406b3fd5b3cf68a40\u0022}, \u0022source\u0022: \u0022from matplotlib import pyplot as plt\\n\\nclients = np.transpose(client_data)\\npred = reg.predict(client_data)\\nfor i, feat in enumerate([\u0027RM\u0027, \u0027LSTAT\u0027, \u0027PTRATIO\u0027]):\\n    plt.scatter(features[feat], prices, alpha=0.25, c=prices)\\n    plt.scatter(clients[i], pred, color=\u0027black\u0027, marker=\u0027x\u0027, linewidths=2)\\n    plt.xlabel(feat)\\n    plt.ylabel(\u0027MEDV\u0027)\\n    plt.show()\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022a8c32a11f476edc79bc39969cf3e20b114fe1bb4\u0022}, \u0022source\u0022: \u0022**Answer: **\\n\\nClient 1: $403,025.00\\n\\nClient 2: $237,478.72\\n\\nClient 3: $931,636.36\\n\\nIn our initial ** Data Exploration ** section, we saw that the price is positively correlated with the number of rooms and negatively correlated with Neighbourhood Poverty level and Student-teacher ratio of nearby schools. Also these were the statistics of our data.\\n\\nMinimum price: $105,000.00\\n\\nMaximum price: $1,024,800.00\\n\\nMean price: $454,342.94\\n\\nMedian price $438,900.00\\n\\nStandard deviation of prices: $165,340.28\\n\\nSo we see that for Client 1 and 2, the price of the house is below the median price of the houses. This is reasonable because of \\n\\na) High Poverty Level and Student to Teacher ratio for client 2.\\n\\nb) Average Poverty level and Student to Teacher ratio for client 1.\\n\\nFor Client 3, we see that the price is well over the median house price and very close to the maximum house price. This is also reasonable because of very low Poverty Level and Student to Teacher ratio and also a high number of rooms.\\n\\nSo overall, the prices for all the clients seem reasonable.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u002200218dfe3da410c11f049582b22e812cef1682a1\u0022}, \u0022source\u0022: \u0022### Perfomance Metric\\n\\nLet us calculate the R squared value for our model.\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u00220113d69f2528fd07047faea91626a2af55f8f865\u0022}, \u0022source\u0022: \u0022reg = fit_model(X_train, y_train)\\npred = reg.predict(X_test)\\nscore = performance_metric(y_test,pred)\\nprint(\\\u0022R Squared Value: \\\u0022 + str(score))\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022f7141377aa6c6225854a7dadad777b5f60ee450e\u0022}, \u0022source\u0022: \u0022So we get a pretty good R squared score from our model.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022ac04d7e5a52fbeb60af4a17966d9561d8de82040\u0022}, \u0022source\u0022: \u0022### Visualization\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022b1f1fb15fab90119dbd804d0d7fc295ca5d92c3c\u0022}, \u0022source\u0022: \u0022import matplotlib.pyplot as plt\\nplt.hist(prices, bins = 20)\\nfor price in reg.predict(client_data):\\n    plt.axvline(price, lw = 5, c = \u0027r\u0027)\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022543f5237d479c96fc446c20091f8c020a4f89215\u0022}, \u0022source\u0022: \u0022### Sensitivity\\nAn optimal model is not necessarily a robust model. Sometimes, a model is either too complex or too simple to sufficiently generalize to new data. Sometimes, a model could use a learning algorithm that is not appropriate for the structure of the data given. Other times, the data itself could be too noisy or contain too few samples to allow a model to adequately capture the target variable \\u2014 i.e., the model is underfitted. \\n\\n**Run the code cell below to run the `fit_model` function ten times with different training and testing sets to see how the prediction for a specific client changes with respect to the data it\u0027s trained on.**\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022bd7654c6a6310b40779b27f8612ca7e8c4119a63\u0022}, \u0022source\u0022: \u0022PredictTrials(features, prices, fit_model, client_data)\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u002225dde0d3d5d35ff3b8042ac3cdce10a53d36c51e\u0022}, \u0022source\u0022: \u0022### Question 11 - Applicability\\n\\n* In a few sentences, discuss whether the constructed model should or should not be used in a real-world setting.  \\n\\n**Hint:** Take a look at the range in prices as calculated in the code snippet above. Some questions to answering:\\n- How relevant today is data that was collected from 1978? How important is inflation?\\n- Are the features present in the data sufficient to describe a home? Do you think factors like quality of apppliances in the home, square feet of the plot area, presence of pool or not etc should factor in?\\n- Is the model robust enough to make consistent predictions?\\n- Would data collected in an urban city like Boston be applicable in a rural city?\\n- Is it fair to judge the price of an individual home based on the characteristics of the entire neighborhood?\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022c9e6e63584a79ec5e019f3f31599a7d52086b718\u0022}, \u0022source\u0022: \u0022**Answer: ** \\n\\n1) The data which was collected in 1978 is not so relevant today because demographics and economy has changed a lot since then.\\n\\n2) The features present in the data is not sufficient to describe a home. There are only three features present right now. We can add more features like crime rate, transportation avalibility, presence of pool or not, square feet of the plot area, quality of appliances, flooring in the home and more.\\n\\n3) This model based on its current feature is robust enough to make consistent predictions with a small margin of error.\\n\\n4) Data collected in an urban city like Boston may not be applicable in a rural city as many properties will change like the Demographics, Economy, Average income etc. So we would have to take in account a lot of other features in order to build an effective model\\n\\n5) Neighbourhood plays a very vital role in judging the price of a house like the crime rate, schools, transportation etc. But if an individual house has some marked characteristics which can overshadow the factors that neighbourhood plays, then it would not be fair to judge the price of an individual home based on the characteristics of the entire neighborhood.\u0022}], \u0022nbformat_minor\u0022: 0, \u0022nbformat\u0022: 4, \u0022metadata\u0022: {\u0022kernelspec\u0022: {\u0022name\u0022: \u0022python3\u0022, \u0022language\u0022: \u0022python\u0022, \u0022display_name\u0022: \u0022Python 3\u0022}, \u0022language_info\u0022: {\u0022nbconvert_exporter\u0022: \u0022python\u0022, \u0022pygments_lexer\u0022: \u0022ipython3\u0022, \u0022version\u0022: \u00223.6.1\u0022, \u0022name\u0022: \u0022python\u0022, \u0022file_extension\u0022: \u0022.py\u0022, \u0022mimetype\u0022: \u0022text/x-python\u0022, \u0022codemirror_mode\u0022: {\u0022name\u0022: \u0022ipython\u0022, \u0022version\u0022: 3}}}}","dateCreated":"2017-07-11T15:27:57.323Z"},"kernelRun":{"id":1332019,"kernelId":307985,"status":"complete","type":"batch","sourceType":"notebook","language":"python","title":"Predicting Boston House Prices","dateCreated":"2017-07-11T15:27:57.323Z","dateEvaluated":"2017-07-11T15:27:58.24Z","workerContainerPort":null,"workerUptimeSeconds":409781,"workerIPAddress":"10.3.0.101     ","scriptLanguageId":9,"scriptLanguageName":"IPython Notebook HTML","renderedOutputUrl":"https://www.kaggleusercontent.com/kf/1332019/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..fd4fO8Bhn8INzikCm-Gqaw.ZO-wI0ie7OFW5Dk1lYS2eQZalShGtZAAMGFyxJhtTQBIAzoBw0ur_kq9Wn7FTYyLagJW_WBmoUgyi_8WBL4PMwaAfCaqbznJTZk8KjUneKs_hBaQCM71yT7TsNwcC0hEedDXRrwtaiw7IvQKcijKq2ean37imRR1uS7XqTlQSBI.ueObF0iBlR_hIFptdHFnyA/__results__.html","commit":{"id":7355833,"settings":{"dockerImageVersionId":null,"dataSources":[{"sourceType":"DatasetVersion","sourceId":2485,"databundleVersionId":null,"mountSlug":null}],"sourceType":"notebook","language":"python","isGpuEnabled":false,"isInternetEnabled":false},"source":"{\u0022cells\u0022: [{\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u002283dcb1a7c0f2d6184105bf5fadfc95b845b1bb71\u0022}, \u0022source\u0022: \u0022## Getting Started\\nIn this project, we will evaluate the performance and predictive power of a model that has been trained and tested on data collected from homes in suburbs of Boston, Massachusetts. A model trained on this data that is seen as a *good fit* could then be used to make certain predictions about a home \\u2014 in particular, its monetary value. This model would prove to be invaluable for someone like a real estate agent who could make use of such information on a daily basis.\\n\\nThe dataset for this project originates from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Housing). The Boston housing data was collected in 1978 and each of the 506 entries represent aggregated data about 14 features for homes from various suburbs in Boston, Massachusetts. For the purposes of this project, the following preprocessing steps have been made to the dataset:\\n- 16 data points have an `\u0027MEDV\u0027` value of 50.0. These data points likely contain **missing or censored values** and have been removed.\\n- 1 data point has an `\u0027RM\u0027` value of 8.78. This data point can be considered an **outlier** and has been removed.\\n- The features `\u0027RM\u0027`, `\u0027LSTAT\u0027`, `\u0027PTRATIO\u0027`, and `\u0027MEDV\u0027` are essential. The remaining **non-relevant features** have been excluded.\\n- The feature `\u0027MEDV\u0027` has been **multiplicatively scaled** to account for 35 years of market inflation.\\n\\nRun the code cell below to load the Boston housing dataset, along with a few of the necessary Python libraries required for this project. You will know the dataset loaded successfully if the size of the dataset is reported.\u0022}, {\u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022_uuid\u0022: \u0022e2af66277939d7011a90826443705ca6c4e44b81\u0022, \u0022trusted\u0022: false, \u0022_cell_guid\u0022: \u00225fff6d9f-e77f-4b02-b79f-f1623cfe4c20\u0022}, \u0022outputs\u0022: [], \u0022cell_type\u0022: \u0022code\u0022, \u0022execution_count\u0022: null, \u0022source\u0022: \u0022# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n# For example, here\u0027s several helpful packages to load in \\n\\nimport numpy as np # linear algebra\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\nfrom sklearn.cross_validation import ShuffleSplit\\n\\n# Pretty display for notebooks\\n%matplotlib inline\\n# Input data files are available in the \\\u0022../input/\\\u0022 directory.\\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\\n\\nfrom subprocess import check_output\\nprint(check_output([\\\u0022ls\\\u0022, \\\u0022../input\\\u0022]).decode(\\\u0022utf8\\\u0022))\\n\\n\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u002263d06b86597d9ca010eaa00f6801220dc435a181\u0022}, \u0022source\u0022: \u0022# Load the Boston housing dataset\\ndata = pd.read_csv(\u0027../input/housing.csv\u0027)\\nprices = data[\u0027MEDV\u0027]\\nfeatures = data.drop(\u0027MEDV\u0027, axis = 1)\\n\\ndata.head()\\n\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u00221578947bd37c23afa5fed31e35e97c06fdb4e574\u0022}, \u0022source\u0022: \u0022## Data Exploration\\nIn this first section of this project, we will make a cursory investigation about the Boston housing data and provide our observations. Familiarizing ourself with the data through an explorative process is a fundamental practice to help us better understand and justify our results.\\n\\nSince the main goal of this project is to construct a working model which has the capability of predicting the value of houses, we will need to separate the dataset into **features** and the **target variable**. The **features**, `\u0027RM\u0027`, `\u0027LSTAT\u0027`, and `\u0027PTRATIO\u0027`, give us quantitative information about each data point. The **target variable**, `\u0027MEDV\u0027`, will be the variable we seek to predict. These are stored in `features` and `prices`, respectively.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022e1160a8ba82ab251623ccda98a130660168ac2cf\u0022}, \u0022source\u0022: \u0022### Implementation: Calculate Statistics\\nFor our very first coding implementation, we will calculate descriptive statistics about the Boston housing prices. Since `numpy` has already been imported for us, use this library to perform the necessary calculations. These statistics will be extremely important later on to analyze various prediction results from the constructed model.\\n\\nIn the code cell below, we will need to implement the following:\\n- Calculate the minimum, maximum, mean, median, and standard deviation of `\u0027MEDV\u0027`, which is stored in `prices`.\\n  - Store each calculation in their respective variable.\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u00229d388fff06110d5428097f2c1d3aa0d4784ed748\u0022}, \u0022source\u0022: \u0022# TODO: Minimum price of the data\\nminimum_price = np.mean(prices)\\n\\n# TODO: Maximum price of the data\\nmaximum_price = np.max(prices)\\n\\n# TODO: Mean price of the data\\nmean_price = np.mean(prices)\\n\\n# TODO: Median price of the data\\nmedian_price = np.median(prices)\\n\\n# TODO: Standard deviation of prices of the data\\nstd_price = np.std(prices)\\n\\n# Show the calculated statistics\\nprint(\\\u0022Statistics for Boston housing dataset:\\\\n\\\u0022)\\nprint(\\\u0022Minimum price: ${:,.2f}\\\u0022.format(minimum_price))\\nprint(\\\u0022Maximum price: ${:,.2f}\\\u0022.format(maximum_price))\\nprint(\\\u0022Mean price: ${:,.2f}\\\u0022.format(mean_price))\\nprint(\\\u0022Median price ${:,.2f}\\\u0022.format(median_price))\\nprint(\\\u0022Standard deviation of prices: ${:,.2f}\\\u0022.format(std_price))\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022e3d80e14179ca40fc011a7fe157e039360d4565f\u0022}, \u0022source\u0022: \u0022### Question 1 - Feature Observation\\nAs a reminder, we are using three features from the Boston housing dataset: `\u0027RM\u0027`, `\u0027LSTAT\u0027`, and `\u0027PTRATIO\u0027`. For each data point (neighborhood):\\n- `\u0027RM\u0027` is the average number of rooms among homes in the neighborhood.\\n- `\u0027LSTAT\u0027` is the percentage of homeowners in the neighborhood considered \\\u0022lower class\\\u0022 (working poor).\\n- `\u0027PTRATIO\u0027` is the ratio of students to teachers in primary and secondary schools in the neighborhood.\\n\\n\\n** Using your intuition, for each of the three features above, do you think that an increase in the value of that feature would lead to an **increase** in the value of `\u0027MEDV\u0027` or a **decrease** in the value of `\u0027MEDV\u0027`? Justify your answer for each.**\\n\\n**Hint:** This problem can phrased using examples like below.  \\n* Would you expect a home that has an `\u0027RM\u0027` value(number of rooms) of 6 be worth more or less than a home that has an `\u0027RM\u0027` value of 7?\\n* Would you expect a neighborhood that has an `\u0027LSTAT\u0027` value(percent of lower class workers) of 15 have home prices be worth more or less than a neighborhood that has an `\u0027LSTAT\u0027` value of 20?\\n* Would you expect a neighborhood that has an `\u0027PTRATIO\u0027` value(ratio of students to teachers) of 10 have home prices be worth more or less than a neighborhood that has an `\u0027PTRATIO\u0027` value of 15?\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u00222aa5e98163f393861e8e0c2901a2aca2623ff295\u0022}, \u0022source\u0022: \u0022**Answer: ** In my opinion, the value of \u0027MEDV\u0027 will be dependent on these 3 features in the following way:\\n\\n1) **RM** - The more the value of RM, the more will be the value of \u0027MEDV\u0027. Because it\u0027s pretty evident that with increase in the number of rooms, the price of the house will increase.\\n\\n2) **LSTAT** - The more the value of LSTAT, the less will be the value of \u0027MEDV\u0027. Because with increase in the percentage of \\\u0022lower class\\\u0022 homeowners in the neighbourhood, the crime rate in the neighbourhood may increase. Even though LSTAT doesn\u0027t have a causal effect on the crime rate in the neighbourhood, they are likely to be positively correlated. One more factor is if there are greater percentages of \\\u0022lower class\\\u0022 homeowners in the neighbourhood, then more likely very expensive real estate owners will not build their housing complexes in that region as most of the people will not be able to afford it. So in average, the houses in that region will be cheaper.\\n\\n3) **PTRATIO** - The lesser the value of PTRATIO, the more will be the value of \u0027MEDV\u0027. Because if the students to teacher ratio is low, then that means individual students gets much more attention from the students as opposed to a region where this ratio is high. Over there, as the number of students will be much higher than the number of teachers, teachers will not be able to attend to students individually everytime and hence this may affect the education of the students. So regions with a low PTRATIO will have higher prices for houses.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022c189a186d89871eda1087ce6c49b079daab3681c\u0022}, \u0022source\u0022: \u0022## Initial Visualization\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u002254dba09fe89619bd0f948c93a621627e46ec5443\u0022}, \u0022source\u0022: \u0022# Using pyplot\\nimport matplotlib.pyplot as plt\\nplt.figure(figsize=(20, 5))\\n\\n# i: index\\nfor i, col in enumerate(features.columns):\\n    # 3 plots here hence 1, 3\\n    plt.subplot(1, 3, i+1)\\n    x = data[col]\\n    y = prices\\n    plt.plot(x, y, \u0027o\u0027)\\n    # Create regression line\\n    plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\\n    plt.title(col)\\n    plt.xlabel(col)\\n    plt.ylabel(\u0027prices\u0027)\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u002213df265ec830bfe1179613c298b54f588e64d953\u0022}, \u0022source\u0022: \u0022----\\n\\n## Developing a Model\\nIn this second section of the project, we will develop the tools and techniques necessary for a model to make a prediction. Being able to make accurate evaluations of each model\u0027s performance through the use of these tools and techniques helps to greatly reinforce the confidence in our predictions.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u002207dbfdb1055a7927dece3449ea6feb8053069eb1\u0022}, \u0022source\u0022: \u0022### Implementation: Define a Performance Metric\\nIt is difficult to measure the quality of a given model without quantifying its performance over training and testing. This is typically done using some type of performance metric, whether it is through calculating some type of error, the goodness of fit, or some other useful measurement. For this project, we will be calculating the [*coefficient of determination*](http://stattrek.com/statistics/dictionary.aspx?definition=coefficient_of_determination), R\u003csup\u003e2\u003c/sup\u003e, to quantify our model\u0027s performance. The coefficient of determination for a model is a useful statistic in regression analysis, as it often describes how \\\u0022good\\\u0022 that model is at making predictions. \\n\\nThe values for R\u003csup\u003e2\u003c/sup\u003e range from 0 to 1, which captures the percentage of squared correlation between the predicted and actual values of the **target variable**. A model with an R\u003csup\u003e2\u003c/sup\u003e of 0 is no better than a model that always predicts the *mean* of the target variable, whereas a model with an R\u003csup\u003e2\u003c/sup\u003e of 1 perfectly predicts the target variable. Any value between 0 and 1 indicates what percentage of the target variable, using this model, can be explained by the **features**. _A model can be given a negative R\u003csup\u003e2\u003c/sup\u003e as well, which indicates that the model is **arbitrarily worse** than one that always predicts the mean of the target variable._\\n\\nFor the `performance_metric` function in the code cell below, we will need to implement the following:\\n- Use `r2_score` from `sklearn.metrics` to perform a performance calculation between `y_true` and `y_predict`.\\n- Assign the performance score to the `score` variable.\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022d268b4e29e08e739f91e0189da76e0cde2081635\u0022}, \u0022source\u0022: \u0022# TODO: Import \u0027r2_score\u0027\\n\\ndef performance_metric(y_true, y_predict):\\n    \\\u0022\\\u0022\\\u0022 Calculates and returns the performance score between \\n        true and predicted values based on the metric chosen. \\\u0022\\\u0022\\\u0022\\n    \\n    # TODO: Calculate the performance score between \u0027y_true\u0027 and \u0027y_predict\u0027\\n    from sklearn.metrics import r2_score\\n    score = r2_score(y_true, y_predict)\\n    \\n    # Return the score\\n    return score\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022b3bd35934d4d70cb869e857a2d53eeafe3614c08\u0022}, \u0022source\u0022: \u0022### Question 2 - Goodness of Fit\\nAssume that a dataset contains five data points and a model made the following predictions for the target variable:\\n\\n| True Value | Prediction |\\n| :----------: | :--------: |\\n| 3.0 | 2.5 |\\n| -0.5 | 0.0 |\\n| 2.0 | 2.1 |\\n| 7.0 | 7.8 |\\n| 4.2 | 5.3 |\\n\\nRun the code cell below to use the `performance_metric` function and calculate this model\u0027s coefficient of determination.\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u002244edc96225a6f25ae990780980fb57e27d7cdc15\u0022}, \u0022source\u0022: \u0022# Calculate the performance of this model\\nscore = performance_metric([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\\nprint(\\\u0022Model has a coefficient of determination, R^2, of {:.3f}.\\\u0022.format(score))\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u002278c19a1ced1b900ccb4c9b09b372d71a6012513c\u0022}, \u0022source\u0022: \u0022### Visualization\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022ba68132b4609872212187e36ce3fbb70725001fd\u0022}, \u0022source\u0022: \u0022import numpy as np\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\ntrue, pred = [3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3]\\n\\n#Plot true values\\ntrue_handle = plt.scatter(true, true, alpha=0.6, color=\u0027blue\u0027, label=\u0027true\u0027)\\n\\n#Reference line\\nfit = np.poly1d(np.polyfit(true,true,1))\\nlims = np.linspace(min(true) - 1, max(true) + 1)\\nplt.plot(lims, fit(lims), alpha=0.3, color=\u0027black\u0027)\\n\\n#Plot predicted values\\npred_handle = plt.scatter(true, pred, alpha=0.6, color=\u0027red\u0027, label=\u0027predicted\u0027)\\n\\n#Legend and show\\nplt.legend(handles=[true_handle,pred_handle], loc=\u0027upper left\u0027)\\nplt.show()\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022e2b8c7d0c0f96621ea60b2b8ab6ce82bb5ddd30f\u0022}, \u0022source\u0022: \u0022* Would you consider this model to have successfully captured the variation of the target variable? \\n* Why or why not?\\n\\n** Hint: **  The R2 score is the proportion of the variance in the dependent variable that is predictable from the independent variable. In other words:\\n* R2 score of 0 means that the dependent variable cannot be predicted from the independent variable.\\n* R2 score of 1 means the dependent variable can be predicted from the independent variable.\\n* R2 score between 0 and 1 indicates the extent to which the dependent variable is predictable. An \\n* R2 score of 0.40 means that 40 percent of the variance in Y is predictable from X.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022dbac254833e0f72dc232e73087e4c29433b4de8b\u0022}, \u0022source\u0022: \u0022**Answer:** Yes, this model has successfully captured the variation of the target variable. This is because we are getting a very high R2 value of 0.923. That means 92.3% of the variance in the True Value is predictable from the Prediction. As this is a very high percentage, we can call this model to be a successful model.\\n\\nThe only drawback is there are only 5 datapoints here. So this might not be statistically significant. Another caveat is that whether the model is successful also depends largely on the application. So for some projects 0.923 is sufficient, whereas for others it could be a low score.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022acd84f7818208f81b8b0c63b129bf0a26a184b58\u0022}, \u0022source\u0022: \u0022### Implementation: Shuffle and Split Data\\nOur next implementation requires that we take the Boston housing dataset and split the data into training and testing subsets. Typically, the data is also shuffled into a random order when creating the training and testing subsets to remove any bias in the ordering of the dataset.\\n\\nFor the code cell below, we will need to implement the following:\\n- Use `train_test_split` from `sklearn.cross_validation` to shuffle and split the `features` and `prices` data into training and testing sets.\\n  - Split the data into 80% training and 20% testing.\\n  - Set the `random_state` for `train_test_split` to a value of your choice. This ensures results are consistent.\\n- Assign the train and testing splits to `X_train`, `X_test`, `y_train`, and `y_test`.\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u00224df0e9e910b51813cd2668282048e638ca0f4350\u0022}, \u0022source\u0022: \u0022# TODO: Import \u0027train_test_split\u0027\\nfrom sklearn import cross_validation\\n\\n# TODO: Shuffle and split the data into training and testing subsets\\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(features, prices, test_size = 0.2, random_state = 42)\\n\\n# Success\\nprint(\\\u0022Training and testing split was successful.\\\u0022)\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u00227d79ec8433e24d85ad0393eeac9aa3353d56d890\u0022}, \u0022source\u0022: \u0022### Question 3 - Training and Testing\\n\\n* What is the benefit to splitting a dataset into some ratio of training and testing subsets for a learning algorithm?\\n\\n**Hint:** Think about how overfitting or underfitting is contingent upon how splits on data is done.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u00220d8459d1d73bbf4d311a5ee2756494230d05abd5\u0022}, \u0022source\u0022: \u0022**Answer: ** A possible alternative to splitting a dataset into training and testing data would be to train and test on the same data. But that creates a problem. Here there is a very high chance of getting a high variance model which may eventually lead to a 100% accuracy rate with addition of new features, but that\u0027s only because it is overfitting the data. It has developed such a complex model that it will have limited or no ability to generalize data and so when we use that model on unknown data, it will give us very very low accuracy. So to avoid that, we can split the data into training and testing sets and train the model on the training data. Then the testing accuracy is a much better estimate than the training accuracy. \\n\\nBut then, the split might create a problem too. If we have a very limited dataset, then even if we take out a small sample of it as testing data, then also , we are losing a portion of the data. So there\u0027s an inherent trade off here which might cause underfitting due to limited datasets. This is where we can take advantage of K-fold cross validation where we divide all the datapoints into k number of bins and then run k separate learning experiments. In each of those, we pick one of those k subsets as our testing set and the remaining k-1 bins as our training sets. This is how we can maximize the machine\u0027s learning experiment.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u00225087a8047d0c16736c01f9df6fd1590889871f55\u0022}, \u0022source\u0022: \u0022----\\n\\n## Analyzing Model Performance\\nIn this third section of the project, we\u0027ll take a look at several models\u0027 learning and testing performances on various subsets of training data. Additionally, we\u0027ll investigate one particular algorithm with an increasing `\u0027max_depth\u0027` parameter on the full training set to observe how model complexity affects performance. Graphing our model\u0027s performance based on varying criteria can be beneficial in the analysis process, such as visualizing behavior that may not have been apparent from the results alone.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022d880c42e3da181c932151496c59f1f4837be5431\u0022}, \u0022source\u0022: \u0022### Learning Curves\\nThe following code cell produces four graphs for a decision tree model with different maximum depths. Each graph visualizes the learning curves of the model for both training and testing as the size of the training set is increased. Note that the shaded region of a learning curve denotes the uncertainty of that curve (measured as the standard deviation). The model is scored on both the training and testing sets using R\u003csup\u003e2\u003c/sup\u003e, the coefficient of determination.  \\n\\nRun the code cell below and use these graphs to answer the following question.\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u00225626afc91763b60b38edf02f634ad2e26a5c07a9\u0022}, \u0022source\u0022: \u0022#Define the necessary functions for plotting\\n###########################################\\n# Suppress matplotlib user warnings\\n# Necessary for newer version of matplotlib\\nimport warnings\\nwarnings.filterwarnings(\\\u0022ignore\\\u0022, category = UserWarning, module = \\\u0022matplotlib\\\u0022)\\n#\\n# Display inline matplotlib plots with IPython\\nfrom IPython import get_ipython\\nget_ipython().run_line_magic(\u0027matplotlib\u0027, \u0027inline\u0027)\\n###########################################\\n\\nimport matplotlib.pyplot as pl\\nimport numpy as np\\nimport sklearn.learning_curve as curves\\nfrom sklearn.tree import DecisionTreeRegressor\\nfrom sklearn.cross_validation import ShuffleSplit, train_test_split\\n\\ndef ModelLearning(X, y):\\n    \\\u0022\\\u0022\\\u0022 Calculates the performance of several models with varying sizes of training data.\\n        The learning and testing scores for each model are then plotted. \\\u0022\\\u0022\\\u0022\\n    \\n    # Create 10 cross-validation sets for training and testing\\n    cv = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.2, random_state = 0)\\n\\n    # Generate the training set sizes increasing by 50\\n    train_sizes = np.rint(np.linspace(1, X.shape[0]*0.8 - 1, 9)).astype(int)\\n\\n    # Create the figure window\\n    fig = pl.figure(figsize=(10,7))\\n\\n    # Create three different models based on max_depth\\n    for k, depth in enumerate([1,3,6,10]):\\n        \\n        # Create a Decision tree regressor at max_depth = depth\\n        regressor = DecisionTreeRegressor(max_depth = depth)\\n\\n        # Calculate the training and testing scores\\n        sizes, train_scores, test_scores = curves.learning_curve(regressor, X, y, \\\\\\n            cv = cv, train_sizes = train_sizes, scoring = \u0027r2\u0027)\\n        \\n        # Find the mean and standard deviation for smoothing\\n        train_std = np.std(train_scores, axis = 1)\\n        train_mean = np.mean(train_scores, axis = 1)\\n        test_std = np.std(test_scores, axis = 1)\\n        test_mean = np.mean(test_scores, axis = 1)\\n\\n        # Subplot the learning curve \\n        ax = fig.add_subplot(2, 2, k+1)\\n        ax.plot(sizes, train_mean, \u0027o-\u0027, color = \u0027r\u0027, label = \u0027Training Score\u0027)\\n        ax.plot(sizes, test_mean, \u0027o-\u0027, color = \u0027g\u0027, label = \u0027Testing Score\u0027)\\n        ax.fill_between(sizes, train_mean - train_std, \\\\\\n            train_mean + train_std, alpha = 0.15, color = \u0027r\u0027)\\n        ax.fill_between(sizes, test_mean - test_std, \\\\\\n            test_mean + test_std, alpha = 0.15, color = \u0027g\u0027)\\n        \\n        # Labels\\n        ax.set_title(\u0027max_depth = %s\u0027%(depth))\\n        ax.set_xlabel(\u0027Number of Training Points\u0027)\\n        ax.set_ylabel(\u0027Score\u0027)\\n        ax.set_xlim([0, X.shape[0]*0.8])\\n        ax.set_ylim([-0.05, 1.05])\\n    \\n    # Visual aesthetics\\n    ax.legend(bbox_to_anchor=(1.05, 2.05), loc=\u0027lower left\u0027, borderaxespad = 0.)\\n    fig.suptitle(\u0027Decision Tree Regressor Learning Performances\u0027, fontsize = 16, y = 1.03)\\n    fig.tight_layout()\\n    fig.show()\\n\\n\\ndef ModelComplexity(X, y):\\n    \\\u0022\\\u0022\\\u0022 Calculates the performance of the model as model complexity increases.\\n        The learning and testing errors rates are then plotted. \\\u0022\\\u0022\\\u0022\\n    \\n    # Create 10 cross-validation sets for training and testing\\n    cv = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.2, random_state = 0)\\n\\n    # Vary the max_depth parameter from 1 to 10\\n    max_depth = np.arange(1,11)\\n\\n    # Calculate the training and testing scores\\n    train_scores, test_scores = curves.validation_curve(DecisionTreeRegressor(), X, y, \\\\\\n        param_name = \\\u0022max_depth\\\u0022, param_range = max_depth, cv = cv, scoring = \u0027r2\u0027)\\n\\n    # Find the mean and standard deviation for smoothing\\n    train_mean = np.mean(train_scores, axis=1)\\n    train_std = np.std(train_scores, axis=1)\\n    test_mean = np.mean(test_scores, axis=1)\\n    test_std = np.std(test_scores, axis=1)\\n\\n    # Plot the validation curve\\n    pl.figure(figsize=(7, 5))\\n    pl.title(\u0027Decision Tree Regressor Complexity Performance\u0027)\\n    pl.plot(max_depth, train_mean, \u0027o-\u0027, color = \u0027r\u0027, label = \u0027Training Score\u0027)\\n    pl.plot(max_depth, test_mean, \u0027o-\u0027, color = \u0027g\u0027, label = \u0027Validation Score\u0027)\\n    pl.fill_between(max_depth, train_mean - train_std, \\\\\\n        train_mean + train_std, alpha = 0.15, color = \u0027r\u0027)\\n    pl.fill_between(max_depth, test_mean - test_std, \\\\\\n        test_mean + test_std, alpha = 0.15, color = \u0027g\u0027)\\n    \\n    # Visual aesthetics\\n    pl.legend(loc = \u0027lower right\u0027)\\n    pl.xlabel(\u0027Maximum Depth\u0027)\\n    pl.ylabel(\u0027Score\u0027)\\n    pl.ylim([-0.05,1.05])\\n    pl.show()\\n\\n\\ndef PredictTrials(X, y, fitter, data):\\n    \\\u0022\\\u0022\\\u0022 Performs trials of fitting and predicting data. \\\u0022\\\u0022\\\u0022\\n\\n    # Store the predicted prices\\n    prices = []\\n\\n    for k in range(10):\\n        # Split the data\\n        X_train, X_test, y_train, y_test = train_test_split(X, y, \\\\\\n            test_size = 0.2, random_state = k)\\n        \\n        # Fit the data\\n        reg = fitter(X_train, y_train)\\n        \\n        # Make a prediction\\n        pred = reg.predict([data[0]])[0]\\n        prices.append(pred)\\n        \\n        # Result\\n        print(\\\u0022Trial {}: ${:,.2f}\\\u0022.format(k+1, pred))\\n\\n    # Display price range\\n    print(\\\u0022\\\\nRange in prices: ${:,.2f}\\\u0022.format(max(prices) - min(prices)))\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u00224be0e269a3ea1d5a71b0831c9ab8652e4a495f6b\u0022}, \u0022source\u0022: \u0022# Produce learning curves for varying training set sizes and maximum depths\\nModelLearning(features, prices)\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022537d6af7237ea9f3f4cf34d9639c11bc6746d6e6\u0022}, \u0022source\u0022: \u0022### Question 4 - Learning the Data\\n* Choose one of the graphs above and state the maximum depth for the model. \\n* What happens to the score of the training curve as more training points are added? What about the testing curve? \\n* Would having more training points benefit the model? \\n\\n**Hint:** Are the learning curves converging to particular scores? Generally speaking, the more data you have, the better. But if your training and testing curves are converging with a score above your benchmark threshold, would this be necessary?\\nThink about the pros and cons of adding more training points based on if the training and testing curves are converging.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u00222bcdd955693b9270603f6a2deb3ccd82b608aa0a\u0022}, \u0022source\u0022: \u0022Answer:\\n\\nA) max_depth = 1 (High Bias Scenario):  We see that initially the Testing Score(green line)  increases with increase in number of training points. But then it plateaus at a very low accuracy score of 0.4 or 40% and increase in number of training points have no effect. This shows that the model does not generalize well on unseen data. On the other hand, the Training Score(red line)  decreases with increase in the number of training points and gets saturated at a score of approximately 0.4 or 40%. This shows that the model is actually underfitting the data and is not complex enough. In this scenario, adding more training points will not benefit the model. Instead, its complexity should be increased for better fitting the dataset.\\n\\nB)max_depth = 3 (Best scenario): Testing Score(green line) increases with increase in training points. It reaches a pretty high score of 0.8 and so we can see the model generalizes well. The Training Score(red line) decreases slightly and reaches 0.8 and stays constant. So we see it fits the model well and reaches a pretty high score. The testing score has two significant phases where the rates of change are different. One is the positive rate of change which goes on uptil approximately 200 training points (within this positive rate of change, we again observe two different rates. One is uptil 50 training points where the rate of increase is very high.The other is between 50 - 200 where the rate of increase is much lower.) and the other is the region where it plateaus with no/very little rate of change which is beyond 200 training points. So if we are below 200 training points, adding more training points will definitely improve the score but beyond that adding more training points will not be very useful as the rate plateaus.\\n\\nC) max_depth = 6 (High Variance Scenario): Testing Score(green line) increases with increase in training points and reaches 0.7. Even though this is not a bad accuracy, it is not generalizing the data as well as max_depth = 3. The Training Score(red line) decrease ever so slightly and stays at 0.9 which is a big sign that it is overfitting the data. It is a High Variance problem. Here also, the testing score show a similar behaviour as the previous one (it plateaus after 200 training points). So once again, we will get an improvement in the testing score by adding more training points when the nuber of training points is less than 200, but after that adding more training points will not benefit us much.\\n\\nD) max_depth = 10 (Higher Variance Scenario): Testing Score(green line) increases with increase in training points and reaches 0.7. So same problem as the previous one. It is not generalizing the data as well as scenario B). The  Training Score(red line) remains constant throughout showing a perfect accuracy of 100% or a score of 1 which tells us it is definitely overfitting the data. This is also a very High Variance problem. Once again the curve show exactly the same behaviour where adding more training points upto 200 will increase the score but not beyond that.\\n\\n\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022af253272e47160270847f617a581ccb6c1b55dd8\u0022}, \u0022source\u0022: \u0022### Complexity Curves\\nThe following code cell produces a graph for a decision tree model that has been trained and validated on the training data using different maximum depths. The graph produces two complexity curves \\u2014 one for training and one for validation. Similar to the **learning curves**, the shaded regions of both the complexity curves denote the uncertainty in those curves, and the model is scored on both the training and validation sets using the `performance_metric` function.  \\n\\n** Run the code cell below and use this graph to answer the following two questions Q5 and Q6. **\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022ce7909fa56ac27f6e4327c4a44156eadfac63218\u0022}, \u0022source\u0022: \u0022ModelComplexity(X_train, y_train)\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u00222b4e1477c3bdeb1afe152d657cecf817a5eca097\u0022}, \u0022source\u0022: \u0022### Question 5 - Bias-Variance Tradeoff\\n* When the model is trained with a maximum depth of 1, does the model suffer from high bias or from high variance? \\n* How about when the model is trained with a maximum depth of 10? What visual cues in the graph justify your conclusions?\\n\\n**Hint:** High bias is a sign of underfitting(model is not complex enough to pick up the nuances in the data) and high variance is a sign of overfitting(model is by-hearting the data and cannot generalize well). Think about which model(depth 1 or 10) aligns with which part of the tradeoff.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u00229ecedaca5241dcfbb0b9c5ea8c1f30a86aef2fbc\u0022}, \u0022source\u0022: \u0022Answer: We can easily recognize a problem related to High Bias or High Variance by simply looking at the graph of training and testing scores.\\n\\nIf there is High Bias, there will be very little gap between Training and Testing Scores. This is because in High Bias scenarios, the model underfits the data and also cannot generalize the data well resulting in both curves converging to a low score.\\n\\nIf there is High Variance, there will be a large gap between the Training and Testing Scores. This is because in High Variance model, even though the model fits well, it does not generalize well as a result of overfitting. This leads to a high Training Score but a relatively low Testing/Validation Score.\\n\\nA) Maximum Depth = 1 (High Bias): Here both Training and Testing Scores are low. So the model is not fitting well and so it is not generalizing well. Thus the two curves are very close to each other and hence this is a High Bias situation.\\n\\nB) Maximum Depth = 10 (High Variance): Here there is a huge gap between Training and Testing Scores. The Training score is almost perfect at 1, but the testing score is much low at around 0.7. So the model is overfitting and hence does not generalize well resulting in a lower Validation Score. So this is a High Variance situation with the curves being far apart. \u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u00221bd38765a62ff32c2c76a663ad4663b80e35124d\u0022}, \u0022source\u0022: \u0022### Question 6 - Best-Guess Optimal Model\\n* Which maximum depth do you think results in a model that best generalizes to unseen data? \\n* What intuition lead you to this answer?\\n\\n** Hint: ** Look at the graph above Question 5 and see where the validation scores lie for the various depths that have been assigned to the model. Does it get better with increased depth? At what point do we get our best validation score without overcomplicating our model? And remember, Occams Razor states \\\u0022Among competing hypotheses, the one with the fewest assumptions should be selected.\\\u0022\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u002212a49303b9a7fa2d6135e389a840675208116f54\u0022}, \u0022source\u0022: \u0022Answer:  Maximum Depth = 4\\n\\nThe validation score seems to plateau here. So this is the highest validation score we can get i.e best generalization of unseen data.\\n\\nThe gap between the Training Score and the Validation Score is not significantly large here too which indicates a High Variance Situation.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022490f25286452dba44fab91dd71764bb788cbe5ea\u0022}, \u0022source\u0022: \u0022-----\\n\\n## Evaluating Model Performance\\nIn this final section of the project, we will construct a model and make a prediction on the client\u0027s feature set using an optimized model from `fit_model`.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u002294519eb50d606e448e8159ec721eb1ccfcbef6d1\u0022}, \u0022source\u0022: \u0022### Question 7 - Grid Search\\n* What is the grid search technique?\\n* How it can be applied to optimize a learning algorithm?\\n\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u002291ad40fe293f9c6caa0fb8cbadf09886f22f4885\u0022}, \u0022source\u0022: \u0022**Answer: ** The Grid search technique allows us to define a grid of the hyperparameters for a specific classifier and then the Grid search technique exhaustively tries out every possible combinations of the hyperparameters values in order to find the best model. After that we can use cross validation techniques like K-fold cross validation or Stratified Shuffle Split to find the highest accuracy by using the hyperparameters suggested by Grid Search technique optimizing the learning algorithm.\\n\\n** Point to Note: ** Due to its exhaustive search nature, grid search can be computationally expensive, especially when data size is large and model is complicated. Sometimes we resort to randomized search in this case to search only some combinations of the parameters. \\n(http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html#sklearn-grid-search-randomizedsearchcv)\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022bcd08529ff2f0047c03073c28b86536624c98859\u0022}, \u0022source\u0022: \u0022### Question 8 - Cross-Validation\\n\\n* What is the k-fold cross-validation training technique? \\n\\n* What benefit does this technique provide for grid search when optimizing a model?\\n\\n**Hint:** When explaining the k-fold cross validation technique, be sure to touch upon what \u0027k\u0027 is, how the dataset is split into different parts for training and testing and the number of times it is run based on the \u0027k\u0027 value.\\n\\nWhen thinking about how k-fold cross validation helps grid search, think about the main drawbacks of grid search which are hinged upon **using a particular subset of data for training or testing** and how k-fold cv could help alleviate that. You can refer to the [docs](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation) for your answer.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022188d1e87e6647bd01915781c6df6badd94f49569\u0022}, \u0022source\u0022: \u0022**Answer: ** In K-fold cross validation technique, we partition the data into k-bins of equal size. After that we run k separate learning experiments. In each of those, we pick one of the k subsets as our testing set. The remaining k-1 bins are put together into the training set. Then we train our machine learning algorithm and just like before test the performance on the testing set. The key thing in cross validation is we run this multiple times (k times) and then we average the k different testing set performances for the k different hold out sets. So we average the test results from those k experiments. So obviously this takes more computation time as now we have to run k separate learning experiments, but the assessment of the learning algorithm will be more accurate.\\n\\nIf we run Grid Search without running a cross validation set, we will have different sets of optimal hyperparameters because without a cross validation set, the estimate of out-of-sample performance would have a high variance. \\n\\nSo in summary, without k-fold cross validation, the Grid Search will select hyper parameter values which works really well on the sample train test split data but there is a high risk that it will work poorly for unknown datasets because of high variance.\\n\\n\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022caa9510684ac27b4e6a4b8a562f72c47ec3e588b\u0022}, \u0022source\u0022: \u0022### Implementation: Fitting a Model\\nOur final implementation requires that we bring everything together and train a model using the **decision tree algorithm**. To ensure that we are producing an optimized model, we will train the model using the grid search technique to optimize the `\u0027max_depth\u0027` parameter for the decision tree. The `\u0027max_depth\u0027` parameter can be thought of as how many questions the decision tree algorithm is allowed to ask about the data before making a prediction. Decision trees are part of a class of algorithms called *supervised learning algorithms*.\\n\\nIn addition, we will find our implementation is using `ShuffleSplit()` for an alternative form of cross-validation (see the `\u0027cv_sets\u0027` variable). While it is not the K-Fold cross-validation technique you describe in **Question 8**, this type of cross-validation technique is just as useful!. The `ShuffleSplit()` implementation below will create 10 (`\u0027n_splits\u0027`) shuffled sets, and for each shuffle, 20% (`\u0027test_size\u0027`) of the data will be used as the *validation set*. While we\u0027re working on our implementation, we\u0027ll think about the contrasts and similarities it has to the K-fold cross-validation technique.\\n\\nPlease note that ShuffleSplit has different parameters in scikit-learn versions 0.17 and 0.18.\\nFor the `fit_model` function in the code cell below, we will need to implement the following:\\n- Use [`DecisionTreeRegressor`](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) from `sklearn.tree` to create a decision tree regressor object.\\n  - Assign this object to the `\u0027regressor\u0027` variable.\\n- Create a dictionary for `\u0027max_depth\u0027` with the values from 1 to 10, and assign this to the `\u0027params\u0027` variable.\\n- Use [`make_scorer`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html) from `sklearn.metrics` to create a scoring function object.\\n  - Pass the `performance_metric` function as a parameter to the object.\\n  - Assign this scoring function to the `\u0027scoring_fnc\u0027` variable.\\n- Use [`GridSearchCV`](http://scikit-learn.org/0.17/modules/generated/sklearn.grid_search.GridSearchCV.html) from `sklearn.grid_search` to create a grid search object.\\n  - Pass the variables `\u0027regressor\u0027`, `\u0027params\u0027`, `\u0027scoring_fnc\u0027`, and `\u0027cv_sets\u0027` as parameters to the object. \\n  - Assign the `GridSearchCV` object to the `\u0027grid\u0027` variable.\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022de5e7d278411c525d71bbfd198f0c310d3dd8f46\u0022}, \u0022source\u0022: \u0022# TODO: Import \u0027make_scorer\u0027, \u0027DecisionTreeRegressor\u0027, and \u0027GridSearchCV\u0027\\nfrom sklearn.tree import DecisionTreeRegressor\\nfrom sklearn.metrics import make_scorer\\nfrom sklearn.grid_search import GridSearchCV\\n\\ndef fit_model(X, y):\\n    \\\u0022\\\u0022\\\u0022 Performs grid search over the \u0027max_depth\u0027 parameter for a \\n        decision tree regressor trained on the input data [X, y]. \\\u0022\\\u0022\\\u0022\\n    \\n    # Create cross-validation sets from the training data\\n    # sklearn version 0.18: ShuffleSplit(n_splits=10, test_size=0.1, train_size=None, random_state=None)\\n    # sklearn versiin 0.17: ShuffleSplit(n, n_iter=10, test_size=0.1, train_size=None, random_state=None)\\n    cv_sets = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.20, random_state = 0)\\n\\n    # TODO: Create a decision tree regressor object\\n    \\n    regressor = DecisionTreeRegressor(random_state = 1001)\\n\\n    # TODO: Create a dictionary for the parameter \u0027max_depth\u0027 with a range from 1 to 10\\n    tree_range = range(1, 11)\\n    params = dict(max_depth=[1,2,3,4,5,6,7,8,9,10])\\n\\n    # TODO: Transform \u0027performance_metric\u0027 into a scoring function using \u0027make_scorer\u0027 \\n    scoring_fnc = make_scorer(performance_metric)\\n\\n    # TODO: Create the grid search cv object --\u003e GridSearchCV()\\n    # Make sure to include the right parameters in the object:\\n    # (estimator, param_grid, scoring, cv) which have values \u0027regressor\u0027, \u0027params\u0027, \u0027scoring_fnc\u0027, and \u0027cv_sets\u0027 respectively.\\n    grid = GridSearchCV(regressor,params,scoring=scoring_fnc,cv=cv_sets)\\n\\n    # Fit the grid search object to the data to compute the optimal model\\n    grid = grid.fit(X, y)\\n\\n    # Return the optimal model after fitting the data\\n    return grid.best_estimator_\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u002236ee52bb6d430084c3f0a2a4e0545fff99916316\u0022}, \u0022source\u0022: \u0022### Making Predictions\\nOnce a model has been trained on a given set of data, it can now be used to make predictions on new sets of input data. In the case of a *decision tree regressor*, the model has learned *what the best questions to ask about the input data are*, and can respond with a prediction for the **target variable**. We can use these predictions to gain information about data where the value of the target variable is unknown \\u2014 such as data the model was not trained on.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u00220aaf4b8bf51630a96abf85b00b467feaf19536a6\u0022}, \u0022source\u0022: \u0022### Question 9 - Optimal Model\\n\\n* What maximum depth does the optimal model have? How does this result compare to your guess in **Question 6**?  \\n\\nRun the code block below to fit the decision tree regressor to the training data and produce an optimal model.\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022e952e6e5557ed258fbfdf0dc5cf83b9a481a779c\u0022}, \u0022source\u0022: \u0022# Fit the training data to the model using grid search\\nreg = fit_model(X_train, y_train)\\n\\n# Produce the value for \u0027max_depth\u0027\\nprint(\\\u0022Parameter \u0027max_depth\u0027 is {} for the optimal model.\\\u0022.format(reg.get_params()[\u0027max_depth\u0027]))\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022c5c8354e3415694e1dfa5a77d45aeabc32a0cc5b\u0022}, \u0022source\u0022: \u0022** Hint: ** The answer comes from the output of the code snipped above.\\n\\n**Answer: ** The optimum model has a maximum depth of 4. This exactly matches our guess from ** Question 6 **. Both results are reliable as in both cases, we did cross validation with Shufflesplit combined with checking against a range of the max_depth hyperparamters to give us the most optimal value of the max_depth. So based on our course of action, there is very little chance that our model will work poorly for unknown datasets because of high variance.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u002279b82a252a87655e7014c3e0d856bd77e0599bc7\u0022}, \u0022source\u0022: \u0022### Question 10 - Predicting Selling Prices\\nImagine that we were a real estate agent in the Boston area looking to use this model to help price homes owned by our clients that they wish to sell. We have collected the following information from three of our clients:\\n\\n| Feature | Client 1 | Client 2 | Client 3 |\\n| :---: | :---: | :---: | :---: |\\n| Total number of rooms in home | 5 rooms | 4 rooms | 8 rooms |\\n| Neighborhood poverty level (as %) | 17% | 32% | 3% |\\n| Student-teacher ratio of nearby schools | 15-to-1 | 22-to-1 | 12-to-1 |\\n\\n* What price would you recommend each client sell his/her home at? \\n* Do these prices seem reasonable given the values for the respective features? \\n\\n**Hint:** Use the statistics you calculated in the **Data Exploration** section to help justify your response.  Of the three clients, client 3 has has the biggest house, in the best public school neighborhood with the lowest poverty level; while client 2 has the smallest house, in a neighborhood with a relatively high poverty rate and not the best public schools.\\n\\nRun the code block below to have your optimized model make predictions for each client\u0027s home.\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022353f3ccdba3e93361baf3a1b13e932f3f15986a1\u0022}, \u0022source\u0022: \u0022# Produce a matrix for client data\\nclient_data = [[5, 17, 15], # Client 1\\n               [4, 32, 22], # Client 2\\n               [8, 3, 12]]  # Client 3\\n\\n# Show predictions\\nfor i, price in enumerate(reg.predict(client_data)):\\n    print(\\\u0022Predicted selling price for Client {}\u0027s home: ${:,.2f}\\\u0022.format(i+1, price))\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022a145c5bf4700f486ac330e20cfbf7917a8f1c6c6\u0022}, \u0022source\u0022: \u0022### Visualization\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022cba2ba3b4c485cfa92132f3406b3fd5b3cf68a40\u0022}, \u0022source\u0022: \u0022from matplotlib import pyplot as plt\\n\\nclients = np.transpose(client_data)\\npred = reg.predict(client_data)\\nfor i, feat in enumerate([\u0027RM\u0027, \u0027LSTAT\u0027, \u0027PTRATIO\u0027]):\\n    plt.scatter(features[feat], prices, alpha=0.25, c=prices)\\n    plt.scatter(clients[i], pred, color=\u0027black\u0027, marker=\u0027x\u0027, linewidths=2)\\n    plt.xlabel(feat)\\n    plt.ylabel(\u0027MEDV\u0027)\\n    plt.show()\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022a8c32a11f476edc79bc39969cf3e20b114fe1bb4\u0022}, \u0022source\u0022: \u0022**Answer: **\\n\\nClient 1: $403,025.00\\n\\nClient 2: $237,478.72\\n\\nClient 3: $931,636.36\\n\\nIn our initial ** Data Exploration ** section, we saw that the price is positively correlated with the number of rooms and negatively correlated with Neighbourhood Poverty level and Student-teacher ratio of nearby schools. Also these were the statistics of our data.\\n\\nMinimum price: $105,000.00\\n\\nMaximum price: $1,024,800.00\\n\\nMean price: $454,342.94\\n\\nMedian price $438,900.00\\n\\nStandard deviation of prices: $165,340.28\\n\\nSo we see that for Client 1 and 2, the price of the house is below the median price of the houses. This is reasonable because of \\n\\na) High Poverty Level and Student to Teacher ratio for client 2.\\n\\nb) Average Poverty level and Student to Teacher ratio for client 1.\\n\\nFor Client 3, we see that the price is well over the median house price and very close to the maximum house price. This is also reasonable because of very low Poverty Level and Student to Teacher ratio and also a high number of rooms.\\n\\nSo overall, the prices for all the clients seem reasonable.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u002200218dfe3da410c11f049582b22e812cef1682a1\u0022}, \u0022source\u0022: \u0022### Perfomance Metric\\n\\nLet us calculate the R squared value for our model.\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u00220113d69f2528fd07047faea91626a2af55f8f865\u0022}, \u0022source\u0022: \u0022reg = fit_model(X_train, y_train)\\npred = reg.predict(X_test)\\nscore = performance_metric(y_test,pred)\\nprint(\\\u0022R Squared Value: \\\u0022 + str(score))\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022f7141377aa6c6225854a7dadad777b5f60ee450e\u0022}, \u0022source\u0022: \u0022So we get a pretty good R squared score from our model.\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022ac04d7e5a52fbeb60af4a17966d9561d8de82040\u0022}, \u0022source\u0022: \u0022### Visualization\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022b1f1fb15fab90119dbd804d0d7fc295ca5d92c3c\u0022}, \u0022source\u0022: \u0022import matplotlib.pyplot as plt\\nplt.hist(prices, bins = 20)\\nfor price in reg.predict(client_data):\\n    plt.axvline(price, lw = 5, c = \u0027r\u0027)\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022543f5237d479c96fc446c20091f8c020a4f89215\u0022}, \u0022source\u0022: \u0022### Sensitivity\\nAn optimal model is not necessarily a robust model. Sometimes, a model is either too complex or too simple to sufficiently generalize to new data. Sometimes, a model could use a learning algorithm that is not appropriate for the structure of the data given. Other times, the data itself could be too noisy or contain too few samples to allow a model to adequately capture the target variable \\u2014 i.e., the model is underfitted. \\n\\n**Run the code cell below to run the `fit_model` function ten times with different training and testing sets to see how the prediction for a specific client changes with respect to the data it\u0027s trained on.**\u0022}, {\u0022cell_type\u0022: \u0022code\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022bd7654c6a6310b40779b27f8612ca7e8c4119a63\u0022}, \u0022source\u0022: \u0022PredictTrials(features, prices, fit_model, client_data)\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u002225dde0d3d5d35ff3b8042ac3cdce10a53d36c51e\u0022}, \u0022source\u0022: \u0022### Question 11 - Applicability\\n\\n* In a few sentences, discuss whether the constructed model should or should not be used in a real-world setting.  \\n\\n**Hint:** Take a look at the range in prices as calculated in the code snippet above. Some questions to answering:\\n- How relevant today is data that was collected from 1978? How important is inflation?\\n- Are the features present in the data sufficient to describe a home? Do you think factors like quality of apppliances in the home, square feet of the plot area, presence of pool or not etc should factor in?\\n- Is the model robust enough to make consistent predictions?\\n- Would data collected in an urban city like Boston be applicable in a rural city?\\n- Is it fair to judge the price of an individual home based on the characteristics of the entire neighborhood?\u0022}, {\u0022cell_type\u0022: \u0022markdown\u0022, \u0022outputs\u0022: [], \u0022execution_count\u0022: null, \u0022metadata\u0022: {\u0022_execution_state\u0022: \u0022idle\u0022, \u0022collapsed\u0022: false, \u0022_uuid\u0022: \u0022c9e6e63584a79ec5e019f3f31599a7d52086b718\u0022}, \u0022source\u0022: \u0022**Answer: ** \\n\\n1) The data which was collected in 1978 is not so relevant today because demographics and economy has changed a lot since then.\\n\\n2) The features present in the data is not sufficient to describe a home. There are only three features present right now. We can add more features like crime rate, transportation avalibility, presence of pool or not, square feet of the plot area, quality of appliances, flooring in the home and more.\\n\\n3) This model based on its current feature is robust enough to make consistent predictions with a small margin of error.\\n\\n4) Data collected in an urban city like Boston may not be applicable in a rural city as many properties will change like the Demographics, Economy, Average income etc. So we would have to take in account a lot of other features in order to build an effective model\\n\\n5) Neighbourhood plays a very vital role in judging the price of a house like the crime rate, schools, transportation etc. But if an individual house has some marked characteristics which can overshadow the factors that neighbourhood plays, then it would not be fair to judge the price of an individual home based on the characteristics of the entire neighborhood.\u0022}], \u0022nbformat_minor\u0022: 0, \u0022nbformat\u0022: 4, \u0022metadata\u0022: {\u0022kernelspec\u0022: {\u0022name\u0022: \u0022python3\u0022, \u0022language\u0022: \u0022python\u0022, \u0022display_name\u0022: \u0022Python 3\u0022}, \u0022language_info\u0022: {\u0022nbconvert_exporter\u0022: \u0022python\u0022, \u0022pygments_lexer\u0022: \u0022ipython3\u0022, \u0022version\u0022: \u00223.6.1\u0022, \u0022name\u0022: \u0022python\u0022, \u0022file_extension\u0022: \u0022.py\u0022, \u0022mimetype\u0022: \u0022text/x-python\u0022, \u0022codemirror_mode\u0022: {\u0022name\u0022: \u0022ipython\u0022, \u0022version\u0022: 3}}}}","dateCreated":"2017-07-11T15:27:57.323Z"},"resources":null,"isolatorResults":"\u003cresults\u003e\u003cdisk_kb_free\u003e472572\u003c/disk_kb_free\u003e\u003cdocker_image_id\u003esha256:a99d687187793d17d7c1118beffbf67ddf8a800b4acdc86edef2f7c045ed5281\u003c/docker_image_id\u003e\u003cdocker_image_name\u003ekaggle/python\u003c/docker_image_name\u003e\u003cexit_code\u003e0\u003c/exit_code\u003e\u003cfailure_message /\u003e\u003cout_of_memory\u003eFalse\u003c/out_of_memory\u003e\u003crun_time_seconds\u003e2642.25961135101\u003c/run_time_seconds\u003e\u003csucceeded\u003eTrue\u003c/succeeded\u003e\u003ctimeout_exceeded\u003eFalse\u003c/timeout_exceeded\u003e\u003cused_all_space\u003eFalse\u003c/used_all_space\u003e\u003cwas_killed\u003eFalse\u003c/was_killed\u003e\u003c/results\u003e","runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-python/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/python/","dockerImageDigest":null,"dockerImageId":"sha256:a99d687187793d17d7c1118beffbf67ddf8a800b4acdc86edef2f7c045ed5281","dockerImageName":"kaggle/python","diskKbFree":472572,"failureMessage":"","exitCode":0,"queuedSeconds":0,"outputSizeBytes":0,"runTimeSeconds":2642.25961135101,"usedAllSpace":false,"timeoutExceeded":false,"isValidStatus":false,"wasGpuEnabled":false,"wasInternetEnabled":false,"outOfMemory":false,"invalidPathErrors":false,"succeeded":true,"wasKilled":false},"outputFilesTotalSizeBytes":624054,"dockerImageVersionId":null,"usedCustomDockerImage":false,"dataSources":[{"sourceType":"DatasetVersion","sourceId":2485,"databundleVersionId":null,"mountSlug":""}]},"author":{"id":1122229,"displayName":"Sagarnil Das","email":null,"editedEmail":null,"editedEmailCode":null,"userName":"sagarnildass","thumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1122229-gp.jpg","profileUrl":"/sagarnildass","registerDate":"0001-01-01T00:00:00Z","lastVisitDate":"0001-01-01T00:00:00Z","statusId":0,"performanceTier":1,"userRoles":null,"userLogins":null,"groupIds":null,"duplicateUsers":null,"hasPhoneVerifications":false,"failedNerdchas":0,"hasPendingNerdcha":false,"deleteRequests":null,"userAttributes":null,"isAdmin":false,"isTvc":false,"isKaggleBot":false,"isAdminOrTvc":false,"isAnonymous":false,"canAct":false,"canBeSeen":false,"thumbnailName":null,"activationCode":"00000000-0000-0000-0000-000000000000","isPhoneVerified":false},"baseUrl":"/sagarnildass/predicting-boston-house-prices","collaborators":{"owner":{"userId":1122229,"groupId":null,"groupMemberCount":null,"profileUrl":"/sagarnildass","thumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1122229-gp.jpg","name":"Sagarnil Das","slug":"sagarnildass","userTier":1,"joinDate":null,"type":"owner","isUser":true,"isGroup":false},"collaborators":[]},"initialTab":null,"log":"[{\n  \u0022data\u0022: \u0022[NbConvertApp] Converting notebook __notebook_source__.ipynb to html\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 2.3256556389969774\n},{\n  \u0022data\u0022: \u0022[NbConvertApp] ERROR | Notebook JSON is invalid: Additional properties are not allowed (\u0027execution_count\u0027, \u0027outputs\u0027 were unexpected)\\n\\nFailed validating \u0027additionalProperties\u0027 in markdown_cell:\\n\\nOn instance[\u0027cells\u0027][0]:\\n{\u0027cell_type\u0027: \u0027markdown\u0027,\\n \u0027execution_count\u0027: None,\\n \u0027metadata\u0027: {\u0027_execution_state\u0027: \u0027idle\u0027,\\n              \u0027_uuid\u0027: \u002783dcb1a7c0f2d6184105bf5fadfc95b845b1bb71\u0027,\\n              \u0027collapsed\u0027: False},\\n \u0027outputs\u0027: [\u0027...0 outputs...\u0027],\\n \u0027source\u0027: \u0027## Getting Started\\\\n\u0027\\n           \u0027In this project, we will evaluate the perform...\u0027}\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 2.3305412770132534\n},{\n  \u0022data\u0022: \u0022[NbConvertApp] Writing 354150 bytes to __results__.html\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 2.5681902460055426\n}{\n  \u0022data\u0022: \u0022[NbConvertApp] Converting notebook __notebook_source__.ipynb to notebook\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 2.3658412090153433\n},{\n  \u0022data\u0022: \u0022[NbConvertApp] ERROR | Notebook JSON is invalid: Additional properties are not allowed (\u0027outputs\u0027, \u0027execution_count\u0027 were unexpected)\\n\\nFailed validating \u0027additionalProperties\u0027 in markdown_cell:\\n\\nOn instance[\u0027cells\u0027][0]:\\n{\u0027cell_type\u0027: \u0027markdown\u0027,\\n \u0027execution_count\u0027: None,\\n \u0027metadata\u0027: {\u0027_execution_state\u0027: \u0027idle\u0027,\\n              \u0027_uuid\u0027: \u002783dcb1a7c0f2d6184105bf5fadfc95b845b1bb71\u0027,\\n              \u0027collapsed\u0027: False},\\n \u0027outputs\u0027: [\u0027...0 outputs...\u0027],\\n \u0027source\u0027: \u0027## Getting Started\\\\n\u0027\\n           \u0027In this project, we will evaluate the perform...\u0027}\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 2.371672483975999\n},{\n  \u0022data\u0022: \u0022[NbConvertApp] Executing notebook with kernel: python3\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 2.3788149249739945\n},{\n  \u0022data\u0022: \u0022Fontconfig warning: ignoring C.UTF-8: not a valid language tag\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 4.223236974969041\n},{\n  \u0022data\u0022: \u0022[NbConvertApp] ERROR | Notebook JSON is invalid: Additional properties are not allowed (\u0027outputs\u0027, \u0027execution_count\u0027 were unexpected)\\n\\nFailed validating \u0027additionalProperties\u0027 in markdown_cell:\\n\\nOn instance[\u0027cells\u0027][0]:\\n{\u0027cell_type\u0027: \u0027markdown\u0027,\\n \u0027execution_count\u0027: None,\\n \u0027metadata\u0027: {\u0027_execution_state\u0027: \u0027idle\u0027,\\n              \u0027_uuid\u0027: \u002783dcb1a7c0f2d6184105bf5fadfc95b845b1bb71\u0027,\\n              \u0027collapsed\u0027: False},\\n \u0027outputs\u0027: [\u0027...0 outputs...\u0027],\\n \u0027source\u0027: \u0027## Getting Started\\\\n\u0027\\n           \u0027In this project, we will evaluate the perform...\u0027}\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 17.173530521977227\n},{\n  \u0022data\u0022: \u0022[NbConvertApp] Writing 430107 bytes to __notebook__.ipynb\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 17.184823037998285\n}{\n  \u0022data\u0022: \u0022[NbConvertApp] Converting notebook __notebook__.ipynb to html\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 2.263581295032054\n},{\n  \u0022data\u0022: \u0022[NbConvertApp] ERROR | Notebook JSON is invalid: Additional properties are not allowed (\u0027outputs\u0027, \u0027execution_count\u0027 were unexpected)\\n\\nFailed validating \u0027additionalProperties\u0027 in markdown_cell:\\n\\nOn instance[\u0027cells\u0027][0]:\\n{\u0027cell_type\u0027: \u0027markdown\u0027,\\n \u0027execution_count\u0027: None,\\n \u0027metadata\u0027: {\u0027_execution_state\u0027: \u0027idle\u0027,\\n              \u0027_uuid\u0027: \u002783dcb1a7c0f2d6184105bf5fadfc95b845b1bb71\u0027,\\n              \u0027collapsed\u0027: False},\\n \u0027outputs\u0027: [\u0027...0 outputs...\u0027],\\n \u0027source\u0027: \u0027## Getting Started\\\\n\u0027\\n           \u0027In this project, we will evaluate the perform...\u0027}\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 2.2727416250272654\n},{\n  \u0022data\u0022: \u0022[NbConvertApp] Support files will be in __results___files/\\n[NbConvertApp] Making directory __results___files\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 2.5288285529823042\n},{\n  \u0022data\u0022: \u0022[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Writing 361283 bytes to __results__.html\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 2.532679330033716\n}","outputFiles":[],"outputFilesCropped":false,"ouputFilesOwnerInfo":{"databundleVersionId":0,"dataset":null,"competition":null,"kernel":{"kernelId":307985,"kernelVersionId":1332019,"dataviewToken":"eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..5tLB9l7HWbW7VMCpwpB0Gg.dOgF4bqjrRv5tlCrQjNez0uZ4IRqLegOTht9qKxV04tYzXFWpUVmnNOwSt2VhFm1MgwxHXo8n__mjHMdCza9zKWa-nM0cJrau-WYqroqIld6B-cEx2V-RB32Hv7XOE6vHibt2KjPkwjykap2xqEZF72W-78kE4iacmPwwtL9jeSo90sSzf7doT3tdjuKUQrQQqs7rwAxZ2hZxLlotDRIgxrEe2vPHNaBhuK15qnzlvFp888pv0SyrRuDp4-pqmSJizvqiBIh_6NitiaY5slqSe0ObZ_AL3gmZPhxb0_yez0QQ8tG_JAAIucpVniOrLrq_a2ZLqXdEnVbTLBe1iuI8esrfPvkCgHhRLuapPiPmlVRNxgHqANLFA15BO6Z2lunbZi1fop8v8EcT3LH53w3GYs6SXnhl3fZ7SpLCh46aAL96Ua55yfGmP6aYTeaYfDf-zur7NtwCYQ1iPajlHoxvFk4MmShqQBGBrq0RIQXga8.3Yhi_tCicrKRY_Q2csKUmA","scope":"sagarnildass/predicting-boston-house-prices"},"previewsDisabled":false},"pageMessages":[],"dataSources":[{"imageUrl":"https://storage.googleapis.com/kaggle-datasets-images/1379/2485/a52db2794593657403a4235bfc4147aa/dataset-thumbnail.jpg","sourceUrl":"/schirmerchad/bostonhoustingmlnd","slug":"bostonhoustingmlnd","lastUpdated":"2017-06-11T15:07:11.507Z","overview":"Concerns housing values in suburbs of Boston","sourceType":"dataset","sourceVersionType":"fileset","sourceId":2485,"sourceVersionNumber":1,"maxVersionNumber":1,"descriptionMimeType":"text/plain","deleted":false,"private":false,"privateButVisible":false,"ownerInfo":{"databundleVersionId":2485,"dataset":{"datasetId":1379,"datasetVersionId":2485,"dataviewToken":"eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..dKljLpLRubmubfilSXWJsA.wSFJ2ZVcApwCl_hpZ4XTQeUM6vggdnH5AruUzUWh71FyFumK9YgkG-uJWVEAp3j403fwO20OtRQqxJKJqAxZcPr-8Cp5IsIBrcHyvjhbrI0Wrae2qR2qIvwilrSO3K7xTLMFW20wSFeRq0zrtWDucJ2UlqQe7r-VTtp973iaYtzpIBCXjmkP310DNwJwsF8CZArPiNhKtg4dLReQ1n6aWvESwuwrg2tZ_X7x2MLt1tR7tx7QsL3y-tuCi0b9x1_ShpcsYfzCe66xQ-ATR6yQQiUpvZmekhv2SVQVou4YUWZMzkSQp1VEbmr3csgERv-eb8f4mUsGmABFHWDLoHw6I6Snkt1ZCVYhG1XTu23htNInoJLmKF3ZeupbBbMgde0PwM4d92vObcu8BG0Y109M8jDQktdboZhWPfR3FoECuorxLC23g3ZRkqxr-hzkiReXUmAbKDPEXDqfFaiYFU3ONO_tIizbHU91i5uys853OxJV8GXW3NJHyfBBWEcQvTtGk5O_V0z3n4hZe30C-5d_fg.USEN3dXyQIbwcxNRE3i_sw","scope":"schirmerchad/bostonhoustingmlnd"},"competition":null,"kernel":null,"previewsDisabled":false},"mountSlug":"","type":"dataSource","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[{"id":7638,"blobFileId":326999,"databundleVersionId":2485,"databundleVersionObjectType":"file","url":null,"relativePath":"../input/housing.csv","creationDate":"2017-06-11T15:06:53Z","isDummy":false,"size":12435,"fullPath":"../input/housing.csv","previewUrl":null,"downloadUrl":"/schirmerchad/bostonhoustingmlnd/downloads/housing.csv/1","fileType":".csv","contentLength":12435,"contentType":"application/vnd.ms-excel","contentMD5":"4Dq06blouexGAT1hpoedQA==","validationErrors":null,"type":"databundleVersionObject","collapsed":false,"info":{"metrics":{"tableMetrics":{"exception":null,"rowCount":489},"columnMetrics":[]},"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":null,"render":null,"children":[{"columns":[{"order":0,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":{"metrics":{"finiteCount":489,"infiniteCount":0,"mean":6.2402883435582908,"standardDeviation":0.64299129735443084,"minimum":3.561,"minimumFinite":3.561,"maximum":8.398,"maximumFinite":3.561,"quantiles":[{"point":0.25,"value":5.88},{"point":0.5,"value":6.185},{"point":0.75,"value":6.575}],"histogram":{"buckets":[{"index":0,"label":"-Infinity - 4.04","leftValue":"-Infinity","rightValue":4.0447,"count":2},{"index":1,"label":"4.04 - 4.53","leftValue":4.0447,"rightValue":4.5283999999999995,"count":4},{"index":2,"label":"4.53 - 5.01","leftValue":4.5283999999999995,"rightValue":5.0121,"count":10},{"index":3,"label":"5.01 - 5.50","leftValue":5.0121,"rightValue":5.4958,"count":25},{"index":4,"label":"5.50 - 5.98","leftValue":5.4958,"rightValue":5.9795,"count":122},{"index":5,"label":"5.98 - 6.46","leftValue":5.9795,"rightValue":6.4632,"count":178},{"index":6,"label":"6.46 - 6.95","leftValue":6.4632,"rightValue":6.9468999999999994,"count":86},{"index":7,"label":"6.95 - 7.43","leftValue":6.9468999999999994,"rightValue":7.4306,"count":44},{"index":8,"label":"7.43 - 7.91","leftValue":7.4306,"rightValue":7.9143,"count":11},{"index":9,"label":"7.91 - Infinity","leftValue":7.9143,"rightValue":"Infinity","count":7}],"type":"numeric"},"column":{"order":0,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":null,"info":null,"firestorePath":null,"firestoreKey":null,"name":"RM","description":null},"type":"numeric","extendedType":null,"exception":null,"nullCount":0,"nonNullCount":489,"validCount":489,"invalidCount":0}},"firestorePath":null,"firestoreKey":null,"name":"RM","description":null},{"order":1,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":{"metrics":{"finiteCount":489,"infiniteCount":0,"mean":12.939631901840492,"standardDeviation":7.0747447848268168,"minimum":1.98,"minimumFinite":1.98,"maximum":37.97,"maximumFinite":1.98,"quantiles":[{"point":0.25,"value":7.37},{"point":0.5,"value":11.69},{"point":0.75,"value":17.12}],"histogram":{"buckets":[{"index":0,"label":"-Infinity - 5.58","leftValue":"-Infinity","rightValue":5.5790000000000006,"count":68},{"index":1,"label":"5.58 - 9.18","leftValue":5.5790000000000006,"rightValue":9.178,"count":105},{"index":2,"label":"9.18 - 12.78","leftValue":9.178,"rightValue":12.777000000000001,"count":96},{"index":3,"label":"12.78 - 16.38","leftValue":12.777000000000001,"rightValue":16.376,"count":84},{"index":4,"label":"16.38 - 19.98","leftValue":16.376,"rightValue":19.975,"count":62},{"index":5,"label":"19.98 - 23.57","leftValue":19.975,"rightValue":23.574,"count":28},{"index":6,"label":"23.57 - 27.17","leftValue":23.574,"rightValue":27.173000000000002,"count":21},{"index":7,"label":"27.17 - 30.77","leftValue":27.173000000000002,"rightValue":30.772000000000002,"count":16},{"index":8,"label":"30.77 - 34.37","leftValue":30.772000000000002,"rightValue":34.371,"count":5},{"index":9,"label":"34.37 - Infinity","leftValue":34.371,"rightValue":"Infinity","count":4}],"type":"numeric"},"column":{"order":1,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":null,"info":null,"firestorePath":null,"firestoreKey":null,"name":"LSTAT","description":null},"type":"numeric","extendedType":null,"exception":null,"nullCount":0,"nonNullCount":489,"validCount":489,"invalidCount":0}},"firestorePath":null,"firestoreKey":null,"name":"LSTAT","description":null},{"order":2,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":{"metrics":{"finiteCount":489,"infiniteCount":0,"mean":18.516564417177921,"standardDeviation":2.1091076376127682,"minimum":12.6,"minimumFinite":12.6,"maximum":22.0,"maximumFinite":12.6,"quantiles":[{"point":0.25,"value":17.4},{"point":0.5,"value":19.1},{"point":0.75,"value":20.2}],"histogram":{"buckets":[{"index":0,"label":"-Infinity - 13.54","leftValue":"-Infinity","rightValue":13.54,"count":13},{"index":1,"label":"13.54 - 14.48","leftValue":13.54,"rightValue":14.48,"count":0},{"index":2,"label":"14.48 - 15.42","leftValue":14.48,"rightValue":15.42,"count":53},{"index":3,"label":"15.42 - 16.36","leftValue":15.42,"rightValue":16.36,"count":15},{"index":4,"label":"16.36 - 17.30","leftValue":16.36,"rightValue":17.3,"count":35},{"index":5,"label":"17.30 - 18.24","leftValue":17.3,"rightValue":18.240000000000002,"count":67},{"index":6,"label":"18.24 - 19.18","leftValue":18.240000000000002,"rightValue":19.18,"count":76},{"index":7,"label":"19.18 - 20.12","leftValue":19.18,"rightValue":20.12,"count":40},{"index":8,"label":"20.12 - 21.06","leftValue":20.12,"rightValue":21.060000000000002,"count":172},{"index":9,"label":"21.06 - Infinity","leftValue":21.060000000000002,"rightValue":"Infinity","count":18}],"type":"numeric"},"column":{"order":2,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":null,"info":null,"firestorePath":null,"firestoreKey":null,"name":"PTRATIO","description":null},"type":"numeric","extendedType":null,"exception":null,"nullCount":0,"nonNullCount":489,"validCount":489,"invalidCount":0}},"firestorePath":null,"firestoreKey":null,"name":"PTRATIO","description":null},{"order":3,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":{"metrics":{"finiteCount":489,"infiniteCount":0,"mean":454342.94478527608,"standardDeviation":165171.13154429477,"minimum":105000.0,"minimumFinite":105000.0,"maximum":1024800.0,"maximumFinite":105000.0,"quantiles":[{"point":0.25,"value":350700.0},{"point":0.5,"value":438900.0},{"point":0.75,"value":518700.0}],"histogram":{"buckets":[{"index":0,"label":"-Infinity - 196980.00","leftValue":"-Infinity","rightValue":196980.0,"count":21},{"index":1,"label":"196980.00 - 288960.00","leftValue":196980.0,"rightValue":288960.0,"count":48},{"index":2,"label":"288960.00 - 380940.00","leftValue":288960.0,"rightValue":380940.0,"count":81},{"index":3,"label":"380940.00 - 472920.00","leftValue":380940.0,"rightValue":472920.0,"count":146},{"index":4,"label":"472920.00 - 564900.00","leftValue":472920.0,"rightValue":564900.0,"count":96},{"index":5,"label":"564900.00 - 656880.00","leftValue":564900.0,"rightValue":656880.0,"count":39},{"index":6,"label":"656880.00 - 748860.00","leftValue":656880.0,"rightValue":748860.0,"count":30},{"index":7,"label":"748860.00 - 840840.00","leftValue":748860.0,"rightValue":840840.0,"count":13},{"index":8,"label":"840840.00 - 932820.00","leftValue":840840.0,"rightValue":932820.0,"count":8},{"index":9,"label":"932820.00 - Infinity","leftValue":932820.0,"rightValue":"Infinity","count":7}],"type":"numeric"},"column":{"order":3,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":null,"info":null,"firestorePath":null,"firestoreKey":null,"name":"MEDV","description":null},"type":"numeric","extendedType":null,"exception":null,"nullCount":0,"nonNullCount":489,"validCount":489,"invalidCount":0}},"firestorePath":null,"firestoreKey":null,"name":"MEDV","description":null}],"totalRows":null,"type":"genericTable","collapsed":true,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"firestorePath":null,"firestoreKey":null,"name":"","description":null}],"firestorePath":null,"firestoreKey":null,"name":"housing.csv","description":"housing data file"}],"firestorePath":null,"firestoreKey":null,"name":"Boston Housing","description":"### Context\n\nThe dataset for this project originates from the UCI Machine Learning Repository. The Boston housing data was collected in 1978 and each of the 506 entries represent aggregated data about 14 features for homes from various suburbs in Boston, Massachusetts.\n\n### Acknowledgements\n\nhttps://github.com/udacity/machine-learning\n\nhttps://archive.ics.uci.edu/ml/datasets/Housing"}],"versions":[{"id":1332019,"kernelVersionId":null,"isForkParent":false,"isNotebook":true,"languageName":"Python","lastRunTime":"2017-07-11T15:27:57.323Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":0,"linesInsertedFromPrevious":641,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-python/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/python/","dockerImageId":"sha256:a99d687187793d17d7c1118beffbf67ddf8a800b4acdc86edef2f7c045ed5281","dockerImageName":"kaggle/python","exitCode":0,"failureMessage":"","isValidStatus":true,"runTimeSeconds":2642.25961135101,"succeeded":true,"timeoutExceeded":false,"usedAllSpace":false},"status":"complete","title":"Predicting Boston House Prices","url":"/sagarnildass/predicting-boston-house-prices?scriptVersionId=1332019","versionNumber":1,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null}],"categories":{"categories":[],"type":"script"},"submitToCompetitionInfo":null,"downloadAllFilesUrl":"/kernels/svzip/1332019","submission":null,"menuLinks":[{"href":"/sagarnildass/predicting-boston-house-prices/notebook","text":"Notebook","title":"Notebook","tab":"notebook","count":null,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/sagarnildass/predicting-boston-house-prices/code","text":"Code","title":"Code","tab":"code","count":null,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/sagarnildass/predicting-boston-house-prices/data","text":"Data","title":"Data","tab":"data","count":1,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/sagarnildass/predicting-boston-house-prices/log","text":"Log","title":"Log","tab":"log","count":null,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/sagarnildass/predicting-boston-house-prices/comments","text":"Comments","title":"Comments","tab":"comments","count":0,"showZeroCountExplicitly":true,"reportEventCategory":null,"reportEventType":null}],"rightMenuLinks":[],"callToAction":{"href":"/kernels/fork-version/1332019","text":"Fork Notebook","title":"Fork Notebook","tab":null,"count":null,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},"voteButton":{"totalVotes":24,"hasAlreadyVotedUp":false,"hasAlreadyVotedDown":false,"canUpvote":true,"canDownvote":false,"voteUpUrl":"/kernels/vote?id=307985","voteDownUrl":null,"voters":[{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/149680-fb.jpg","displayName":"Albert Anthony D. Gavino","profileUrl":"/bertmanila","tier":"Contributor","tierInt":1,"userId":149680,"userName":"bertmanila"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"Ciao","profileUrl":"/asd956458817","tier":"Contributor","tierInt":1,"userId":864333,"userName":"asd956458817"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"Brandon Yong","profileUrl":"/brandonyongys","tier":"Novice","tierInt":0,"userId":951643,"userName":"brandonyongys"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/983908-kg.jpg","displayName":"Karthik Ravi","profileUrl":"/karthik28","tier":"Contributor","tierInt":1,"userId":983908,"userName":"karthik28"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1122229-gp.jpg","displayName":"Sagarnil Das","profileUrl":"/sagarnildass","tier":"Contributor","tierInt":1,"userId":1122229,"userName":"sagarnildass"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"Shawn_Chen","profileUrl":"/eric455265","tier":"Novice","tierInt":0,"userId":1344771,"userName":"eric455265"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1365260-kg.jpg","displayName":"Vanessa Ozogu","profileUrl":"/vanessao","tier":"Contributor","tierInt":1,"userId":1365260,"userName":"vanessao"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1427255-gp.jpg","displayName":"Junaid","profileUrl":"/junaid388","tier":"Novice","tierInt":0,"userId":1427255,"userName":"junaid388"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1649435-gp.jpg","displayName":"Harsha Rathi","profileUrl":"/harsharathi10","tier":"Novice","tierInt":0,"userId":1649435,"userName":"harsharathi10"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1860361-kg.png","displayName":"Pranab Sarkar","profileUrl":"/sarkarpranab66","tier":"Contributor","tierInt":1,"userId":1860361,"userName":"sarkarpranab66"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"ODosari","profileUrl":"/odosari","tier":"Novice","tierInt":0,"userId":2003867,"userName":"odosari"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/2087405-gp.jpg","displayName":"Venu","profileUrl":"/venupannala","tier":"Novice","tierInt":0,"userId":2087405,"userName":"venupannala"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/2119905-gr.jpg","displayName":"roboblob","profileUrl":"/roboblob","tier":"Novice","tierInt":0,"userId":2119905,"userName":"roboblob"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"niraj","profileUrl":"/nirajsinh","tier":"Novice","tierInt":0,"userId":2130548,"userName":"nirajsinh"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"murali","profileUrl":"/murali2020","tier":"Novice","tierInt":0,"userId":2180478,"userName":"murali2020"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/2207481-gp.jpg","displayName":"Adhish Kumar","profileUrl":"/adhishk1","tier":"Contributor","tierInt":1,"userId":2207481,"userName":"adhishk1"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"Luke Bianculli","profileUrl":"/lbianculli","tier":"Novice","tierInt":0,"userId":2236384,"userName":"lbianculli"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/2552141-gp.jpg","displayName":"Somesh","profileUrl":"/somrevaai","tier":"Novice","tierInt":0,"userId":2552141,"userName":"somrevaai"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/2592382-gp.jpg","displayName":"satyaaditya","profileUrl":"/satyaaditya","tier":"Novice","tierInt":0,"userId":2592382,"userName":"satyaaditya"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"darkmfj","profileUrl":"/darkmfj","tier":"Novice","tierInt":0,"userId":2989216,"userName":"darkmfj"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"yunsoo","profileUrl":"/lystar11","tier":"Novice","tierInt":0,"userId":3081985,"userName":"lystar11"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/3201630-fb.jpg","displayName":"SHIKHAR SAXENA","profileUrl":"/shikhar721","tier":"Novice","tierInt":0,"userId":3201630,"userName":"shikhar721"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"Pranav","profileUrl":"/pkanchi","tier":"Novice","tierInt":0,"userId":3359210,"userName":"pkanchi"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"unknown1234","profileUrl":"/unknown1234","tier":"Novice","tierInt":0,"userId":3406374,"userName":"unknown1234"}],"currentUserInfo":{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1050034-gp.jpg","displayName":"KentHsieh","userName":"insightseeker","profileUrl":"/insightseeker","tier":"Novice","tierInt":0,"userId":1050034},"showVoters":true,"alwaysShowVoters":true},"parentDataSource":null,"parentName":"Boston Housing","parentUrl":"/schirmerchad/bostonhoustingmlnd","thumbnailImageUrl":"https://storage.googleapis.com/kaggle-datasets-images/1379/2485/a52db2794593657403a4235bfc4147aa/dataset-thumbnail.jpg","canWrite":false,"canAdminister":false,"datasetHidden":false,"forkParentIsRedacted":false,"forkDiffLinesChanged":0,"forkDiffLinesDeleted":0,"forkDiffLinesInserted":0,"forkDiffUrl":null,"forkParentAuthorDisplayName":null,"forkParentAuthorUrl":null,"forkParentTitle":null,"forkParentUrl":null,"canSeeDataExplorerV2":true,"canSeeRevampedViewer":true,"canSeeInnerTableOfContents":true,"simplifiedViewer":false,"kernelOutputDataset":null});performance && performance.mark && performance.mark("KernelViewer.componentCouldBootstrap");</script>

<form action="https://www.kaggle.com/sagarnildass/predicting-boston-house-prices" id="__AjaxAntiForgeryForm" method="post"><input name="X-XSRF-TOKEN" type="hidden" value="CfDJ8LdUzqlsSWBPr4Ce3rb9VL9yu6tM7X7dz4JrC4CPpFlmrG3zdNFffn4kjkmzi2tYrcOK9Kcd5ELyH-7rux49L7gel9Rwnf9QTNL6rYxXT_KGZqn_3aqC-5UBIsgXJaQMcROnih5EutLrT1OUaEP7a4XpNFKgQXZg0Zs4_R0c7eGgb9QgZOL_RUYrty3JL5MRMA"></form>

<script type="text/x-mathjax-config;executed=true">
    MathJax.Hub.Config({
        "HTML-CSS": {
            preferredFont: "TeX",
            availableFonts: ["STIX", "TeX"],
            linebreaks: {
                automatic: true
            },
            EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
        },
        tex2jax: {
            inlineMath: [["\\(", "\\)"], ["\\\\(", "\\\\)"]],
            displayMath: [["$$", "$$"], ["\\[", "\\]"]],
            processEscapes: true,
            ignoreClass: "tex2jax_ignore|dno"
        },
        TeX: {
            noUndefined: {
                attributes: {
                    mathcolor: "red",
                    mathbackground: "#FFEEEE",
                    mathsize: "90%"
                }
            }
        },
        Macros: {
            href: "{}"
        },
        skipStartupTypeset: true,
        messageStyle: "none"
    });
</script>
<script type="text/javascript" async="" crossorigin="anonymous" src="./Predicting Boston House Prices _ Kaggle_files/MathJax.js.下載"></script>



    </div>

        <div class="site-layout__footer">
            <footer class="site-footer">
    <div class="site-footer__content">
        <div class="site-footer__copyright">
            <span>© 2019 Kaggle Inc</span>
        </div>
        <nav class="site-footer__nav">
            <a href="https://www.kaggle.com/team">Our Team</a>
            <a href="https://www.kaggle.com/terms">Terms</a>
            <a href="https://www.kaggle.com/privacy">Privacy</a>
            <a href="https://www.kaggle.com/contact">Contact/Support</a>
        </nav>
        <nav class="site-footer__social">
            <div data-component-name="SocialIcons" style="display: flex; flex-direction: column; flex: 1 0 auto;"><div style="--mdc-theme-on-primary:#fff; --mdc-theme-on-surface:rgba(0, 0, 0, 0.87); --mdc-theme-text-primary-on-background:rgba(0, 0, 0, 0.87); --mdc-theme-text-secondary-on-background:rgba(0, 0, 0, 0.54); --mdc-theme-text-hint-on-background:rgba(0, 0, 0, 0.38); --mdc-theme-text-disabled-on-background:rgba(0, 0, 0, 0.38); --mdc-theme-text-icon-on-background:rgba(0, 0, 0, 0.38); --mdc-theme-primary:#20BEFF; --mdc-theme-error:#F58B8A; --mdc-theme-background:#F8F8F8; --mdc-theme-surface:#F8F8F8; --mdc-theme-primary-bg:#20BEFF; --mdc-theme-secondary-bg:#919294;"><div class="social-icons"><a class="social-icons__link" href="https://www.youtube.com/kaggle" title="Follow Kaggle on YouTube"><i class="fa fa-youtube-square"></i></a><a class="social-icons__link" href="http://www.twitter.com/kaggle" title="Follow Kaggle on Twitter"><i class="fa fa-twitter-square"></i></a><a class="social-icons__link" href="http://www.facebook.com/kaggle" title="Follow Kaggle on Facebook"><i class="fa fa-facebook-square"></i></a><a class="social-icons__link" href="http://www.linkedin.com/company/kaggle" title="Follow Kaggle on LinkedIn"><i class="fa fa-linkedin-square"></i></a></div></div></div><script>var Kaggle=window.Kaggle||{};Kaggle.State=Kaggle.State||[];Kaggle.State.push();performance && performance.mark && performance.mark("SocialIcons.componentCouldBootstrap");</script>
        </nav>
    </div>
</footer>

        </div>
</div>



    <!-- Cheers, web-5b7bc98cc8-w5h5pp. -->

    </main>


<iframe id="intercom-frame" style="position: absolute !important; opacity: 0 !important; width: 1px !important; height: 1px !important; top: 0 !important; left: 0 !important; border: none !important; display: block !important; z-index: -1 !important;" aria-hidden="true" tabindex="-1" src="./Predicting Boston House Prices _ Kaggle_files/saved_resource.html"></iframe><div id="intercom-css-container"><style data-emotion="intercom-global"></style></div><div id="intercom-container" class="intercom-namespace"><style>html.intercom-mobile-messenger-active,html.intercom-mobile-messenger-active > body,html.intercom-modal-open,#intercom-container-body{overflow:hidden !important;}html.intercom-mobile-messenger-active,html.intercom-mobile-messenger-active > body{position:static !important;}html.intercom-mobile-messenger-active > body{height:0 !important;margin:0 !important;}iframe#intercom-frame{position:absolute !important;opacity:0 !important;width:1px !important;height:1px !important;top:0 !important;left:0 !important;border:none !important;display:block !important;z-index:-1 !important;}</style><div class="intercom-app" aria-live="polite"><div id="intercom-modal-container"></div></div></div></body></html>