<!DOCTYPE html>
<!-- saved from url=(0039)https://www.zhihu.com/question/38121173 -->
<html lang="zh" data-hairline="true" data-theme="light"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="renderer" content="webkit"><meta name="force-rendering" content="webkit"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"><meta data-react-helmet="true" name="description" property="og:description" content="Linear least squares, Lasso,ridge regression有何本质区别？还有ridge regression uses L2 regularizat…"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png" sizes="152x152"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.b3e6278d.png" sizes="120x120"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7a750095.png" sizes="76x76"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.a4a761d4.png" sizes="60x60"><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/static/search.xml" title="知乎"><link rel="dns-prefetch" href="https://static.zhimg.com/"><link rel="dns-prefetch" href="https://pic1.zhimg.com/"><link rel="dns-prefetch" href="https://pic2.zhimg.com/"><link rel="dns-prefetch" href="https://pic3.zhimg.com/"><link rel="dns-prefetch" href="https://pic4.zhimg.com/"><style>
.u-safeAreaInset-top {
  height: constant(safe-area-inset-top) !important;
  height: env(safe-area-inset-top) !important;
  
}
.u-safeAreaInset-bottom {
  height: constant(safe-area-inset-bottom) !important;
  height: env(safe-area-inset-bottom) !important;
  
}
</style><link href="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/app.8e85635046f207102080.css" rel="stylesheet"><link href="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/question-routes.042095ae5a957a9ac73c.css" rel="stylesheet"><script defer="" crossorigin="anonymous" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/init.js.下載" data-sentry-config="{&quot;dsn&quot;:&quot;https://65e244586890460588f00f2987137aa8@crash2.zhihu.com/193&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;2250-11349395&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrors&quot;:[&quot;origin message&quot;,&quot;Network request failed&quot;,&quot;Loading chunk&quot;,&quot;这个系统不支持该功能。&quot;,&quot;Can&#39;t find variable: webkit&quot;,&quot;Can&#39;t find variable: $&quot;,&quot;内存不足&quot;,&quot;out of memory&quot;,&quot;DOM Exception 18&quot;,&quot;The operation is insecure&quot;,&quot;[object Event]&quot;,&quot;[object FileError]&quot;,&quot;[object DOMError]&quot;,&quot;[object Object]&quot;,&quot;拒绝访问。&quot;,&quot;Maximum call stack size exceeded&quot;,&quot;UploadError&quot;,&quot;无法 fetch&quot;,&quot;draft-js&quot;,&quot;缺少 JavaScript 对象&quot;,&quot;componentWillEnter&quot;,&quot;componentWillLeave&quot;,&quot;componentWillAppear&quot;,&quot;getInlineStyleAt&quot;,&quot;getCharacterList&quot;],&quot;whitelistUrls&quot;:[&quot;static.zhihu.com&quot;]}"></script><link rel="stylesheet" type="text/css" href="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/richinput.6c6fcb145ace96c2ea0c.css"><script charset="utf-8" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/main.richinput.f29007bc75519b296be3.js.下載"></script><link rel="stylesheet" type="text/css" href="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/modals.06b0b2abf7f99561b746.css"><script charset="utf-8" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/main.modals.7cc66fee260fa5275fe4.js.下載"></script><link rel="stylesheet" type="text/css" href="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/signflow.a9fd05c5b833fa59ed49.css"><script charset="utf-8" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/main.signflow.2776198f0c51deaad605.js.下載"></script></head><body class="Entry-body"><div id="root"><div><div class="LoadingBar"></div><div class="AdblockBanner"><div class="AdblockBanner-inner">我们检测到你可能使用了 AdBlock 或 Adblock Plus，它的部分策略可能会影响到正常功能的使用（如关注）。<br>你可以设定特殊规则或将知乎加入白名单，以便我们更好地提供服务。 （<a href="https://www.zhihu.com/question/54919485" target="_blank">为什么？</a>）</div><button type="button" class="Button AdblockBanner-close Button--plain"><svg viewBox="0 0 14 14" class="Icon Icon--remove" width="16" height="16" aria-hidden="true" style="height: 16px; width: 16px;"><title></title><g><path d="M8.486 7l5.208-5.207c.408-.408.405-1.072-.006-1.483-.413-.413-1.074-.413-1.482-.005L7 5.515 1.793.304C1.385-.103.72-.1.31.31-.103.724-.103 1.385.305 1.793L5.515 7l-5.21 5.207c-.407.408-.404 1.072.007 1.483.413.413 1.074.413 1.482.005L7 8.485l5.207 5.21c.408.407 1.072.404 1.483-.007.413-.413.413-1.074.005-1.482L8.485 7z"></path></g></svg></button></div><div><header role="banner" class="Sticky AppHeader is-hidden is-fixed" data-za-module="TopNavBar" style="width: 1903px; top: 0px; left: 0px;"><div class="AppHeader-inner"><a href="https://www.zhihu.com/" aria-label="知乎"><svg viewBox="0 0 200 91" class="Icon ZhihuLogo ZhihuLogo--blue Icon--logo" width="64" height="30" aria-hidden="true" style="height: 30px; width: 64px;"><title></title><g><path d="M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z" fill-rule="evenodd"></path></g></svg></a><ul role="navigation" class="Tabs AppHeader-Tabs"><li role="tab" class="Tabs-item AppHeader-Tab Tabs-item--noMeta"><a class="Tabs-link AppHeader-TabsLink" href="https://www.zhihu.com/" data-za-not-track-link="true">首页</a></li><li role="tab" class="Tabs-item AppHeader-Tab Tabs-item--noMeta"><a class="Tabs-link AppHeader-TabsLink" href="https://www.zhihu.com/explore" data-za-not-track-link="true">发现</a></li><li role="tab" class="Tabs-item AppHeader-Tab Tabs-item--noMeta"><a class="Tabs-link AppHeader-TabsLink" href="https://www.zhihu.com/question/waiting" data-za-not-track-link="true">等你来答</a></li></ul><div class="SearchBar" role="search" data-za-module="PresetWordItem"><div class="SearchBar-toolWrapper"><form class="SearchBar-tool"><div><div class="Popover"><div class="SearchBar-input Input-wrapper Input-wrapper--grey"><input type="text" maxlength="100" autocomplete="off" role="combobox" aria-expanded="false" aria-autocomplete="list" aria-activedescendant="AutoComplete3--1" id="Popover2-toggle" aria-haspopup="true" aria-owns="Popover2-content" class="Input" placeholder="双向暗恋是一种怎样的体验" value=""><div class="Input-after"><button aria-label="搜索" type="button" class="Button SearchBar-searchIcon Button--primary"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Search" fill="currentColor" viewBox="0 0 24 24" width="18" height="18"><path d="M17.068 15.58a8.377 8.377 0 0 0 1.774-5.159 8.421 8.421 0 1 0-8.42 8.421 8.38 8.38 0 0 0 5.158-1.774l3.879 3.88c.957.573 2.131-.464 1.488-1.49l-3.879-3.878zm-6.647 1.157a6.323 6.323 0 0 1-6.316-6.316 6.323 6.323 0 0 1 6.316-6.316 6.323 6.323 0 0 1 6.316 6.316 6.323 6.323 0 0 1-6.316 6.316z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></form></div></div><div class="AppHeader-userInfo"><div class="AppHeader-profile"><div><button type="button" class="Button AppHeader-login Button--blue">登录</button><button type="button" class="Button Button--primary Button--blue">加入知乎</button></div></div></div></div><div><div class="PageHeader is-shown"><div class="QuestionHeader-content"><div class="QuestionHeader-main"><h1 class="QuestionHeader-title">Linear least squares, Lasso,ridge regression有何本质区别？</h1></div><div class="QuestionHeader-side" data-za-detail-view-path-module="ToolBar" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;38121173&quot;}}}"><div class="QuestionButtonGroup"><button type="button" class="Button FollowButton Button--primary Button--blue">关注问题</button><button type="button" class="Button Button--blue"><svg viewBox="0 0 12 12" class="Icon QuestionButton-icon Button-icon Icon--modify" width="14" height="16" aria-hidden="true" style="height: 16px; width: 14px;"><title></title><g><path d="M.423 10.32L0 12l1.667-.474 1.55-.44-2.4-2.33-.394 1.564zM10.153.233c-.327-.318-.85-.31-1.17.018l-.793.817 2.49 2.414.792-.814c.318-.328.312-.852-.017-1.17l-1.3-1.263zM3.84 10.536L1.35 8.122l6.265-6.46 2.49 2.414-6.265 6.46z" fill-rule="evenodd"></path></g></svg>写回答</button></div></div></div></div></div></header><div class="Sticky--holder" style="position: relative; top: 0px; right: 0px; bottom: 0px; left: 0px; display: block; float: none; margin: 0px; height: 52px;"></div></div><main role="main" class="App-main"><div class="QuestionPage" itemscope="" itemtype="http://schema.org/Question" data-za-detail-view-path-module="QuestionItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;38121173&quot;}}}"><meta itemprop="name" content="Linear least squares, Lasso,ridge regression有何本质区别？"><meta itemprop="url" content="https://www.zhihu.com/question/38121173"><meta itemprop="keywords" content="数据挖掘,机器学习,数据结构"><meta itemprop="answerCount" content="23"><meta itemprop="commentCount" content="0"><meta itemprop="dateCreated" content="2015-12-02T13:09:46.000Z"><meta itemprop="dateModified" content="2015-12-02T13:09:46.000Z"><meta itemprop="zhihu:visitsCount"><meta itemprop="zhihu:followerCount" content="2004"><div data-zop-question="{&quot;title&quot;:&quot;Linear least squares, Lasso,ridge regression有何本质区别？&quot;,&quot;topics&quot;:[{&quot;name&quot;:&quot;数据挖掘&quot;,&quot;id&quot;:&quot;19553534&quot;},{&quot;name&quot;:&quot;机器学习&quot;,&quot;id&quot;:&quot;19559450&quot;},{&quot;name&quot;:&quot;数据结构&quot;,&quot;id&quot;:&quot;19591797&quot;}],&quot;id&quot;:38121173,&quot;isEditable&quot;:false}"><div class="QuestionStatus"></div><div class="QuestionHeader" data-za-detail-view-path-module="QuestionDescription" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;38121173&quot;}}}"><div class="QuestionHeader-content"><div class="QuestionHeader-main"><div class="QuestionHeader-tags"><div class="QuestionHeader-topics"><div class="Tag QuestionTopic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19553534&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/19553534" target="_blank"><div class="Popover"><div id="Popover4-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover4-content">数据挖掘</div></div></a></span></div><div class="Tag QuestionTopic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19559450&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/19559450" target="_blank"><div class="Popover"><div id="Popover5-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover5-content">机器学习</div></div></a></span></div><div class="Tag QuestionTopic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19591797&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/19591797" target="_blank"><div class="Popover"><div id="Popover6-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover6-content">数据结构</div></div></a></span></div></div></div><h1 class="QuestionHeader-title">Linear least squares, Lasso,ridge regression有何本质区别？</h1><div><div class="QuestionHeader-detail"><div class="QuestionRichText QuestionRichText--expandable QuestionRichText--collapsed"><div><span class="RichText ztext" itemprop="text">Linear least squares, Lasso,ridge regression有何本质区别？ 还有<a href="https://link.zhihu.com/?target=http%3A//en.wikipedia.org/wiki/Ridge_regression" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">ridge regression</a> uses…</span><button type="button" class="Button QuestionRichText-more Button--plain">显示全部<svg viewBox="0 0 10 6" class="Icon QuestionRichText-more-icon Icon--arrow" width="10" height="16" aria-hidden="true" style="height: 16px; width: 10px;"><title></title><g><path d="M8.716.217L5.002 4 1.285.218C.99-.072.514-.072.22.218c-.294.29-.294.76 0 1.052l4.25 4.512c.292.29.77.29 1.063 0L9.78 1.27c.293-.29.293-.76 0-1.052-.295-.29-.77-.29-1.063 0z"></path></g></svg></button></div></div></div></div></div><div class="QuestionHeader-side"><div class="QuestionHeader-follow-status"><div class="QuestionFollowStatus"><div class="NumberBoard QuestionFollowStatus-counts NumberBoard--divider"><div class="NumberBoard-item"><div class="NumberBoard-itemInner"><div class="NumberBoard-itemName">关注者</div><strong class="NumberBoard-itemValue" title="2004">2,004</strong></div></div><div class="NumberBoard-item"><div class="NumberBoard-itemInner"><div class="NumberBoard-itemName">被浏览</div><strong class="NumberBoard-itemValue" title="101596">101,596</strong></div></div></div></div></div></div></div><div class="QuestionHeader-footer"><div class="QuestionHeader-footer-inner"><div class="QuestionHeader-main QuestionHeader-footer-main"><div class="QuestionButtonGroup"><button type="button" class="Button FollowButton Button--primary Button--blue">关注问题</button><button type="button" class="Button Button--blue"><svg viewBox="0 0 12 12" class="Icon QuestionButton-icon Button-icon Icon--modify" width="14" height="16" aria-hidden="true" style="height: 16px; width: 14px;"><title></title><g><path d="M.423 10.32L0 12l1.667-.474 1.55-.44-2.4-2.33-.394 1.564zM10.153.233c-.327-.318-.85-.31-1.17.018l-.793.817 2.49 2.414.792-.814c.318-.328.312-.852-.017-1.17l-1.3-1.263zM3.84 10.536L1.35 8.122l6.265-6.46 2.49 2.414-6.265 6.46z" fill-rule="evenodd"></path></g></svg>写回答</button></div><div class="QuestionHeaderActions"><button type="button" class="Button Button--grey Button--withIcon Button--withLabel" style="margin-right: 16px;"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Invite Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M4 10V8a1 1 0 1 1 2 0v2h2a1 1 0 0 1 0 2H6v2a1 1 0 0 1-2 0v-2H2a1 1 0 0 1 0-2h2zm10.455 2c-2.21 0-4-1.79-4-4s1.79-4 4-4 4 1.79 4 4-1.79 4-4 4zm-7 6c0-2.66 4.845-4 7.272-4C17.155 14 22 15.34 22 18v1.375c0 .345-.28.625-.625.625H8.08a.625.625 0 0 1-.625-.625V18z" fill-rule="evenodd"></path></svg></span>邀请回答</button><div class="QuestionHeader-Comment"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>添加评论</button></div><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="Popover7-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover7-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><div class="Popover"><button aria-label="更多" type="button" id="Popover8-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover8-content" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div><div class="QuestionHeader-actions"></div></div></div></div></div><div><div class="Sticky is-fixed" style="width: 1903px; top: 52px; left: 0px;"></div><div class="Sticky--holder" style="position: static; top: auto; right: auto; bottom: auto; left: auto; display: block; float: none; margin: 0px; height: 0px;"></div></div></div><div class="Question-main"><div class="Question-mainColumn"><div><div id="QuestionAnswers-answers" class="QuestionAnswers-answers" data-zop-feedlistmap="0,0,1,0" data-za-detail-view-path-module="ContentList" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null}}"><div class="Card AnswersNavWrapper"><div class="ListShortcut"><div class="List"><div class="List-header"><h4 class="List-headerText"><span>23 个回答</span></h4><div class="List-headerOptions"><div class="Popover"><button role="combobox" aria-expanded="false" type="button" id="Popover9-toggle" aria-haspopup="true" aria-owns="Popover9-content" class="Button Select-button Select-plainButton Button--plain">默认排序<span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Select Select-arrow" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M12 16.183l2.716-2.966a.757.757 0 0 1 1.064.001.738.738 0 0 1 0 1.052l-3.247 3.512a.758.758 0 0 1-1.064 0L8.22 14.27a.738.738 0 0 1 0-1.052.758.758 0 0 1 1.063 0L12 16.183zm0-9.365L9.284 9.782a.758.758 0 0 1-1.064 0 .738.738 0 0 1 0-1.052l3.248-3.512a.758.758 0 0 1 1.065 0L15.78 8.73a.738.738 0 0 1 0 1.052.757.757 0 0 1-1.063.001L12 6.818z" fill-rule="evenodd"></path></svg></span></button></div></div></div><div><div class=""><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="0" data-zop="{&quot;authorName&quot;:&quot;童话李&quot;,&quot;itemId&quot;:403986652,&quot;title&quot;:&quot;Linear least squares, Lasso,ridge regression有何本质区别？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="403986652" itemprop="acceptedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="0" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;403986652&quot;,&quot;upvote_num&quot;:416,&quot;comment_num&quot;:37,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;38121173&quot;,&quot;author_member_hash_id&quot;:&quot;2aabe2f784afcc242b18cad062235cca&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="童话李"><meta itemprop="image" content="https://pic1.zhimg.com/v2-116898dc61a60b17b597e10cf47f3fa1_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/larry-LJY"><meta itemprop="zhihu:followerCount" content="480"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover10-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover10-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/larry-LJY"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-116898dc61a60b17b597e10cf47f3fa1_xs.jpg" srcset="https://pic1.zhimg.com/v2-116898dc61a60b17b597e10cf47f3fa1_l.jpg 2x" alt="童话李"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover11-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover11-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/larry-LJY">童话李</a></div></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="ztext AuthorInfo-badgeText">你哭着对我说：</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button type="button" class="Button Button--plain">416 人赞同了该回答</button></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="416"><meta itemprop="url" content="https://www.zhihu.com/question/38121173/answer/403986652"><meta itemprop="dateCreated" content="2018-05-28T22:04:24.000Z"><meta itemprop="dateModified" content="2018-05-29T03:41:25.000Z"><meta itemprop="commentCount" content="37"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text"><p>这两天的项目就是LASSO跟RIDGE，讲讲我的体会。</p><p>LASSO这两年莫名的非常火，不管什么经济学话题，<b>只要涉及到解释变量有效性的问题，评论人或者审稿人都想看看，你这玩意儿做的LASSO有啥结果没有。潜台词就是，如果LASSO做不出结果是不是你这个话题本身就有问题呢。</b>过两年这阵风过去可能大家的想法又变了，不过身处其中普通人别无选择只能适应，无论如何都不能在潮流中落了下风。</p><p class="ztext-empty-paragraph"><br></p><p>公式之前好多人都写了，我就直接摆几个别人总结最简单的结果，这对我理解背后的机制帮助非常大，图懒得画，都是从网上贴的。</p><p>先随便生成一组数</p><p><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation" alt="[公式]" eeimg="1" data-formula="y=sin (x)+\epsilon\\ x \in [1/3\pi,5/3\pi]"></p><p>本质上是三角函数加上一个正太分布的随机扰动，大概长这样</p><p class="ztext-empty-paragraph"><br></p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/50/v2-d87347a2af705634b5572bf36d446cba_hd.jpg" data-caption="" data-size="normal" data-rawwidth="300" data-rawheight="245" data-default-watermark-src="https://pic2.zhimg.com/50/v2-33c0bce2a63972904b39ff9084fd6ffe_hd.jpg" class="content_image" width="300"/></noscript><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-d87347a2af705634b5572bf36d446cba_hd.jpg" data-caption="" data-size="normal" data-rawwidth="300" data-rawheight="245" data-default-watermark-src="https://pic2.zhimg.com/50/v2-33c0bce2a63972904b39ff9084fd6ffe_hd.jpg" class="content_image lazy" width="300" data-actualsrc="https://pic3.zhimg.com/50/v2-d87347a2af705634b5572bf36d446cba_hd.jpg" data-lazy-status="ok"></figure><p>接着再跑15个OLS回归，里面加上1到15阶的x作为回归变量，</p><p><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(1)" alt="[公式]" eeimg="1" data-formula="y=x^{0}+x^{1}+...+x^{n}\\ n=1,...,15"></p><p>拟合结果大概长这样</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/50/v2-ae7bc7c2e8cd1ddf1c43ba2f31fbb983_hd.jpg" data-caption="" data-size="normal" data-rawwidth="1024" data-rawheight="854" data-default-watermark-src="https://pic4.zhimg.com/50/v2-929ff4f7a3dcc02beb48aa527044cd64_hd.jpg" class="origin_image zh-lightbox-thumb" width="1024" data-original="https://pic1.zhimg.com/v2-ae7bc7c2e8cd1ddf1c43ba2f31fbb983_r.jpg"/></noscript><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-ae7bc7c2e8cd1ddf1c43ba2f31fbb983_hd.jpg" data-caption="" data-size="normal" data-rawwidth="1024" data-rawheight="854" data-default-watermark-src="https://pic4.zhimg.com/50/v2-929ff4f7a3dcc02beb48aa527044cd64_hd.jpg" class="origin_image zh-lightbox-thumb lazy" width="1024" data-original="https://pic1.zhimg.com/v2-ae7bc7c2e8cd1ddf1c43ba2f31fbb983_r.jpg" data-actualsrc="https://pic1.zhimg.com/50/v2-ae7bc7c2e8cd1ddf1c43ba2f31fbb983_hd.jpg" data-lazy-status="ok"></figure><p>上图可以看出，从1到15阶的x，确实拟合的越来越精确，可是与背后真正的函数sin(x)的距离，也经历了先接近后远离的情况。原因是OLS为了把拟合精确度提高，会尽力把噪音也拟合上，最终造成了所谓的过度拟合(overfitting)，catch the noise, not the signal。</p><p><b>那么一个合理的问题就是：如何才能避免过度拟合？</b></p><p>为了回答这个问题，就先看一眼刚刚回归出来的系数，大概长这样。</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/50/v2-ce63b4c6904500a93ab00afbb82ec7ca_hd.jpg" data-caption="" data-size="normal" data-rawwidth="1600" data-rawheight="759" data-default-watermark-src="https://pic4.zhimg.com/50/v2-f3c8655f5455b0d62a997f059a427c2c_hd.jpg" class="origin_image zh-lightbox-thumb" width="1600" data-original="https://pic1.zhimg.com/v2-ce63b4c6904500a93ab00afbb82ec7ca_r.jpg"/></noscript><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-ce63b4c6904500a93ab00afbb82ec7ca_hd.jpg" data-caption="" data-size="normal" data-rawwidth="1600" data-rawheight="759" data-default-watermark-src="https://pic4.zhimg.com/50/v2-f3c8655f5455b0d62a997f059a427c2c_hd.jpg" class="origin_image zh-lightbox-thumb lazy" width="1600" data-original="https://pic1.zhimg.com/v2-ce63b4c6904500a93ab00afbb82ec7ca_r.jpg" data-actualsrc="https://pic1.zhimg.com/50/v2-ce63b4c6904500a93ab00afbb82ec7ca_hd.jpg" data-lazy-status="ok"></figure><p>有看到什么趋势没有？</p><p><b>一个很明显的趋势就是，系数的数量级从个位数奔到了十的5次方到6次方！</b></p><p><b>直觉就是大的系数可以把X微小的变动放大，通过多个正负项的叠加尽量把每个点都拟合上。</b></p><p>这就是在日常实践中判断过度拟合的一个重要标准，系数如果大的离谱，多半是过度拟合了。至于多少是离谱，需要根据经验判断。单变量股票收益率预测回归，在采用百分比收益率，并把X标准化到(0,1)之后如果出来个几十的系数，多半是过度拟合了。就像R^2如果做到20%以上多半是有look ahead bias，其他结果我根本就不想往下看。。。</p><p>那么好怎么解决这个问题？</p><p><b>一个非常直觉的解决方法就是，在目标函数里面把过大的系数进行惩罚(<u>penalty</u>)。</b></p><p><b>LASSO与RIDGE的区别就是怎么进行这个惩罚。</b></p><p>先说LASSO，</p><p>它是这样做惩罚的，在OLS拟合的基础上，对其系数的绝对值进行惩罚，目标函数长这样</p><p><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(2)" alt="[公式]" eeimg="1" data-formula="argmin(y-wx)^2+\alpha |w|"></p><p>这样写目标函数就是想达到一个平衡，第一拟合的误差要小，第二<b>系数的绝对值</b>不能太大。</p><p>拟合的图像跟系数分别如下</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/50/v2-184f95e9329f8e71597d2b62724eb21f_hd.jpg" data-caption="" data-size="normal" data-rawwidth="1024" data-rawheight="833" data-default-watermark-src="https://pic4.zhimg.com/50/v2-8ba8e565a8cdc9728cd57e7760f9a536_hd.jpg" class="origin_image zh-lightbox-thumb" width="1024" data-original="https://pic1.zhimg.com/v2-184f95e9329f8e71597d2b62724eb21f_r.jpg"/></noscript><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-184f95e9329f8e71597d2b62724eb21f_hd.jpg" data-caption="" data-size="normal" data-rawwidth="1024" data-rawheight="833" data-default-watermark-src="https://pic4.zhimg.com/50/v2-8ba8e565a8cdc9728cd57e7760f9a536_hd.jpg" class="origin_image zh-lightbox-thumb lazy" width="1024" data-original="https://pic1.zhimg.com/v2-184f95e9329f8e71597d2b62724eb21f_r.jpg" data-actualsrc="https://pic1.zhimg.com/50/v2-184f95e9329f8e71597d2b62724eb21f_hd.jpg" data-lazy-status="ok"></figure><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/50/v2-09d4817a8332a69800d58f6469deee21_hd.jpg" data-caption="" data-size="normal" data-rawwidth="1024" data-rawheight="334" data-default-watermark-src="https://pic4.zhimg.com/50/v2-5f96838e4b4ed52b1f42d7cea8e42019_hd.jpg" class="origin_image zh-lightbox-thumb" width="1024" data-original="https://pic1.zhimg.com/v2-09d4817a8332a69800d58f6469deee21_r.jpg"/></noscript><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-09d4817a8332a69800d58f6469deee21_hd.jpg" data-caption="" data-size="normal" data-rawwidth="1024" data-rawheight="334" data-default-watermark-src="https://pic4.zhimg.com/50/v2-5f96838e4b4ed52b1f42d7cea8e42019_hd.jpg" class="origin_image zh-lightbox-thumb lazy" width="1024" data-original="https://pic1.zhimg.com/v2-09d4817a8332a69800d58f6469deee21_r.jpg" data-actualsrc="https://pic1.zhimg.com/50/v2-09d4817a8332a69800d58f6469deee21_hd.jpg" data-lazy-status="ok"></figure><p>有以下几个特点，</p><ol><li>随着惩罚力度alpha的增加越来越多的系数变成了0</li><li>系数确实很好的控制在合理区间，以截距项跟一阶项为例，大致都在1到0.X之间。</li><li>随着惩罚力度的增加拟合越来越差，到最后RSS到了37，出现了拟合不足（underfitting）的问题。</li></ol><p class="ztext-empty-paragraph"><br></p><p>再说RIDGE，目标函数长这样</p><p><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(3)" alt="[公式]" eeimg="1" data-formula="argmin(y-wx)^2+\lambda w^2"></p><p>也是想达到一个类似的平衡，第一拟合的误差要小，第二<b>系数的平方</b>不能太大。</p><p>结果如下，</p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/50/v2-f0b57ab6ffc0361c121a4c25958ba22d_hd.jpg" data-caption="" data-size="normal" data-rawwidth="1024" data-rawheight="847" data-default-watermark-src="https://pic4.zhimg.com/50/v2-fddb92b97a3880b239bbac11c8e4daf1_hd.jpg" class="origin_image zh-lightbox-thumb" width="1024" data-original="https://pic2.zhimg.com/v2-f0b57ab6ffc0361c121a4c25958ba22d_r.jpg"/></noscript><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-f0b57ab6ffc0361c121a4c25958ba22d_hd.jpg" data-caption="" data-size="normal" data-rawwidth="1024" data-rawheight="847" data-default-watermark-src="https://pic4.zhimg.com/50/v2-fddb92b97a3880b239bbac11c8e4daf1_hd.jpg" class="origin_image zh-lightbox-thumb lazy" width="1024" data-original="https://pic2.zhimg.com/v2-f0b57ab6ffc0361c121a4c25958ba22d_r.jpg" data-actualsrc="https://pic2.zhimg.com/50/v2-f0b57ab6ffc0361c121a4c25958ba22d_hd.jpg" data-lazy-status="ok"></figure><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/50/v2-a82e351373b93e2b8b749cff0cae3d17_hd.jpg" data-caption="" data-size="normal" data-rawwidth="1024" data-rawheight="334" data-default-watermark-src="https://pic2.zhimg.com/50/v2-9193be2f228736c914cb7e6cf18e75da_hd.jpg" class="origin_image zh-lightbox-thumb" width="1024" data-original="https://pic2.zhimg.com/v2-a82e351373b93e2b8b749cff0cae3d17_r.jpg"/></noscript><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-a82e351373b93e2b8b749cff0cae3d17_hd.jpg" data-caption="" data-size="normal" data-rawwidth="1024" data-rawheight="334" data-default-watermark-src="https://pic2.zhimg.com/50/v2-9193be2f228736c914cb7e6cf18e75da_hd.jpg" class="origin_image zh-lightbox-thumb lazy" width="1024" data-original="https://pic2.zhimg.com/v2-a82e351373b93e2b8b749cff0cae3d17_r.jpg" data-actualsrc="https://pic2.zhimg.com/50/v2-a82e351373b93e2b8b749cff0cae3d17_hd.jpg" data-lazy-status="ok"></figure><p>有以下几个特点，</p><ol><li>随着惩罚力度alpha的增加越来越多的系数变得很小，但不会到0。</li><li>随着惩罚力度的增加拟合越来越差，到最后RSS到了23，出现了拟合不足（underfitting）的问题。但拟合不足的问题似乎比相同条件下LASSO来的轻一些。</li><li>由于系数一直不到0，便没办法做变量选择。</li></ol><p><b>接下来的一个问题是，既然惩罚力度alpha太大了容易拟合不足，太低了容易过度拟合。究竟多大的惩罚力度是合适的？</b></p><p>这个问题对于OLS，LASSO，和RIDGE，有一个相对标准的做法，用赤池信息准则(AIC)或贝叶斯信息准则(BIC)进行判断。</p><p><b>多余的公式就不写了，直觉是这样的，AIC大致都是关于惩罚力度的U型函数，条件形同的情况下AIC越小越好，直接选取AIC最低点对应的惩罚力度alpha。</b>一个例子就是下图。</p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/50/v2-90a4685daecaf241520a09ea74540abf_hd.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="480" data-default-watermark-src="https://pic3.zhimg.com/50/v2-c28462770bd8c163990a7ae009db01fb_hd.jpg" class="origin_image zh-lightbox-thumb" width="640" data-original="https://pic2.zhimg.com/v2-90a4685daecaf241520a09ea74540abf_r.jpg"/></noscript><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-90a4685daecaf241520a09ea74540abf_hd.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="480" data-default-watermark-src="https://pic3.zhimg.com/50/v2-c28462770bd8c163990a7ae009db01fb_hd.jpg" class="origin_image zh-lightbox-thumb lazy" width="640" data-original="https://pic2.zhimg.com/v2-90a4685daecaf241520a09ea74540abf_r.jpg" data-actualsrc="https://pic2.zhimg.com/50/v2-90a4685daecaf241520a09ea74540abf_hd.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><p><b>最后用一幅图总结三者的关系</b></p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/50/v2-9a7fa8a60918d49e329eb35fd731c59f_hd.jpg" data-caption="" data-size="normal" data-rawwidth="300" data-rawheight="169" data-default-watermark-src="https://pic4.zhimg.com/50/v2-2a9ca445446e05168abf222dc4a9e4cf_hd.jpg" class="content_image" width="300"/></noscript><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-9a7fa8a60918d49e329eb35fd731c59f_hd.jpg" data-caption="" data-size="normal" data-rawwidth="300" data-rawheight="169" data-default-watermark-src="https://pic4.zhimg.com/50/v2-2a9ca445446e05168abf222dc4a9e4cf_hd.jpg" class="content_image lazy" width="300" data-actualsrc="https://pic3.zhimg.com/50/v2-9a7fa8a60918d49e329eb35fd731c59f_hd.jpg" data-lazy-status="ok"></figure><p>如果以OLS的系数作为横轴，OLS, LASSO, RIDGE的系数作为纵轴的话，可以画一幅大致如上的图。</p><ol><li>LASSO本质上对OLS的系数做了一个固定数值的惩罚这个数值大致是1/2alpha，这一点是可以严格证明的。但最终其变动的趋势和OLS是一样的，用图中实例就是红线与蓝线其实是平行的。</li><li>RIDGE本质上对OLS的系数做了一个比例上的缩减。可以从图中看出，绿线的斜率变低了。</li></ol><p class="ztext-empty-paragraph"><br></p><p>这几日睁眼到闭眼琢磨的都是这俩东西，希望对你有用。公式是要好好看，但别太纠结于公式，推了半个笔记本最后可能还是不知道该怎么用LASSO。真正做data的时候，难点肯定都不在这里，有空再写吧。</p></span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/38121173/answer/403986652"><span data-tooltip="发布于 2018-05-29 06:04">编辑于 2018-05-29</span></a></div></div><div class="ContentItem-actions RichContent-actions"><span><button aria-label="赞同 416" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 416</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>37 条评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover23-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover23-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button><button data-zop-retract-question="true" type="button" class="Button ContentItem-action ContentItem-rightButton Button--plain"><span class="RichContent-collapsedText">收起</span><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--ArrowDown ContentItem-arrowIcon is-active" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M12 13L8.285 9.218a.758.758 0 0 0-1.064 0 .738.738 0 0 0 0 1.052l4.249 4.512a.758.758 0 0 0 1.064 0l4.246-4.512a.738.738 0 0 0 0-1.052.757.757 0 0 0-1.063 0L12.002 13z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="1" data-zop="{&quot;authorName&quot;:&quot;知乎用户&quot;,&quot;itemId&quot;:75158776,&quot;title&quot;:&quot;Linear least squares, Lasso,ridge regression有何本质区别？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="75158776" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="1" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;75158776&quot;,&quot;upvote_num&quot;:564,&quot;comment_num&quot;:21,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;38121173&quot;,&quot;author_member_hash_id&quot;:&quot;17b5e9a781f9157bc7ee12bca15d6f36&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="知乎用户"><meta itemprop="image" content="https://pic4.zhimg.com/da8e974dc_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/"><meta itemprop="zhihu:followerCount" content="313"><span class="UserLink AuthorInfo-avatarWrapper"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/da8e974dc_xs.jpg" srcset="https://pic4.zhimg.com/da8e974dc_l.jpg 2x" alt="知乎用户"></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name">知乎用户</span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button type="button" class="Button Button--plain">564 人赞同了该回答</button></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="564"><meta itemprop="url" content="https://www.zhihu.com/question/38121173/answer/75158776"><meta itemprop="dateCreated" content="2015-12-04T00:17:09.000Z"><meta itemprop="dateModified" content="2015-12-04T00:17:09.000Z"><meta itemprop="commentCount" content="21"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text"><p>Linear regression一般只对low dimension适用，比如n=50, p=5，而且这五个变量还不存在multicolinearity.</p><p>Ridge Regression的提出就是为了解决multicolinearity的，加一个L2 penalty term也是因为算起来方便。然而它并不能shrink parameters to 0.所以没法做variable selection。</p><p>LASSO是针对Ridge Regression的没法做variable selection的问题提出来的，L1 penalty虽然算起来麻烦，没有解析解，但是可以把某些系数shrink到0啊。</p><p>然而LASSO虽然可以做variable selection，但是不consistent啊，而且当n很小时至多只能选出n个变量；而且不能做group selection。</p>于是有了在L1和L2 penalty之间做个权重就是elastic net, 针对不consistent有了adaptive lasso，针对不能做group selection有了group lasso, 在graphical models里有了graphical lasso。然后有人说unbiasedness, sparsity and continuity这三条都满足多好，于是有了MCP和SCAD同时满足这三条性质。penalized regression太多了，上面提到的都是比较popular的方法了。</span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/38121173/answer/75158776"><span data-tooltip="发布于 2015-12-04 08:17">发布于 2015-12-04</span></a></div></div><div class="ContentItem-actions"><span><button aria-label="赞同 564" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 564</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>21 条评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover22-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover22-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button></div></div></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="2" data-zop="{&quot;authorName&quot;:&quot;知乎用户&quot;,&quot;itemId&quot;:166238142,&quot;title&quot;:&quot;Linear least squares, Lasso,ridge regression有何本质区别？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="166238142" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="2" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;166238142&quot;,&quot;upvote_num&quot;:184,&quot;comment_num&quot;:18,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;38121173&quot;,&quot;author_member_hash_id&quot;:&quot;edcb3f84977d280287bc025bdbe86465&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="知乎用户"><meta itemprop="image" content="https://pic4.zhimg.com/da8e974dc_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/"><meta itemprop="zhihu:followerCount" content="7873"><span class="UserLink AuthorInfo-avatarWrapper"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/da8e974dc_xs.jpg" srcset="https://pic4.zhimg.com/da8e974dc_l.jpg 2x" alt="知乎用户"></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name">知乎用户</span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button type="button" class="Button Button--plain">184 人赞同了该回答</button></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="184"><meta itemprop="url" content="https://www.zhihu.com/question/38121173/answer/166238142"><meta itemprop="dateCreated" content="2017-05-07T19:34:18.000Z"><meta itemprop="dateModified" content="2017-05-07T23:39:06.000Z"><meta itemprop="commentCount" content="18"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text"><p>很多回答都很全面了，大意就是lasso在优化过程的目标函数中使用如下的L1 penalty：</p><figure><noscript><img src="https://pic3.zhimg.com/50/v2-6640aa578fb80502a3548a133e7b42db_hd.jpg" data-rawwidth="764" data-rawheight="154" class="origin_image zh-lightbox-thumb" width="764" data-original="https://pic3.zhimg.com/v2-6640aa578fb80502a3548a133e7b42db_r.jpg"/></noscript><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-6640aa578fb80502a3548a133e7b42db_hd.jpg" data-rawwidth="764" data-rawheight="154" class="origin_image zh-lightbox-thumb lazy" width="764" data-original="https://pic3.zhimg.com/v2-6640aa578fb80502a3548a133e7b42db_r.jpg" data-actualsrc="https://pic3.zhimg.com/50/v2-6640aa578fb80502a3548a133e7b42db_hd.jpg" data-lazy-status="ok"></figure><p>从而把一些线性回归项的系数“逼成”零；ridge是用L2 penalty，旨在把系数变得小一些，但非完全成零。两者原理上的区别可由下图表示：</p><figure><noscript><img src="https://pic3.zhimg.com/50/v2-2a88e2acc009fa4de3edeb51e683ca02_hd.jpg" data-rawwidth="602" data-rawheight="399" class="origin_image zh-lightbox-thumb" width="602" data-original="https://pic3.zhimg.com/v2-2a88e2acc009fa4de3edeb51e683ca02_r.jpg"/></noscript><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-2a88e2acc009fa4de3edeb51e683ca02_hd.jpg" data-rawwidth="602" data-rawheight="399" class="origin_image zh-lightbox-thumb lazy" width="602" data-original="https://pic3.zhimg.com/v2-2a88e2acc009fa4de3edeb51e683ca02_r.jpg" data-actualsrc="https://pic3.zhimg.com/50/v2-2a88e2acc009fa4de3edeb51e683ca02_hd.jpg" data-lazy-status="ok"></figure><p>不难看出由于L1 penalty规定的范围“四四方方、有棱有角”，所以最优解的系数会被刚好缩成零，因此lasso可以实现对变量的选择（系数为零的变量就被筛掉了）。</p><p>有趣的是，我们还可以将所有变量分组，然后在目标函数中惩罚每一组的L2范数，这样达到的效果就是可以将一整组的系数同时消成零，即抹掉一整组的变量，这种手法叫做<b>group lasso</b>，其目标函数如下：</p><figure><noscript><img src="https://pic3.zhimg.com/50/v2-4221797634d200611e9aff12e8242761_hd.jpg" data-rawwidth="957" data-rawheight="164" class="origin_image zh-lightbox-thumb" width="957" data-original="https://pic3.zhimg.com/v2-4221797634d200611e9aff12e8242761_r.jpg"/></noscript><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-4221797634d200611e9aff12e8242761_hd.jpg" data-rawwidth="957" data-rawheight="164" class="origin_image zh-lightbox-thumb lazy" width="957" data-original="https://pic3.zhimg.com/v2-4221797634d200611e9aff12e8242761_r.jpg" data-actualsrc="https://pic3.zhimg.com/50/v2-4221797634d200611e9aff12e8242761_hd.jpg" data-lazy-status="ok"></figure><p>其中我们把所有变量分为 <img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(4)" alt="[公式]" eeimg="1" data-formula="m"> 组，第一项是通常的OLS，第二项是每一组系数的L2范数之和。这里， <img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(5)" alt="[公式]" eeimg="1" data-formula="\lambda">控制整体惩罚的力度，<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(6)" alt="[公式]" eeimg="1" data-formula="\sqrt{\rho_l}">是每一组的加权，可以按需调节。</p><p>比如一个regression若有10个系数 <img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(7)" alt="[公式]" eeimg="1" data-formula="\beta_1, \beta_2, ..., \beta_{10}">，我们如果选择将其分成2组：其中 <img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(8)" alt="[公式]" eeimg="1" data-formula="\beta_1, ..., \beta_5"> 一组， <img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(9)" alt="[公式]" eeimg="1" data-formula="\beta_6, ..., \beta_{10}"> 一组。那么group lasso的惩罚项目将会是：</p><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(10)" alt="[公式]" eeimg="1" data-formula="\rho(\sqrt{p_1}\sqrt{\sum_{i = 1}^5\beta_i^2} + \sqrt{p_2}\sqrt{\sum_{j = 6}^{10}\beta_j^2})"><p>通过施加group-wise的L2 penalty，我们有可能促使 <img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(11)" alt="[公式]" eeimg="1" data-formula="\beta_1 = \beta_2 = ... = \beta_5 = 0"> 或者 <img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(12)" alt="[公式]" eeimg="1" data-formula="\beta_6 = \beta_7 = ... = \beta_{10} = 0"> 。 </p><p>最后，还有一种lasso和group lasso的奇葩结合，叫做<b>sparse group lasso</b>，由 Simon et al 在2013年提出，sparse group lasso的目标函数（如下）的惩罚项中，既有所有系数的L1范数，又有每一组系数的L2范数</p><figure><noscript><img src="https://pic4.zhimg.com/50/v2-63a3bd6361cf5420a4c0c8878729c41f_hd.jpg" data-rawwidth="1024" data-rawheight="154" class="origin_image zh-lightbox-thumb" width="1024" data-original="https://pic4.zhimg.com/v2-63a3bd6361cf5420a4c0c8878729c41f_r.jpg"/></noscript><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-63a3bd6361cf5420a4c0c8878729c41f_hd.jpg" data-rawwidth="1024" data-rawheight="154" class="origin_image zh-lightbox-thumb lazy" width="1024" data-original="https://pic4.zhimg.com/v2-63a3bd6361cf5420a4c0c8878729c41f_r.jpg" data-actualsrc="https://pic4.zhimg.com/50/v2-63a3bd6361cf5420a4c0c8878729c41f_hd.jpg" data-lazy-status="ok"></figure><p>其中 <img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(5)" alt="[公式]" eeimg="1" data-formula="\lambda"> 依然控制总体的惩罚力度，有新引入 <img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(13)" alt="[公式]" eeimg="1" data-formula="\alpha"> 控制两个惩罚项之间的相互强弱。所以sparse group lasso既可以把系数和变量一组一组地筛掉，又可以在剩下的组中筛掉一些单个的系数，原理图如下：</p><figure><noscript><img src="https://pic1.zhimg.com/50/v2-0382de0682555ee4e73f0e63dbd2abe9_hd.jpg" data-rawwidth="443" data-rawheight="146" class="origin_image zh-lightbox-thumb" width="443" data-original="https://pic1.zhimg.com/v2-0382de0682555ee4e73f0e63dbd2abe9_r.jpg"/></noscript><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-0382de0682555ee4e73f0e63dbd2abe9_hd.jpg" data-rawwidth="443" data-rawheight="146" class="origin_image zh-lightbox-thumb lazy" width="443" data-original="https://pic1.zhimg.com/v2-0382de0682555ee4e73f0e63dbd2abe9_r.jpg" data-actualsrc="https://pic1.zhimg.com/50/v2-0382de0682555ee4e73f0e63dbd2abe9_hd.jpg" data-lazy-status="ok"></figure><p>当然了，这只是在简单OLS背景下的lasso、ridge、和group lasso和sparse group lasso，更常用的目标函数的第一项一般是log likelihood（用于maximum likelihood手法）。相似的概念也可以迁移到其他场景，比如因子分析模型（factor analysis model），其中group lasso可以帮助进行对可被观测的变量选取，而sparse group lasso可以选取隐藏因子，我统计的thesis做的就是这个啦。</p></span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/38121173/answer/166238142"><span data-tooltip="发布于 2017-05-08 03:34">编辑于 2017-05-08</span></a></div></div><div class="ContentItem-actions RichContent-actions"><span><button aria-label="赞同 184" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 184</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>18 条评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover26-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover26-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button><button data-zop-retract-question="true" type="button" class="Button ContentItem-action ContentItem-rightButton Button--plain"><span class="RichContent-collapsedText">收起</span><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--ArrowDown ContentItem-arrowIcon is-active" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M12 13L8.285 9.218a.758.758 0 0 0-1.064 0 .738.738 0 0 0 0 1.052l4.249 4.512a.758.758 0 0 0 1.064 0l4.246-4.512a.738.738 0 0 0 0-1.052.757.757 0 0 0-1.063 0L12.002 13z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div class="Pc-word"><div class="Pc-word-card"><a target="_blank" href="https://www.togocareer.com/services.html?tgcChannel==zhihu&amp;tuwen0805"><div class="Pc-word-card-brand"><div class="Pc-word-card-brand-wrapper"><img width="20" height="20" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-b02efb32af2aada63f09e5d0a275df75_250x250.jpeg" alt="logo"><span>Togocareer</span></div></div></a><div class="Pc-word-card-sign"><div class="Pc-word-card-sign-label">广告​<svg class="Icon Icon--triangle Pc-word-card-sign-svg" viewBox="0 0 24 24"><path d="M9.218 16.78a.737.737 0 0 0 1.052 0l4.512-4.249a.758.758 0 0 0 0-1.063L10.27 7.22a.737.737 0 0 0-1.052 0 .759.759 0 0 0-.001 1.063L13 12l-3.782 3.716a.758.758 0 0 0 0 1.063z" fill-rule="evenodd"></path></svg></div><div class="Pc-word-card-sign-popup Pc-word-card-sign-popup--isHidden"><span class="Pc-word-card-sign-popup-arrow"></span><div class="Pc-word-card-sign-popup-menu"><button type="button">不感兴趣</button><a target="_blank" rel="noopener noreferrer" href="https://www.zhihu.com/promotion-intro">知乎广告介绍</a></div></div></div><a target="_blank" href="https://www.togocareer.com/services.html?tgcChannel==zhihu&amp;tuwen0805"><h2 class="Pc-word-card-title">对于留学生来说，回国求职，如何进入名企工作？实习机会怎么找？</h2><div class="Pc-word-card-content  "><span>留学生回国，在国内的发展情况如何？2019年校招、社招信息、500强名企内推名额预约，实习，求职更快捷！</span><span class="Pc-word-card-content-cta  ">查看详情</span></div></a></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="4" data-zop="{&quot;authorName&quot;:&quot;杨军&quot;,&quot;itemId&quot;:85813729,&quot;title&quot;:&quot;Linear least squares, Lasso,ridge regression有何本质区别？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="85813729" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="4" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;85813729&quot;,&quot;upvote_num&quot;:123,&quot;comment_num&quot;:12,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;38121173&quot;,&quot;author_member_hash_id&quot;:&quot;2e4dfd1e401bed61d2d21fa6cb4f8600&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="杨军"><meta itemprop="image" content="https://pic2.zhimg.com/v2-0a767e48ee617044b3898761e96da2d9_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/yang-jun-14"><meta itemprop="zhihu:followerCount" content="11605"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover15-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover15-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/yang-jun-14"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-0a767e48ee617044b3898761e96da2d9_xs.jpg" srcset="https://pic2.zhimg.com/v2-0a767e48ee617044b3898761e96da2d9_l.jpg 2x" alt="杨军"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover16-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover16-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/yang-jun-14">杨军</a></div></div><a class="UserLink-badge" data-tooltip="优秀回答者" href="https://www.zhihu.com/question/48509984" target="_blank" rel="noopener noreferrer"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--BadgeGlorious" fill="currentColor" viewBox="0 0 24 24" width="18" height="18"><g fill="none" fill-rule="evenodd"><path fill="#FF9500" d="M2.64 13.39c1.068.895 1.808 2.733 1.66 4.113l.022-.196c-.147 1.384.856 2.4 2.24 2.278l-.198.016c1.387-.122 3.21.655 4.083 1.734l-.125-.154c.876 1.084 2.304 1.092 3.195.027l-.127.152c.895-1.068 2.733-1.808 4.113-1.66l-.198-.022c1.386.147 2.402-.856 2.279-2.238l.017.197c-.122-1.388.655-3.212 1.734-4.084l-.154.125c1.083-.876 1.092-2.304.027-3.195l.152.127c-1.068-.895-1.808-2.732-1.66-4.113l-.022.198c.147-1.386-.856-2.4-2.24-2.279l.198-.017c-1.387.123-3.21-.654-4.083-1.733l.125.153c-.876-1.083-2.304-1.092-3.195-.027l.127-.152c-.895 1.068-2.733 1.808-4.113 1.662l.198.02c-1.386-.147-2.4.857-2.279 2.24L4.4 6.363c.122 1.387-.655 3.21-1.734 4.084l.154-.126c-1.083.878-1.092 2.304-.027 3.195l-.152-.127z"></path><path fill="#FFF" d="M12.034 14.959L9.379 16.58c-.468.286-.746.09-.617-.449l.721-3.025-2.362-2.024c-.417-.357-.317-.681.236-.725l3.1-.249 1.195-2.872c.21-.507.55-.512.763 0l1.195 2.872 3.1.249c.547.043.657.365.236.725l-2.362 2.024.721 3.025c.128.534-.144.738-.617.449l-2.654-1.621z"></path></g></svg></span></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="AuthorInfo-badgeText"><span><span><a href="https://www.zhihu.com/people/yang-jun-14/creations/19813032">深度学习（Deep Learning）</a>、</span><span><a href="https://www.zhihu.com/people/yang-jun-14/creations/19559450">机器学习</a> </span>话题</span>的优秀回答者</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button type="button" class="Button Button--plain">123 人赞同了该回答</button></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="123"><meta itemprop="url" content="https://www.zhihu.com/question/38121173/answer/85813729"><meta itemprop="dateCreated" content="2016-02-11T13:51:40.000Z"><meta itemprop="dateModified" content="2016-02-11T14:40:13.000Z"><meta itemprop="commentCount" content="12"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text"><p>线性回归问题是很经典的机器学习问题了。
</p><p>适用的方法也蛮多，有标准的Ordinary Least Squares，还有带了L2正则的Ridge Regression以及L1正则的Lasso Regression。
</p><p>这些不同的回归模型的差异和设计动机是什么？
</p><p>在本帖的一个高票回答[1]里，把这个问题讨论得其实已经相当清楚了。
</p><p>我在这里的回答更多是一个知识性的总结，在Scott Young的《如何高效学习》[6]里提到高效学习的几个环节： 获取、理解、拓展、纠错、应用、测试。 <br>在我来看，用自己的语言对来整理对一个问题的认识，就是理解和扩展的一种形式，而发在这里也算是一种应用、测试兼顾纠错的形式了。
</p><p>首先来看什么是回归问题，直白来说，就是给定 <br><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(14)" alt="[公式]" eeimg="1" data-formula="\vec X \in \mathcal R_D (1.1)"><br><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(15)" alt="[公式]" eeimg="1" data-formula="Y \in \mathcal R (1.2)"><br><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(16)" alt="[公式]" eeimg="1" data-formula="Y=\boldsymbol f(\vec X) (1.3)"><br>其中映射函数
<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(17)" alt="[公式]" eeimg="1" data-formula="\boldsymbol f">未知，但是我们手上有一堆数据样本<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(18)" alt="[公式]" eeimg="1" data-formula="\mathcal T">，形式如下：
<br><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(19)" alt="[公式]" eeimg="1" data-formula="(\vec X_1, Y_1), (\vec X_2, Y_2), ... (\vec X_n, Y_n)      (2.1)"><br>我们期望从数据样本里推断出映射函数
<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(17)" alt="[公式]" eeimg="1" data-formula="\boldsymbol f">，满足<br><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(20)" alt="[公式]" eeimg="1" data-formula="argmin_fE(f(\vec X_i) - Y_i)^2   (3.1)"><br>即期望推断出的映射函数在数据样本上与真实目标的期望差异尽可能最小化。
</p><p>通常来说，数据样本中每个样本的出现频率都可以认为是1，而我们要推断的映射函数可以认为是
<br>一个线性函数<br><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(21)" alt="[公式]" eeimg="1" data-formula="f(\vec X)=w_1X_1 + w_2X_2 + ... + w_nX_n + \beta(4.1)"><br>其中
<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(22)" alt="[公式]" eeimg="1" data-formula="w_1, w_2, ... w_n, \beta (4.2)">就是我们要推断的关键参数了。
<br>这样的问题就是线性回归(Linear Regression)问题。
</p><p>Ordinary Linear Square的求解方法很直白，结合上面的描述，我们可以将 <img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(23)" alt="[公式]" eeimg="1" data-formula="argmin_fE(f(\vec X) - Y)^2 (5.1)"><br>具像化为求解函数<br><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(24)" alt="[公式]" eeimg="1" data-formula="\sum_{i=1}^{N}(f(\vec X_i) - Y_i)^2 (5.2)"><br>的最小值以及对应的关键参数。</p><p>对于这个目标函数，我们可以通过求导计算[2]，直接得出解析解如下 ：
<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(25)" alt="[公式]" eeimg="1" data-formula="\vec w = (X_TX)^{-1}X^T\vec Y(5.3)"></p><p>当然，这是一个典型的Convex优化问题，也可以通过迭代求优的算法来进行求解，比如Gradient Descent或者Newton法[2]。
</p><p>看起来不错，那么为什么我们还要在OLS的基础上提供了Ridge Regression(L2正则)和Lasso Regression(L1正则)呢?
</p><p>如果说得笼统一些的话，是为了避免over-fit，如果再深入一些，则可以这样来理解：
<br>      不引入正则项的OLS的解很可能会不stable，具体来说，两次不同的采样所采集到的训练数据，用于训练同一个线性回归模型，训练数据的些微差异，最终学出的模型差异可能是巨大的。在[3]里有一个例子：
<br></p><figure><noscript><img src="https://pic4.zhimg.com/50/3720ff6c0dc84fff5000e24cd939cc88_hd.jpg" data-rawwidth="871" data-rawheight="393" class="origin_image zh-lightbox-thumb" width="871" data-original="https://pic4.zhimg.com/3720ff6c0dc84fff5000e24cd939cc88_r.jpg"/></noscript><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/3720ff6c0dc84fff5000e24cd939cc88_hd.jpg" data-rawwidth="871" data-rawheight="393" class="origin_image zh-lightbox-thumb lazy" width="871" data-original="https://pic4.zhimg.com/3720ff6c0dc84fff5000e24cd939cc88_r.jpg" data-actualsrc="https://pic4.zhimg.com/50/3720ff6c0dc84fff5000e24cd939cc88_hd.jpg" data-lazy-status="ok"></figure><br>还是在[3]里，提供了一个定量的证明：
<br><figure><noscript><img src="https://pic2.zhimg.com/50/a2f38e0dad18e77666e30f6f2798e99c_hd.jpg" data-rawwidth="878" data-rawheight="162" class="origin_image zh-lightbox-thumb" width="878" data-original="https://pic2.zhimg.com/a2f38e0dad18e77666e30f6f2798e99c_r.jpg"/></noscript><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/a2f38e0dad18e77666e30f6f2798e99c_hd.jpg" data-rawwidth="878" data-rawheight="162" class="origin_image zh-lightbox-thumb lazy" width="878" data-original="https://pic2.zhimg.com/a2f38e0dad18e77666e30f6f2798e99c_r.jpg" data-actualsrc="https://pic2.zhimg.com/50/a2f38e0dad18e77666e30f6f2798e99c_hd.jpg" data-lazy-status="ok"></figure><br>结果的证明细节，有兴趣的同学可以自己去查阅，这里直接把关键点引用如下：
<br><figure><noscript><img src="https://pic2.zhimg.com/50/a2f38e0dad18e77666e30f6f2798e99c_hd.jpg" data-rawwidth="878" data-rawheight="162" class="origin_image zh-lightbox-thumb" width="878" data-original="https://pic2.zhimg.com/a2f38e0dad18e77666e30f6f2798e99c_r.jpg"/></noscript><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/a2f38e0dad18e77666e30f6f2798e99c_hd.jpg" data-rawwidth="878" data-rawheight="162" class="origin_image zh-lightbox-thumb lazy" width="878" data-original="https://pic2.zhimg.com/a2f38e0dad18e77666e30f6f2798e99c_r.jpg" data-actualsrc="https://pic2.zhimg.com/50/a2f38e0dad18e77666e30f6f2798e99c_hd.jpg" data-lazy-status="ok"></figure><br>一个有名的病态矩阵是Hilbert矩阵[4]，其形如下：
<br><figure><noscript><img src="https://pic4.zhimg.com/50/ed68d8679bc49283f46b9da45e4a7d61_hd.jpg" data-rawwidth="436" data-rawheight="166" class="origin_image zh-lightbox-thumb" width="436" data-original="https://pic4.zhimg.com/ed68d8679bc49283f46b9da45e4a7d61_r.jpg"/></noscript><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/ed68d8679bc49283f46b9da45e4a7d61_hd.jpg" data-rawwidth="436" data-rawheight="166" class="origin_image zh-lightbox-thumb lazy" width="436" data-original="https://pic4.zhimg.com/ed68d8679bc49283f46b9da45e4a7d61_r.jpg" data-actualsrc="https://pic4.zhimg.com/50/ed68d8679bc49283f46b9da45e4a7d61_hd.jpg" data-lazy-status="ok"></figure><p></p><p>再回到我们关于OLS的讨论，我们不难看出，随着训练样本采样次数的增加，采样到病态阵的概率会增多，这样一来学出的模型的稳定性就比较
<br>差。想象一下，今天训练出来的模型跟明天训练出的模型，存在明显的模型权值差异，这看起来并不是一件非常好的事情。
</p><p>那么病态阵的根本原因是什么呢？条件数的描述还是相对有些抽象，所以这里又引入了奇异阵的概念。
</p><p>什么是奇异阵呢？
<br>形式化来说，不存在逆矩阵的方阵（因为OLS的闭式解里，需要求逆的矩阵是
<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(26)" alt="[公式]" eeimg="1" data-formula="X^TX(6.1)"><br>一定是一个方阵，所以这里仅讨论方阵这一特殊形态）就是奇异阵，即
<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(27)" alt="[公式]" eeimg="1" data-formula="A^{-1}A=\mathcal I, \not \exists A^{-1}(6.2)"><br>那么再具体一些，什么样的方阵会不存在逆呢？
<br>     非满秩、矩阵的特征值之和为0、矩阵的行列式为0。
<br>满足这三个条件中的任意一个条件，即可推出方阵为奇异阵，不可求逆，实际上这三个条件是可以互相推导出来的。
<br>而我们又知道，一个方阵的逆矩阵可以通过其伴随矩阵和行列式求出来[7]
<br><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(28)" alt="[公式]" eeimg="1" data-formula="A^{-1}=\frac {1} {|A|} adj(A); adj(A) \in R^{n*n}, (adj(A))_{ij}=(-1)^{i+j}|A_{\setminus j,\setminus i}|(6.3)"></p><p>从这里，其实可以看的出来，对于近似奇异阵，其行列式非常接近于0，所以<br><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(29)" alt="[公式]" eeimg="1" data-formula="\frac 1 {|A|} (6.4)"><br>会是一个非常大的数，这其实反映出来就是在计算A的逆矩阵时，伴随矩阵上的些微变化，会被很大程度上放大，就会导致多次采样出来的训练数据，学出的模型差异很大，这是除了上面提到的条件数以外的另一种比较形象的解释了。 
<br>如果类比普通代数的话，奇异阵就好比是0，0不存在倒数，越接近0的数，其倒数越大，同样，奇异阵不存在逆矩阵，而接近奇异阵的逆矩阵的波动也会比较大。
<br>所以跟奇异阵有多像(数学语言里称为near singularity)，决定了我们的线性回归模型有多么不稳定。
<br>既然知道了奇异阵的性质并且了解到是奇异阵使得从训练数据中学出来的线性回归模型不那么稳定，那么我们是不是可以人为地通过引入一些transformation让
<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(30)" alt="[公式]" eeimg="1" data-formula="X^TX(7.1)">长得跟奇异阵不那么像呢？
<br>我们知道
<br><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(31)" alt="[公式]" eeimg="1" data-formula="|A|=\Pi_i^n \lambda_i(7.2)"><br><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(32)" alt="[公式]" eeimg="1" data-formula="trA=\sum_{i=1}{n}\lambda_i(7.3)"><br><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(33)" alt="[公式]" eeimg="1" data-formula="trA=\sum_{i=1}^nA_{ii}(7.4)"><br>于是一个直观的思路是在方阵<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(34)" alt="[公式]" eeimg="1" data-formula="X^TX">的对角线元素上施加如下的线性变换 <br><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(35)" alt="[公式]" eeimg="1" data-formula="X^TX+\lambda\mathcal  I (7.5)"><br>其中
<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(36)" alt="[公式]" eeimg="1" data-formula="\mathcal I">是单位阵。
<br>有了上面的变换以后，对角线的元素为全零的概率就可以有效降低，也就达到了减少
<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(34)" alt="[公式]" eeimg="1" data-formula="X^TX"><br>near singularity的程度，进而减少线性回归模型不稳定性的目的了。
<br>那么这个变换跟Ridge或是Lasso有什么关系呢？
<br>实际上，
<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(37)" alt="[公式]" eeimg="1" data-formula="\vec w=(X^TX+\lambda \mathcal I)^{-1}X\vec Y(7.6)">(7.6)
正是对下面的
<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(38)" alt="[公式]" eeimg="1" data-formula="\sum_{i=1}^{N}(\vec w\vec X_i - Y_i)^2 + \lambda ||\vec w||_2^2(7.7)"><br>这个优化问题为
<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(39)" alt="[公式]" eeimg="1" data-formula="\vec w">计算闭式解得到的结果(在[2]里OLS的闭式解的推导过程里加入二范数正则项，蛮自然地就会得到7.6里的结果，[8]里也有类似的推论)。
<br>而(7.7)正是Ridge Regression的标准写法。
<br>进一步，Lasso Regression的写法是
<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(40)" alt="[公式]" eeimg="1" data-formula="\sum_{i=1}^{N}(\vec w\vec X_i - Y_i)^2 + \lambda ||\vec w||_1"><br>这实际上也是在原始矩阵上施加了一些变换，期望离奇异阵远一些，另外1范数的引入，使得模型训练的过程本身包含了model selection的功能，在上面的回复里都举出了很多的例子，在一本像样些的ML/DM的教材里也大抵都有着比形象的示例图，在这里我就不再重复了。
<br>一个略微想提一下的是，对于2范数，本质上其实是对误差的高斯先验，而1范数则对应于误差的Laplace先验，这算是另一个理解正则化的视角了。
<br>只不过1范数的引入导致优化目标不再处处连续不能直接求取闭式解，而不得不resort到迭代求优的方法上了，而因为其非处处连续的特点，
即便是在迭代求优的过程中，也变得有些特殊了，这个我们可以在以后讨论OWLQN和LBFGS算法的时后再详细引出来。
</p>[1]. <a href="https://www.zhihu.com/question/38121173" class="internal" data-za-detail-view-id="1043">Linear least squares, Lasso,ridge regression有何本质区别？ - 数据挖掘</a><br>[2]. <a href="https://link.zhihu.com/?target=http%3A//cs229.stanford.edu/notes/cs229-notes1.pdf" class=" external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043"><span class="invisible">http://</span><span class="visible">cs229.stanford.edu/note</span><span class="invisible">s/cs229-notes1.pdf</span><span class="ellipsis"></span></a><br>[3]. 《数值分析导论》(<a href="https://link.zhihu.com/?target=http%3A//book.douban.com/subject/4089812/" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">数值分析导论 (豆瓣)</a> )的 例6.5.3
<br>[4]. 《数值分析导论》(<a href="https://link.zhihu.com/?target=http%3A//book.douban.com/subject/4089812/" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">数值分析导论 (豆瓣)</a> )的例6.5.5
<br>[5]. 关于奇异阵的资料。
<br>[6].  <a href="https://link.zhihu.com/?target=http%3A//book.douban.com/subject/25783654/" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">如何高效学习 (豆瓣)</a><a href="https://link.zhihu.com/?target=http%3A//book.douban.com/subject/25783654/" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">如何高效学习 (豆瓣)</a><br>[7]. <a href="https://link.zhihu.com/?target=http%3A//cs229.stanford.edu/section/cs229-linalg.pdf" class=" external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043"><span class="invisible">http://</span><span class="visible">cs229.stanford.edu/sect</span><span class="invisible">ion/cs229-linalg.pdf</span><span class="ellipsis"></span></a><br>[8]. PRML 3.28</span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/38121173/answer/85813729"><span data-tooltip="发布于 2016-02-11 21:51">编辑于 2016-02-11</span></a></div></div><div class="ContentItem-actions RichContent-actions"><span><button aria-label="赞同 123" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 123</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>12 条评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover28-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover28-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button><button data-zop-retract-question="true" type="button" class="Button ContentItem-action ContentItem-rightButton Button--plain"><span class="RichContent-collapsedText">收起</span><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--ArrowDown ContentItem-arrowIcon is-active" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M12 13L8.285 9.218a.758.758 0 0 0-1.064 0 .738.738 0 0 0 0 1.052l4.249 4.512a.758.758 0 0 0 1.064 0l4.246-4.512a.738.738 0 0 0 0-1.052.757.757 0 0 0-1.063 0L12.002 13z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="5" data-zop="{&quot;authorName&quot;:&quot;Yeung Evan&quot;,&quot;itemId&quot;:104955389,&quot;title&quot;:&quot;Linear least squares, Lasso,ridge regression有何本质区别？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="104955389" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="5" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;104955389&quot;,&quot;upvote_num&quot;:35,&quot;comment_num&quot;:4,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;38121173&quot;,&quot;author_member_hash_id&quot;:&quot;3262be16282302c2060deb22faa2766c&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="Yeung Evan"><meta itemprop="image" content="https://pic3.zhimg.com/02c4a4fbc_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/yiorfun"><meta itemprop="zhihu:followerCount" content="1450"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover18-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover18-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/yiorfun"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/02c4a4fbc_xs.jpg" srcset="https://pic3.zhimg.com/02c4a4fbc_l.jpg 2x" alt="Yeung Evan"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover19-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover19-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/yiorfun">Yeung Evan</a></div></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="ztext AuthorInfo-badgeText">Hell, it's about time.</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button type="button" class="Button Button--plain">35 人赞同了该回答</button></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="35"><meta itemprop="url" content="https://www.zhihu.com/question/38121173/answer/104955389"><meta itemprop="dateCreated" content="2016-06-08T09:00:43.000Z"><meta itemprop="dateModified" content="2017-04-29T22:13:56.000Z"><meta itemprop="commentCount" content="4"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text"><p>不太赞同高票答案关于Lasso产生的motivation，高票答案说：“你可能又要问了，多加的那一项凭什么是模长呢？不能把2-norm改成1-norm吗？”实际上，Tibshirani (1996) 原文中对此也有颇多阐述，Lasso产生的初因并非只是简单地把2修改到1（虽然表现出来确实如此，后面对此详叙）。我尝试在楼上各个答案的基础上，添加一些历史线索。</p><br><p>先回答题主问题：</p><p>最小二乘法的一些讨论可参见 <a href="https://www.zhihu.com/question/23817253/answer/85998617?from=profile_answer_card" class="internal" data-za-detail-view-id="1043">Logistic 回归模型的参数估计为什么不能采用最小二乘法？ - Yeung Evan 的回答</a> ；Linear least squares是对线性回归模型进行参数估计的一种选择，同时我们当然可以从非最小二乘的方法对线性回归模型的参数进行估计。在使用最小二乘法进行求解时，我们会对线性回归模型进行假设，当这些假设在实际中并不满足时，最小二乘法是有问题的。其中很重要的一个假设是要求各个变量要相互独立，而实际样本可能会有较大的共线性（multicollinearity）。从数学上来说，这种情况会造成<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(41)" alt="[公式]" eeimg="1" data-formula="\textbf{X}^\top \textbf{X}">（至少一个）特征值很小，从而估计量的MSE会很大（*）。所以ridge regression就很粗暴，因为<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(41)" alt="[公式]" eeimg="1" data-formula="\textbf{X}^\top \textbf{X}">（至少一个）特征值很小，所以就强行加上一个<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(42)" alt="[公式]" eeimg="1" data-formula="\lambda \textbf{I}">把特征值“掰”大。ridge regression 大约是在1970年Hoerl提出。</p><br><p>后来（或同时）人们考虑回归方程的选择问题，比如说，不一定所有的自变量的数据都需要放入模型，因为有些自变量本身对模型的影响不大，但引入之后却会增加对其他参数的估计所需的计算量以及减少其精度（那个年代计算机还远未普及）。但到底哪些变量需要选，哪些变量需要弃，就是一个颇为棘手的问题。后来比较经典的处理方法就是逐步回归法，又细分为向前逐步回归和向后逐步回归。这一方法的成熟大约也是在70年代左右。再然后，此问题的发展是1993年Frank和Friedman提出的bridge regression，他们提出的对优化目标函数的限制已经是形如<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(43)" alt="[公式]" eeimg="1" data-formula="\sum_j|\beta_j| ^ {\gamma}">的样子了，但可惜他们没有能力进一步对<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(44)" alt="[公式]" eeimg="1" data-formula="\gamma">取特殊值（**）时的情况进行讨论。</p><br><p>最终在1996年Tibshirani把上述两个问题同时解决了：</p><p>（*）MSE偏大时估计的精确度会降低，因为方差偏大；</p><p>（**）<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(45)" alt="[公式]" eeimg="1" data-formula="\gamma=1">时模型具有“挑选”自变量的能力，并将这个情况下的优化问题讲清楚了；
</p><p>这里回到本文开头，看上去只是把 <img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(46)" alt="[公式]" eeimg="1" data-formula="\gamma = 2"> 改成了 <img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(47)" alt="[公式]" eeimg="1" data-formula="\gamma = 1"> ，但实际上背后的工作原理相当复杂，这也是CS的一些研究在严密性上，相对容易忽视的地方：即对变量的选择性从何而来？换句话说，怎么证明把 <img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(44)" alt="[公式]" eeimg="1" data-formula="\gamma"> 数值改了，就把变量压缩了呢？</p><br><p>要回答这个问题是相当难的，仅以ridge回归为例，在考虑 <img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(46)" alt="[公式]" eeimg="1" data-formula="\gamma = 2"> 时，可以通过相当的篇幅证明，在此约束下的回归结果 <img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(48)" alt="[公式]" eeimg="1" data-formula="\hat{\beta}_\text{ridge} &lt; \hat{\beta}_\text{OLS}"> 。这意味着，估计向量的长度缩短了——当然其中的某些分量一定变小了——这就是最初的变量被压缩的雏形。而更一般的变量选择性，需要更艰深的证明，而不是简单一句“把 <img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(44)" alt="[公式]" eeimg="1" data-formula="\gamma"> 值改了，模型就能避免over fitting”——中间的因果逻辑需要相当多的数学推导。</p><br><p>Lasso后续的发展请关注千面人的答案，当然包括真正把Lasso变得“好算”的Lars算法也是该问题的里程碑之一。</p><br><p>上述即是Lasso和ridge regression的一些历史线索，是最自然、最符合一个研究发展脉络的逻辑。在机器学习中还有一些和此问题相关的概念，比如正则化，它们本质上都是相似的，在机器学习中引入正则化这一概念的渊源非我能置喙，但至少说引入正则因子或者惩罚项是为了避免overfitting，是不太自然的。</p></span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/38121173/answer/104955389"><span data-tooltip="发布于 2016-06-08 17:00">编辑于 2017-04-30</span></a></div></div><div class="ContentItem-actions RichContent-actions"><span><button aria-label="赞同 35" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 35</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>4 条评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover39-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover39-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button><button data-zop-retract-question="true" type="button" class="Button ContentItem-action ContentItem-rightButton Button--plain"><span class="RichContent-collapsedText">收起</span><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--ArrowDown ContentItem-arrowIcon is-active" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M12 13L8.285 9.218a.758.758 0 0 0-1.064 0 .738.738 0 0 0 0 1.052l4.249 4.512a.758.758 0 0 0 1.064 0l4.246-4.512a.738.738 0 0 0 0-1.052.757.757 0 0 0-1.063 0L12.002 13z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="6" data-zop="{&quot;authorName&quot;:&quot;姚岑卓&quot;,&quot;itemId&quot;:660460091,&quot;title&quot;:&quot;Linear least squares, Lasso,ridge regression有何本质区别？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="660460091" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="6" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;660460091&quot;,&quot;upvote_num&quot;:57,&quot;comment_num&quot;:2,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;38121173&quot;,&quot;author_member_hash_id&quot;:&quot;0174f2f6bdd730d9773c59c5cb4ed158&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="姚岑卓"><meta itemprop="image" content="https://pic2.zhimg.com/e5156f92c_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/cenzhuo_yao"><meta itemprop="zhihu:followerCount" content="26344"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover29-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover29-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/cenzhuo_yao"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/e5156f92c_xs.jpg" srcset="https://pic2.zhimg.com/e5156f92c_l.jpg 2x" alt="姚岑卓"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover30-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover30-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/cenzhuo_yao">姚岑卓</a></div></div><a class="UserLink-badge" data-tooltip="优秀回答者 · 已认证的个人" href="https://www.zhihu.com/question/48510028" target="_blank" rel="noopener noreferrer"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--BadgeCG" fill="currentColor" viewBox="0 0 24 24" width="18" height="18"><g fill="none" fill-rule="evenodd"><path fill="#0F88EB" d="M12.055 3.172c.21-.165.397-.344.555-.532l-.127.152c.891-1.065 2.319-1.056 3.195.027l-.125-.153c.872 1.08 2.696 1.856 4.083 1.733l-.197.017c1.383-.122 2.386.893 2.239 2.279l.021-.198c-.147 1.381.593 3.218 1.661 4.113l-.152-.127c1.065.891 1.056 2.319-.027 3.195l.154-.125c-1.08.872-1.856 2.696-1.734 4.084l-.017-.197c.123 1.382-.893 2.385-2.279 2.238l.198.021c-1.38-.147-3.218.593-4.113 1.661l.127-.152c-.891 1.065-2.319 1.057-3.195-.027l.125.154a3.716 3.716 0 0 0-.503-.506c.975-.77 2.422-1.25 3.559-1.13l-.198-.021c1.386.147 2.402-.856 2.279-2.238l.017.197c-.122-1.388.655-3.212 1.734-4.084l-.154.125c1.083-.876 1.092-2.304.027-3.195l.152.127c-1.068-.895-1.808-2.732-1.66-4.113l-.022.198c.147-1.386-.856-2.4-2.24-2.279l.198-.017c-1.159.103-2.622-.422-3.58-1.227z"></path><path fill="#FF9500" d="M19.21 10.483l.151.127c-1.069-.895-1.809-2.732-1.662-4.113l-.02.197c.146-1.386-.857-2.4-2.24-2.278l.197-.018c-1.387.124-3.21-.653-4.083-1.732l.125.153c-.876-1.083-2.304-1.092-3.195-.028l.127-.152c-.894 1.068-2.733 1.808-4.113 1.663l.198.02c-1.386-.147-2.4.857-2.279 2.24L2.4 6.363c.122 1.387-.655 3.21-1.734 4.084l.154-.126c-1.083.877-1.092 2.304-.027 3.194L.64 13.39c1.068.894 1.808 2.733 1.661 4.112l.021-.196c-.147 1.385.856 2.401 2.24 2.28l-.198.015c1.387-.122 3.211.655 4.083 1.734l-.124-.154c.184.228.396.397.62.53a1.89 1.89 0 0 0 1.972 0c.215-.127.421-.287.602-.503l-.127.152c.895-1.068 2.733-1.808 4.113-1.66l-.198-.022c1.387.147 2.402-.856 2.28-2.238l.016.197c-.122-1.389.655-3.212 1.734-4.084l-.154.124c1.083-.876 1.092-2.303.028-3.194"></path><path fill="#FFF" d="M14.946 11.082l-2.362 2.024.721 3.025c.128.534-.144.738-.617.45l-2.654-1.623L7.38 16.58c-.468.286-.746.09-.617-.449l.721-3.025-2.362-2.024c-.417-.357-.317-.68.236-.726l3.101-.248 1.194-2.872c.211-.507.55-.512.763 0l1.195 2.872 3.1.248c.547.044.657.365.236.726"></path></g></svg></span></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="AuthorInfo-badgeText"><span><span><a href="https://www.zhihu.com/people/cenzhuo_yao/creations/19558740">统计学</a> </span>话题</span>的优秀回答者</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button type="button" class="Button Button--plain">57 人赞同了该回答</button></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="57"><meta itemprop="url" content="https://www.zhihu.com/question/38121173/answer/660460091"><meta itemprop="dateCreated" content="2019-04-23T04:14:38.000Z"><meta itemprop="dateModified" content="2019-04-23T04:16:05.000Z"><meta itemprop="commentCount" content="2"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text"><p><b>结论：</b>Lasso 和 Ridge 都是基于线性回归模型<b>防止过拟合</b>的工具。其中，<b>Lasso</b> 因为利用L1惩罚导致<b>较小的回归系数将变为0</b>，<b>Ridge</b> 因为利用L2惩罚导致<b>所有回归系数都减小</b>。由于Lasso会让某些回归参数为0而进行特征选取，所以<b>工业界比较偏向Lasso回归</b>。在实际利用中，只要应用Lasso 或者 Ridge，都需要对特征进行<b>标准化</b>。（我们假设下文讨论时，所有特征都已经标准化）。</p><h2>优化函数</h2><p>标准线性回归是希望找到最优的特征线性组合来预测。它通过寻找最优的回归系数来最小化残差平方和。</p><p><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(49)" alt="[公式]" eeimg="1" data-formula="\hat{\beta}_{OLS} = \arg\min_{\beta} \{\sum_i(y_i - x_i\beta)^2\}"></p><p>在特征数量较小时，线性回归表现不错。但是当特征数量较大时，由于样本数量太少，线性回归有可能会过拟合——模型在训练集表现出色，但无法对测试集的数据有较好的表现。而过拟合的一个表现之一就是<b>回归参数过大</b>。</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/50/v2-3c69817baf336aecbf361545109c174e_hd.jpg" data-size="normal" data-rawwidth="300" data-rawheight="204" data-default-watermark-src="https://pic4.zhimg.com/50/v2-ddd3e05a5ef98509f76387585d4348dc_hd.jpg" class="content_image" width="300"/></noscript><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-3c69817baf336aecbf361545109c174e_hd.jpg" data-size="normal" data-rawwidth="300" data-rawheight="204" data-default-watermark-src="https://pic4.zhimg.com/50/v2-ddd3e05a5ef98509f76387585d4348dc_hd.jpg" class="content_image lazy" width="300" data-actualsrc="https://pic1.zhimg.com/50/v2-3c69817baf336aecbf361545109c174e_hd.jpg" data-lazy-status="ok"><figcaption>黑线是正常线性回归，蓝色是过拟合的线性回归</figcaption></figure><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/50/v2-b5b2c5d5316acc2257a88c121ad2a87b_hd.jpg" data-size="normal" data-rawwidth="507" data-rawheight="170" data-default-watermark-src="https://pic3.zhimg.com/50/v2-5e2140edc33602dfa897760f3f1514d1_hd.jpg" class="origin_image zh-lightbox-thumb" width="507" data-original="https://pic1.zhimg.com/v2-b5b2c5d5316acc2257a88c121ad2a87b_r.jpg"/></noscript><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-b5b2c5d5316acc2257a88c121ad2a87b_hd.jpg" data-size="normal" data-rawwidth="507" data-rawheight="170" data-default-watermark-src="https://pic3.zhimg.com/50/v2-5e2140edc33602dfa897760f3f1514d1_hd.jpg" class="origin_image zh-lightbox-thumb lazy" width="507" data-original="https://pic1.zhimg.com/v2-b5b2c5d5316acc2257a88c121ad2a87b_r.jpg" data-actualsrc="https://pic1.zhimg.com/50/v2-b5b2c5d5316acc2257a88c121ad2a87b_hd.jpg" data-lazy-status="ok"><figcaption>过拟合线性模型（右）通常会有过大的回归参数</figcaption></figure><p>正因如此，当我们无法避免的要输入许多特征来进行线性拟合时，我们往往会限制回归参数不要过大。其中有两种描述回归参数大小的方法：（1）用每个回归参数绝对值的和 <img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(50)" alt="[公式]" eeimg="1" data-formula="\sum_i|\beta_i|"> ，或者（2）用每个回归参数平方的和 <img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(51)" alt="[公式]" eeimg="1" data-formula="\sum_i(\beta_i)^2"> 。它们分别代表着 L1 和 L2 惩罚。因此，我们就有两种优化函数：</p><ul><li>Lasso (L1) 回归： <img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(52)" alt="[公式]" eeimg="1" data-formula="\hat{\beta}_{Lasso} = \arg\min_{\beta} \{\sum_i(y_i - x_i\beta)^2+\alpha\sum_j|\beta_j|\}"></li><li>Ridge (L2) 回归： <img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(53)" alt="[公式]" eeimg="1" data-formula="\hat{\beta}_{Ridge} = \arg\min_{\beta} \{\sum_i(y_i - x_i\beta)^2+\alpha\sum_j(\beta_j)^2\}"></li></ul><h2>结果</h2><p>正因为两者在优化函数的差异，导致两者结果不完全相同。Lasso在乎的是参数的绝对大小，而Ridge在乎的是参数平方的大小。在不重要的特征对应的参数已经被缩减很小的情况下，Ridge容易“放过”这个参数，转而去放缩其它特征的参数。因为一个小数的平方会更小，而Ridge认为此时不值得再对限制它的大小。相反，Lasso由于看的是绝对值大小，因此它仅仅在乎相关性。当需要继续限制参数大小时，它不介意继续“压缩”这个参数的值。这就导致，<b>Ridge往往会导致所有参数都缩小但不至于消失，而Lasso会进行特征选择——不重要的参数会被缩减为0</b>。</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/50/v2-2dcdb772b64089720a95c618b12ec326_hd.jpg" data-size="normal" data-rawwidth="490" data-rawheight="273" data-default-watermark-src="https://pic4.zhimg.com/50/v2-0a4f91309a3fc98bafa7b23dafba8da9_hd.jpg" class="origin_image zh-lightbox-thumb" width="490" data-original="https://pic4.zhimg.com/v2-2dcdb772b64089720a95c618b12ec326_r.jpg"/></noscript><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-2dcdb772b64089720a95c618b12ec326_hd.jpg" data-size="normal" data-rawwidth="490" data-rawheight="273" data-default-watermark-src="https://pic4.zhimg.com/50/v2-0a4f91309a3fc98bafa7b23dafba8da9_hd.jpg" class="origin_image zh-lightbox-thumb lazy" width="490" data-original="https://pic4.zhimg.com/v2-2dcdb772b64089720a95c618b12ec326_r.jpg" data-actualsrc="https://pic4.zhimg.com/50/v2-2dcdb772b64089720a95c618b12ec326_hd.jpg" data-lazy-status="ok"><figcaption>由于对回归参数的惩罚不同（蓝色），Lasso （左）倾向于将不重要的参数减小到0，而Ridge（右）倾向同时放缩所有参数。 Source: The Elements of Statistical Learning</figcaption></figure><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/50/v2-3770150cd5608ad4d4fa80d67776dfbf_hd.jpg" data-size="normal" data-rawwidth="294" data-rawheight="141" data-default-watermark-src="https://pic3.zhimg.com/50/v2-3f95cab86a28bc11feff2acecf06ad0d_hd.jpg" class="content_image" width="294"/></noscript><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-3770150cd5608ad4d4fa80d67776dfbf_hd.jpg" data-size="normal" data-rawwidth="294" data-rawheight="141" data-default-watermark-src="https://pic3.zhimg.com/50/v2-3f95cab86a28bc11feff2acecf06ad0d_hd.jpg" class="content_image lazy" width="294" data-actualsrc="https://pic4.zhimg.com/50/v2-3770150cd5608ad4d4fa80d67776dfbf_hd.jpg" data-lazy-status="ok"><figcaption>Lasso（右）会将较小的回归参数压缩为0，但Ridge （左）只会按比例压缩回归参数。Source: The Elements of Statistical Learning</figcaption></figure><h2>工业应用</h2><p>在工业应用上，我们一般倾向于用<b>Lasso回归</b>。因为工业上的数据经常有很多的特征，线性回归往往一定会过拟合。而在防止过拟合中，由于Lasso会让某些回归参数为0而进行特征选取。我们可以对（大多数）没有用的数据进行实现筛选。筛选后的数据往往会小很多，导致更快的计算速度和更小的内存负担。Google 曾经发表过一篇论文，将Lasso回归和在线机器学习结合起来提升个性化广告模型。有兴趣的读者可以阅读一下。</p><a target="_blank" href="https://link.zhihu.com/?target=https%3A//storage.googleapis.com/pub-tools-public-publication-data/pdf/41159.pdf" data-draft-node="block" data-draft-type="link-card" class="LinkCard LinkCard--noImage" data-za-detail-view-id="172"><span class="LinkCard-content"><span class="LinkCard-text"><span class="LinkCard-title" data-text="true">https://storage.googleapis.com/pub-tools-public-publication-data/pdf/41159.pdf</span><span class="LinkCard-meta"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--InsertLink" fill="currentColor" viewBox="0 0 24 24" width="17" height="17"><path d="M6.77 17.23c-.905-.904-.94-2.333-.08-3.193l3.059-3.06-1.192-1.19-3.059 3.058c-1.489 1.489-1.427 3.954.138 5.519s4.03 1.627 5.519.138l3.059-3.059-1.192-1.192-3.059 3.06c-.86.86-2.289.824-3.193-.08zm3.016-8.673l1.192 1.192 3.059-3.06c.86-.86 2.289-.824 3.193.08.905.905.94 2.334.08 3.194l-3.059 3.06 1.192 1.19 3.059-3.058c1.489-1.489 1.427-3.954-.138-5.519s-4.03-1.627-5.519-.138L9.786 8.557zm-1.023 6.68c.33.33.863.343 1.177.029l5.34-5.34c.314-.314.3-.846-.03-1.176-.33-.33-.862-.344-1.176-.03l-5.34 5.34c-.314.314-.3.846.03 1.177z" fill-rule="evenodd"></path></svg></span>storage.googleapis.com</span></span><span class="LinkCard-imageCell"><div class="LinkCard-image LinkCard-image--default"><svg class="Zi Zi--Browser" fill="currentColor" viewBox="0 0 24 24" width="32" height="32"><path d="M11.991 3C7.023 3 3 7.032 3 12s4.023 9 8.991 9C16.968 21 21 16.968 21 12s-4.032-9-9.009-9zm6.237 5.4h-2.655a14.084 14.084 0 0 0-1.242-3.204A7.227 7.227 0 0 1 18.228 8.4zM12 4.836A12.678 12.678 0 0 1 13.719 8.4h-3.438A12.678 12.678 0 0 1 12 4.836zM5.034 13.8A7.418 7.418 0 0 1 4.8 12c0-.621.09-1.224.234-1.8h3.042A14.864 14.864 0 0 0 7.95 12c0 .612.054 1.206.126 1.8H5.034zm.738 1.8h2.655a14.084 14.084 0 0 0 1.242 3.204A7.188 7.188 0 0 1 5.772 15.6zm2.655-7.2H5.772a7.188 7.188 0 0 1 3.897-3.204c-.54.999-.954 2.079-1.242 3.204zM12 19.164a12.678 12.678 0 0 1-1.719-3.564h3.438A12.678 12.678 0 0 1 12 19.164zm2.106-5.364H9.894A13.242 13.242 0 0 1 9.75 12c0-.612.063-1.215.144-1.8h4.212c.081.585.144 1.188.144 1.8 0 .612-.063 1.206-.144 1.8zm.225 5.004c.54-.999.954-2.079 1.242-3.204h2.655a7.227 7.227 0 0 1-3.897 3.204zm1.593-5.004c.072-.594.126-1.188.126-1.8 0-.612-.054-1.206-.126-1.8h3.042c.144.576.234 1.179.234 1.8s-.09 1.224-.234 1.8h-3.042z"></path></svg></div></span></span></a><p></p></span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/38121173/answer/660460091"><span data-tooltip="发布于 2019-04-23 12:14">编辑于 2019-04-23</span></a></div></div><div class="ContentItem-actions RichContent-actions"><span><button aria-label="赞同 57" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 57</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>2 条评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover42-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover42-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button><button data-zop-retract-question="true" type="button" class="Button ContentItem-action ContentItem-rightButton Button--plain"><span class="RichContent-collapsedText">收起</span><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--ArrowDown ContentItem-arrowIcon is-active" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M12 13L8.285 9.218a.758.758 0 0 0-1.064 0 .738.738 0 0 0 0 1.052l4.249 4.512a.758.758 0 0 0 1.064 0l4.246-4.512a.738.738 0 0 0 0-1.052.757.757 0 0 0-1.063 0L12.002 13z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="7" data-zop="{&quot;authorName&quot;:&quot;匿名用户&quot;,&quot;itemId&quot;:75252624,&quot;title&quot;:&quot;Linear least squares, Lasso,ridge regression有何本质区别？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="75252624" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="7" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;75252624&quot;,&quot;upvote_num&quot;:7,&quot;comment_num&quot;:3,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;38121173&quot;,&quot;author_member_hash_id&quot;:&quot;0&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="匿名用户"><meta itemprop="image" content="https://pic1.zhimg.com/aadd7b895_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/"><meta itemprop="zhihu:followerCount" content="0"><span class="UserLink AuthorInfo-avatarWrapper"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/aadd7b895_xs.jpg" srcset="https://pic1.zhimg.com/aadd7b895_l.jpg 2x" alt="匿名用户"></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name">匿名用户</span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button type="button" class="Button Button--plain">7 人赞同了该回答</button></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="7"><meta itemprop="url" content="https://www.zhihu.com/question/38121173/answer/75252624"><meta itemprop="dateCreated" content="2015-12-04T10:50:14.000Z"><meta itemprop="dateModified" content="2015-12-04T10:50:14.000Z"><meta itemprop="commentCount" content="3"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text">先验假设不同</span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/38121173/answer/75252624"><span data-tooltip="发布于 2015-12-04 18:50">发布于 2015-12-04</span></a></div></div><div class="ContentItem-actions"><span><button aria-label="赞同 7" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 7</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>3 条评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover40-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover40-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button></div></div></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="8" data-zop="{&quot;authorName&quot;:&quot;知乎用户&quot;,&quot;itemId&quot;:75306040,&quot;title&quot;:&quot;Linear least squares, Lasso,ridge regression有何本质区别？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="75306040" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="8" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;75306040&quot;,&quot;upvote_num&quot;:40,&quot;comment_num&quot;:13,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;38121173&quot;,&quot;author_member_hash_id&quot;:&quot;93a95105a3c691d7b42fc69c76c9e23d&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="知乎用户"><meta itemprop="image" content="https://pic4.zhimg.com/da8e974dc_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/"><meta itemprop="zhihu:followerCount" content="2676"><span class="UserLink AuthorInfo-avatarWrapper"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/da8e974dc_xs.jpg" srcset="https://pic4.zhimg.com/da8e974dc_l.jpg 2x" alt="知乎用户"></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name">知乎用户</span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button type="button" class="Button Button--plain">40 人赞同了该回答</button></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="40"><meta itemprop="url" content="https://www.zhihu.com/question/38121173/answer/75306040"><meta itemprop="dateCreated" content="2015-12-04T21:46:51.000Z"><meta itemprop="dateModified" content="2016-12-09T21:21:17.000Z"><meta itemprop="commentCount" content="13"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text"><p>J(theta)是模型的cost function<br>y是实际数据<br>y_hat是你的模型推算出来的数据<br>theta是模型的参量</p><p>最原始的regression:<br>J(theta) ＝ sum((y - y_hat)^2)</p><p>加上L1(也就是LASSO):<br>J_lasso(theta) = sum((y - y_hat)^2) + sum(abs(theta))</p><p>加上L2(也就是Ridge):<br>J_ridge(theta) = sum((y - y_hat)^2) + sum(theta^2)</p><p>对于两个参量的cost function来说，把模型视觉化出来就是这样的：<br></p><figure><noscript><img src="https://pic4.zhimg.com/50/c822c410dd5132724d7c10d5381cfd7c_hd.jpg" data-rawwidth="500" data-rawheight="250" class="origin_image zh-lightbox-thumb" width="500" data-original="https://pic4.zhimg.com/c822c410dd5132724d7c10d5381cfd7c_r.jpg"/></noscript><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/c822c410dd5132724d7c10d5381cfd7c_hd.jpg" data-rawwidth="500" data-rawheight="250" class="origin_image zh-lightbox-thumb lazy" width="500" data-original="https://pic4.zhimg.com/c822c410dd5132724d7c10d5381cfd7c_r.jpg" data-actualsrc="https://pic4.zhimg.com/50/c822c410dd5132724d7c10d5381cfd7c_hd.jpg" data-lazy-status="ok"></figure><br>那个长得像年轮蛋糕的圈圈就是J(theta)，左边的圆圈是L2，右边的方块是L1。也就是说你在做最优化的时候，你在原有的空间里叠加了一个圆圈／方块，这样就会有一种“力”把你的theta向0拉，这也是为什么叫LASSO的原因。<p></p>L1和L2什么时候用呢？<br>如果你有很多features，不知道哪个最重要，那么你可以用L1，因为L1会更鼓励theta为0。这样你可以直接用非0的theta，那么模型的复杂程度会降低很多</span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/38121173/answer/75306040"><span data-tooltip="发布于 2015-12-05 05:46">编辑于 2016-12-10</span></a></div></div><div class="ContentItem-actions RichContent-actions"><span><button aria-label="赞同 40" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 40</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>13 条评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover57-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover57-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button><button data-zop-retract-question="true" type="button" class="Button ContentItem-action ContentItem-rightButton Button--plain"><span class="RichContent-collapsedText">收起</span><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--ArrowDown ContentItem-arrowIcon is-active" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M12 13L8.285 9.218a.758.758 0 0 0-1.064 0 .738.738 0 0 0 0 1.052l4.249 4.512a.758.758 0 0 0 1.064 0l4.246-4.512a.738.738 0 0 0 0-1.052.757.757 0 0 0-1.063 0L12.002 13z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="9" data-zop="{&quot;authorName&quot;:&quot;高大花&quot;,&quot;itemId&quot;:75198908,&quot;title&quot;:&quot;Linear least squares, Lasso,ridge regression有何本质区别？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="75198908" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="9" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;75198908&quot;,&quot;upvote_num&quot;:33,&quot;comment_num&quot;:3,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;38121173&quot;,&quot;author_member_hash_id&quot;:&quot;a7c76c09ded62c27de02fcbb1d07d559&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="高大花"><meta itemprop="image" content="https://pic3.zhimg.com/1bd1bc4811ae7e6edf776866a67ae50e_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/gao-ming-zhe-90"><meta itemprop="zhihu:followerCount" content="7"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover34-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover34-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/gao-ming-zhe-90"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/1bd1bc4811ae7e6edf776866a67ae50e_xs.jpg" srcset="https://pic3.zhimg.com/1bd1bc4811ae7e6edf776866a67ae50e_l.jpg 2x" alt="高大花"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover35-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover35-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/gao-ming-zhe-90">高大花</a></div></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="ztext AuthorInfo-badgeText">退休青年</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button type="button" class="Button Button--plain">33 人赞同了该回答</button></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="33"><meta itemprop="url" content="https://www.zhihu.com/question/38121173/answer/75198908"><meta itemprop="dateCreated" content="2015-12-04T04:58:52.000Z"><meta itemprop="dateModified" content="2015-12-04T05:02:47.000Z"><meta itemprop="commentCount" content="3"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text"><p>题主这个问题算是比较基础的，那我回答也详细点好了。个人理解，不当之初欢迎各位大牛指正。<br>知乎公式编辑器导致意外换行问题不知如何解决……各位看官帮帮忙。</p><p> 1. Least-squares（最小二乘法）是最经典的机器学习算法，后续的大部分机器学习算法（包括题主提到的Lasso,ridge regression）都是在其基础上发展而来的。Linear model即<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(54)" alt="[公式]" eeimg="1" data-formula="f _{\theta}(x)=\sum_{j=1}^{b}{\theta _{j}\phi _{j}(x)} ">，只要求得其参数<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(55)" alt="[公式]" eeimg="1" data-formula="\left\{ \theta  \right\} _{j=1}^{b} ">，便可以得到自变量<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(56)" alt="[公式]" eeimg="1" data-formula="x">与因变量<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(57)" alt="[公式]" eeimg="1" data-formula="y">的映射关系。因此有监督回归的任务就是通过<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(58)" alt="[公式]" eeimg="1" data-formula="n">个成对的训练样本<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(59)" alt="[公式]" eeimg="1" data-formula="\left\{ (x_{i}, y_{i} )\right\} _{i=1}^{n} ">来求得学习模型的参数<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(55)" alt="[公式]" eeimg="1" data-formula="\left\{ \theta  \right\} _{j=1}^{b} ">。
</p><p>2. 最小二乘法是<b>对模型的输出<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(60)" alt="[公式]" eeimg="1" data-formula="f _{\theta}(x_{i} )">和训练样本的输出<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(61)" alt="[公式]" eeimg="1" data-formula="\left\{y_{i} \right\} _{i=1}^{n} ">的平方误差<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(62)" alt="[公式]" eeimg="1" data-formula="\sum_{i=1}^{n}{\left( f _{\theta}(x_{i} )-y_{i}  \right) } ^{2} ">为最小时的参数<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(63)" alt="[公式]" eeimg="1" data-formula="\theta _{LS}">进行学习</b>。<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(62)" alt="[公式]" eeimg="1" data-formula="\sum_{i=1}^{n}{\left( f _{\theta}(x_{i} )-y_{i}  \right) } ^{2} ">称之为损失函数。随着学习模型<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(64)" alt="[公式]" eeimg="1" data-formula="f _{\theta}(x)">复杂度的提高，这种经典最小二乘法存在的诸多缺陷也表露出来，其中一个重要的问题就是对训练数据的过拟合（overfitting）。过拟合的原因被认为是学习模型相比真实模型过于复杂。因此<b>为解决过拟合问题，可在损失函数的后面加上一个约束条件从而限制模型的复杂度，这个约束条件即为正则化项（regularizer）。</b>典型的正则化方法就是引入<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(65)" alt="[公式]" eeimg="1" data-formula="l_{2} ">约束，<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(65)" alt="[公式]" eeimg="1" data-formula="l_{2} ">约束的约束条件是参数的<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(55)" alt="[公式]" eeimg="1" data-formula="\left\{ \theta  \right\} _{j=1}^{b} ">的<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(65)" alt="[公式]" eeimg="1" data-formula="l_{2} ">范数小于某个阈值，此时<b>最小二乘学习法的学习目标变为使学习结果的平方误差与正则化项的和最小</b>。虽然这种方法对于预防过拟合非常有效，但当学习模型中的参数特别多时，求解各参数需要花费大量的时间。因此，一种能够把大部分参数都设置为0的学习方法便被提出，就是稀疏学习，Lasso回归就是典型的稀疏学习。
</p><p>3. Lasso回归的<b>本质是<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(66)" alt="[公式]" eeimg="1" data-formula="l_{1} ">约束的最小二乘法，</b>即学习模型参数<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(55)" alt="[公式]" eeimg="1" data-formula="\left\{ \theta  \right\} _{j=1}^{b} ">的<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(66)" alt="[公式]" eeimg="1" data-formula="l_{1} ">范数小于某个阈值。想象一下，<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(65)" alt="[公式]" eeimg="1" data-formula="l_{2} ">约束下的参数分布趋向于以圆点为中心的圆周内部，而<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(66)" alt="[公式]" eeimg="1" data-formula="l_{1} ">约束下的参数则集中分布在各坐标轴附近，因此<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(66)" alt="[公式]" eeimg="1" data-formula="l_{1} ">约束能够有效的将若干参数的解收敛到0。<b>约束的求解相对于<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(65)" alt="[公式]" eeimg="1" data-formula="l_{2} ">约束更为复杂</b>，通常解法是需要数个<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(65)" alt="[公式]" eeimg="1" data-formula="l_{2} ">约束求解的迭代过程。因此，题主所问的<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(66)" alt="[公式]" eeimg="1" data-formula="l_{1} ">和<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(65)" alt="[公式]" eeimg="1" data-formula="l_{2} ">约束如何选取的问题，个人认为如果不考虑<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(67)" alt="[公式]" eeimg="1" data-formula="l_{1} +l_{2} ">约束的弹性网回归，<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(66)" alt="[公式]" eeimg="1" data-formula="l_{1} ">约束更适合参数较多或需要特征提取的情况下，<img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(65)" alt="[公式]" eeimg="1" data-formula="l_{2} ">约束更适合模型简单或是不想求解那么复杂的情况下（MATLAB算这两种约束都很简单）。</p><p>4. 岭回归也是最小二乘法正则化方法的一种延伸，特点是以损失无偏性为代价换取数值稳定性。</p>希望能帮到你。</span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/38121173/answer/75198908"><span data-tooltip="发布于 2015-12-04 12:58">编辑于 2015-12-04</span></a></div></div><div class="ContentItem-actions"><span><button aria-label="赞同 33" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 33</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>3 条评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover43-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover43-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button></div></div></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="10" data-zop="{&quot;authorName&quot;:&quot;知乎用户&quot;,&quot;itemId&quot;:674534695,&quot;title&quot;:&quot;Linear least squares, Lasso,ridge regression有何本质区别？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="674534695" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="10" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;674534695&quot;,&quot;upvote_num&quot;:0,&quot;comment_num&quot;:0,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;38121173&quot;,&quot;author_member_hash_id&quot;:&quot;49d238c14c4ede4de75f21a21776a69d&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="知乎用户"><meta itemprop="image" content="https://pic4.zhimg.com/da8e974dc_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/"><meta itemprop="zhihu:followerCount" content="676"><span class="UserLink AuthorInfo-avatarWrapper"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/da8e974dc_xs.jpg" srcset="https://pic4.zhimg.com/da8e974dc_l.jpg 2x" alt="知乎用户"></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name">知乎用户</span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="AuthorInfo-badgeText">计算机硕士在读</div></div></div></div></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="0"><meta itemprop="url" content="https://www.zhihu.com/question/38121173/answer/674534695"><meta itemprop="dateCreated" content="2019-05-06T12:06:58.000Z"><meta itemprop="dateModified" content="2019-05-06T12:21:07.000Z"><meta itemprop="commentCount" content="0"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text"><p>Bayesian角度就是不同的先验，附赠表格一张:</p><p><b>likelihood  |  Prior  |  Name </b></p><p>Gaussian      Uniform  Least Squares 标准最小二乘</p><p>Gaussian      Gaussian  Ridge             岭回归</p><p>Gaussian      Laplace     Lasso</p><p>Laplace        Uniform    Robust regression</p><p>Student        Uniform   Robust regression </p><p><i>Murphy, Kevin P.Machine learning: a probabilistic perspective. MIT press, 2012.</i></p></span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/38121173/answer/674534695"><span data-tooltip="发布于 2019-05-06 20:06">编辑于 2019-05-06</span></a></div></div><div class="ContentItem-actions"><span><button aria-label="赞同 0" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>添加评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover58-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover58-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button></div></div></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="11" data-zop="{&quot;authorName&quot;:&quot;yujiao li&quot;,&quot;itemId&quot;:258394363,&quot;title&quot;:&quot;Linear least squares, Lasso,ridge regression有何本质区别？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="258394363" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="11" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;258394363&quot;,&quot;upvote_num&quot;:15,&quot;comment_num&quot;:2,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;38121173&quot;,&quot;author_member_hash_id&quot;:&quot;7aa3424f3586ccf4cb8a0331b80cc27c&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="yujiao li"><meta itemprop="image" content="https://pic4.zhimg.com/v2-d71e9f77d79840bb15945ed7281aebfb_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/yujiao-li"><meta itemprop="zhihu:followerCount" content="17"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover44-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover44-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/yujiao-li"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-d71e9f77d79840bb15945ed7281aebfb_xs.jpg" srcset="https://pic4.zhimg.com/v2-d71e9f77d79840bb15945ed7281aebfb_l.jpg 2x" alt="yujiao li"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover45-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover45-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/yujiao-li">yujiao li</a></div></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="ztext AuthorInfo-badgeText">PhD student in micro-data analysis</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button type="button" class="Button Button--plain">15 人赞同了该回答</button></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="15"><meta itemprop="url" content="https://www.zhihu.com/question/38121173/answer/258394363"><meta itemprop="dateCreated" content="2017-11-11T13:06:28.000Z"><meta itemprop="dateModified" content="2017-11-11T13:06:28.000Z"><meta itemprop="commentCount" content="2"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text"><p>鉴于各种专业术语，我这就逗逼一下：</p><p>故事是这样的：我们的自变量中，有些不中用，有些长得跟别人一样（多重共线）。那怎么把没用的自变量搞死，把那些copy别人明星脸的捣蛋自变量拆穿呢？</p><p>Ridge的出场就是因为那些整容女变量的，和人家长得一样，导致这个模型的估计不稳定，即：换一套数据，变量的系数估计就相差巨大，有时候大到不可理喻。那咋整呢？给扣个帽子！你不是大么，我设定个门槛，太大就滚吧。 所以楼上画的那个以原点为圆心的圈，就是限制系数不能出圈的意思。</p><p>Lasso的故事，大家都说了叫变量选择。那怎么个选择法子呢？还是上面那个例图，它不是不让出圈，是不让出那个正方形的牢笼。大家问了，不就是圈和方形的差别，有啥大不了的？我就说，其实大了去了，你想啊：其实为了让残差小，我们得到的beta系数估计值其实都是在圆圈或者方形的边界上的，而lasso是取得四个角上的值，则必有横坐标，或者纵坐标为0. 为零的那个倒霉的系数就这样被剔除出局了，官方说法叫变量选择。而相比人家ridge很圆滑的处世之道了，也就是让那些不中用的家伙系数渺小而已，不彻底搞死到0。</p><p class="ztext-empty-paragraph"><br></p><p>＝＝＝＝＝ </p><p>这是楼上的图，不是原创啊。</p><figure><noscript><img src="https://pic2.zhimg.com/50/v2-8b8ccbf28dc3968c50af65bb5000adb2_hd.jpg" data-caption="" data-rawwidth="500" data-rawheight="250" class="origin_image zh-lightbox-thumb" width="500" data-original="https://pic2.zhimg.com/v2-8b8ccbf28dc3968c50af65bb5000adb2_r.jpg"/></noscript><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-8b8ccbf28dc3968c50af65bb5000adb2_hd.jpg" data-caption="" data-rawwidth="500" data-rawheight="250" class="origin_image zh-lightbox-thumb lazy" width="500" data-original="https://pic2.zhimg.com/v2-8b8ccbf28dc3968c50af65bb5000adb2_r.jpg" data-actualsrc="https://pic2.zhimg.com/50/v2-8b8ccbf28dc3968c50af65bb5000adb2_hd.jpg" data-lazy-status="ok"></figure><p></p></span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/38121173/answer/258394363"><span data-tooltip="发布于 2017-11-11 21:06">发布于 2017-11-11</span></a></div></div><div class="ContentItem-actions"><span><button aria-label="赞同 15" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 15</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>2 条评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover59-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover59-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button></div></div></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="12" data-zop="{&quot;authorName&quot;:&quot;理查德帕克&quot;,&quot;itemId&quot;:80506502,&quot;title&quot;:&quot;Linear least squares, Lasso,ridge regression有何本质区别？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="80506502" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="12" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;80506502&quot;,&quot;upvote_num&quot;:8,&quot;comment_num&quot;:0,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;38121173&quot;,&quot;author_member_hash_id&quot;:&quot;fa110e669b2083b7dec0fd49856686ff&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="理查德帕克"><meta itemprop="image" content="https://pic2.zhimg.com/v2-9a04557cb4f06b4f163427b8e27170fc_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/si-tu-gong-yuan"><meta itemprop="zhihu:followerCount" content="821"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover47-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover47-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/si-tu-gong-yuan"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-9a04557cb4f06b4f163427b8e27170fc_xs.jpg" srcset="https://pic2.zhimg.com/v2-9a04557cb4f06b4f163427b8e27170fc_l.jpg 2x" alt="理查德帕克"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover48-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover48-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/si-tu-gong-yuan">理查德帕克</a></div></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="ztext AuthorInfo-badgeText">Viva la Vida</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button type="button" class="Button Button--plain">8 人赞同了该回答</button></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="8"><meta itemprop="url" content="https://www.zhihu.com/question/38121173/answer/80506502"><meta itemprop="dateCreated" content="2016-01-07T19:22:06.000Z"><meta itemprop="dateModified" content="2016-01-07T19:22:06.000Z"><meta itemprop="commentCount" content="0"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text">这个问题可以从以下几个方面回答：<br>1：optimization的角度，即1楼；<br>2：regularization的角度，L1,L2 regularization都是为了预防或者减少过拟合，机器学习的过程本质上是从一个假设空间(函数空间)中根据一定的算法选择某个最优的假设(函数)的过程，加上一些regularization就是限制了我们所选择的假设(函数)只能在一定的范围内选取，对线性回归而言，过拟合的一个明显标志就是某些权重的值会很大，加上一些限制可以减少这样的情况的发生，L1的解通常稀疏，L2从计算角度而言更方便；<br>3：从贝叶斯角度而言，不同的regularization代表了对权重使用不同的先验分布，这一点建议看PRML的前面的章节，由更具体的描述；</span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/38121173/answer/80506502"><span data-tooltip="发布于 2016-01-08 03:22">发布于 2016-01-08</span></a></div></div><div class="ContentItem-actions"><span><button aria-label="赞同 8" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 8</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>添加评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover60-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover60-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button></div></div></div></div><div class="Pc-word"><div class="Pc-word-card"><a target="_blank" href="https://www.togocareer.com/services.html?tgcChannel==zhihu&amp;tuwen0805"><div class="Pc-word-card-brand"><div class="Pc-word-card-brand-wrapper"><img width="20" height="20" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-b02efb32af2aada63f09e5d0a275df75_250x250.jpeg" alt="logo"><span>Togocareer</span></div></div></a><div class="Pc-word-card-sign"><div class="Pc-word-card-sign-label">广告​<svg class="Icon Icon--triangle Pc-word-card-sign-svg" viewBox="0 0 24 24"><path d="M9.218 16.78a.737.737 0 0 0 1.052 0l4.512-4.249a.758.758 0 0 0 0-1.063L10.27 7.22a.737.737 0 0 0-1.052 0 .759.759 0 0 0-.001 1.063L13 12l-3.782 3.716a.758.758 0 0 0 0 1.063z" fill-rule="evenodd"></path></svg></div><div class="Pc-word-card-sign-popup Pc-word-card-sign-popup--isHidden"><span class="Pc-word-card-sign-popup-arrow"></span><div class="Pc-word-card-sign-popup-menu"><button type="button">不感兴趣</button><a target="_blank" rel="noopener noreferrer" href="https://www.zhihu.com/promotion-intro">知乎广告介绍</a></div></div></div><a target="_blank" href="https://www.togocareer.com/services.html?tgcChannel==zhihu&amp;tuwen0805"><h2 class="Pc-word-card-title">对于留学生来说，回国求职，如何进入名企工作？实习机会怎么找？</h2><div class="Pc-word-card-content  "><span>留学生回国，在国内的发展情况如何？2019年校招、社招信息、500强名企内推名额预约，实习，求职更快捷！</span><span class="Pc-word-card-content-cta  ">查看详情</span></div></a></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="14" data-zop="{&quot;authorName&quot;:&quot;DeviliveD&quot;,&quot;itemId&quot;:125031098,&quot;title&quot;:&quot;Linear least squares, Lasso,ridge regression有何本质区别？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="125031098" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="14" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;125031098&quot;,&quot;upvote_num&quot;:4,&quot;comment_num&quot;:0,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;38121173&quot;,&quot;author_member_hash_id&quot;:&quot;2572dc249c942c1949fd4d0d737fe3f8&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="DeviliveD"><meta itemprop="image" content="https://pic2.zhimg.com/f8731c3b136f36e7068113ee67e2ba21_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/devilived"><meta itemprop="zhihu:followerCount" content="507"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover50-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover50-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/devilived"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/f8731c3b136f36e7068113ee67e2ba21_xs.jpg" srcset="https://pic2.zhimg.com/f8731c3b136f36e7068113ee67e2ba21_l.jpg 2x" alt="DeviliveD"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover51-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover51-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/devilived">DeviliveD</a></div></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="ztext AuthorInfo-badgeText">DON'T PANIC!</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button type="button" class="Button Button--plain">4 人赞同了该回答</button></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="4"><meta itemprop="url" content="https://www.zhihu.com/question/38121173/answer/125031098"><meta itemprop="dateCreated" content="2016-10-05T05:22:18.000Z"><meta itemprop="dateModified" content="2016-12-07T21:21:28.000Z"><meta itemprop="commentCount" content="0"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text">The difference essentially hides in Tuning parameter lambda for those three methods, which is all about answering the following question:  <br>"How much do you like to pay, for coefficients being non-zero?"</span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/38121173/answer/125031098"><span data-tooltip="发布于 2016-10-05 13:22">编辑于 2016-12-08</span></a></div></div><div class="ContentItem-actions"><span><button aria-label="赞同 4" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 4</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>添加评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover61-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover61-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button></div></div></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="15" data-zop="{&quot;authorName&quot;:&quot;知乎用户&quot;,&quot;itemId&quot;:75161277,&quot;title&quot;:&quot;Linear least squares, Lasso,ridge regression有何本质区别？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="75161277" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="15" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;75161277&quot;,&quot;upvote_num&quot;:15,&quot;comment_num&quot;:2,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;38121173&quot;,&quot;author_member_hash_id&quot;:&quot;ba438103b9a16d8c292086bbcb54eb7b&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="知乎用户"><meta itemprop="image" content="https://pic4.zhimg.com/da8e974dc_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/"><meta itemprop="zhihu:followerCount" content="58201"><span class="UserLink AuthorInfo-avatarWrapper"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/da8e974dc_xs.jpg" srcset="https://pic4.zhimg.com/da8e974dc_l.jpg 2x" alt="知乎用户"></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name">知乎用户</span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button type="button" class="Button Button--plain">15 人赞同了该回答</button></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="15"><meta itemprop="url" content="https://www.zhihu.com/question/38121173/answer/75161277"><meta itemprop="dateCreated" content="2015-12-04T00:45:26.000Z"><meta itemprop="dateModified" content="2015-12-04T03:42:31.000Z"><meta itemprop="commentCount" content="2"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text">这个问题我才疏学浅，只能从下面几个方面回答：<br>1，linear regression是个很大的题目，还有general linear regression。题主说的least square, 姑且认为是ordinary least square，那么其实很简单，就是证明了（在特定情况下）这个quadratic loss function估计出来的参数是blue （best linear unbiased estimator），指的是在所有linear，无偏 estimator当中，这个估计的结果是variance最小的。<br>2，加上regularizer的一个数学原因是1中参数估计的表达式存在inverse，如果存在multi-colinearity或者非唯一解，那么estimator 的数值解可能不存在／极不稳定。加入一个regularizer 可以保证inverse的非奇异性。<br>3，l1, l2都是可行的regularizer，唯一的区别可能就是l1得到的估计更加sparse。</span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/38121173/answer/75161277"><span data-tooltip="发布于 2015-12-04 08:45">编辑于 2015-12-04</span></a></div></div><div class="ContentItem-actions"><span><button aria-label="赞同 15" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 15</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>2 条评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover62-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover62-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button></div></div></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="16" data-zop="{&quot;authorName&quot;:&quot;罗德&quot;,&quot;itemId&quot;:575506086,&quot;title&quot;:&quot;Linear least squares, Lasso,ridge regression有何本质区别？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="575506086" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="16" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;575506086&quot;,&quot;upvote_num&quot;:4,&quot;comment_num&quot;:0,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;38121173&quot;,&quot;author_member_hash_id&quot;:&quot;f1870f95d0d41eaf62b5c1d6ee0f0b85&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="罗德"><meta itemprop="image" content="https://pic3.zhimg.com/v2-079215d30377aa8a0666042fbbf7b458_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/guo-jia-44-46"><meta itemprop="zhihu:followerCount" content="1643"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover54-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover54-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/guo-jia-44-46"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-079215d30377aa8a0666042fbbf7b458_xs.jpg" srcset="https://pic3.zhimg.com/v2-079215d30377aa8a0666042fbbf7b458_l.jpg 2x" alt="罗德"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover55-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover55-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/guo-jia-44-46">罗德</a></div></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button type="button" class="Button Button--plain">4 人赞同了该回答</button></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="4"><meta itemprop="url" content="https://www.zhihu.com/question/38121173/answer/575506086"><meta itemprop="dateCreated" content="2019-01-16T17:17:35.000Z"><meta itemprop="dateModified" content="2019-01-16T17:21:50.000Z"><meta itemprop="commentCount" content="0"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text"><p>大家都见过这两张图了</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/50/v2-f4abc22c5d66a67e52958da9a44b3525_hd.jpg" data-rawwidth="1021" data-rawheight="438" data-size="normal" data-caption="" data-default-watermark-src="https://pic1.zhimg.com/50/v2-4429edc27a3273d2fe39fed326cd48d5_hd.jpg" class="origin_image zh-lightbox-thumb" width="1021" data-original="https://pic1.zhimg.com/v2-f4abc22c5d66a67e52958da9a44b3525_r.jpg"/></noscript><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-f4abc22c5d66a67e52958da9a44b3525_hd.jpg" data-rawwidth="1021" data-rawheight="438" data-size="normal" data-caption="" data-default-watermark-src="https://pic1.zhimg.com/50/v2-4429edc27a3273d2fe39fed326cd48d5_hd.jpg" class="origin_image zh-lightbox-thumb lazy" width="1021" data-original="https://pic1.zhimg.com/v2-f4abc22c5d66a67e52958da9a44b3525_r.jpg" data-actualsrc="https://pic1.zhimg.com/50/v2-f4abc22c5d66a67e52958da9a44b3525_hd.jpg" data-lazy-status="ok"></figure><p>在我看到的这两张图的解释里，大多语焉不详。看起来好像是lasso会导致某个变量的解变为0，但为什么呢？我们假定优化的参数为w。</p><p>有一个解释是非常容易理解的。我们采用梯度下降的方法，lasso求导后，导数为1，不论w多么小都不影响这个结果，于是w越来越小，直到为0。由于我们定义0处的导数为0，于是这里就不动了。所谓的稀疏解就是这么回事。而ridge回归，即w的平方的导数是w乘一个常数。 可见，w越小，相应的导数也越小，w1，w2就倾向于一起变小，最终我们得到的w的模可以很小，但无法起到筛选变量的作用。</p><p>但以上是从微积分角度解释了这一现象，如何从图像理解这一现象呢？</p><p>图中，闭合的曲线代表等高线。想象我们身在山坡上，想要找到最低的点，我们自然会沿着梯度的方向走，这一方向在图中，就是所在等高线的垂线。</p><p>有了如上背景，lasso的性质就很好理解了。在ridge的情况下，圆的垂线指向圆心，所以w1，w2会一同变小，但是同时还有loss的影响，于是我们会在中间得到一个均衡的最优解。</p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/50/v2-a4ba1f5639a30963431c1e44e76f3d37_hd.jpg" data-rawwidth="1021" data-rawheight="438" data-size="normal" data-caption="" data-default-watermark-src="https://pic4.zhimg.com/50/v2-d6d7354b5970e44a83b3e89684f4b3d1_hd.jpg" class="origin_image zh-lightbox-thumb" width="1021" data-original="https://pic2.zhimg.com/v2-a4ba1f5639a30963431c1e44e76f3d37_r.jpg"/></noscript><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-a4ba1f5639a30963431c1e44e76f3d37_hd.jpg" data-rawwidth="1021" data-rawheight="438" data-size="normal" data-caption="" data-default-watermark-src="https://pic4.zhimg.com/50/v2-d6d7354b5970e44a83b3e89684f4b3d1_hd.jpg" class="origin_image zh-lightbox-thumb lazy" width="1021" data-original="https://pic2.zhimg.com/v2-a4ba1f5639a30963431c1e44e76f3d37_r.jpg" data-actualsrc="https://pic2.zhimg.com/50/v2-a4ba1f5639a30963431c1e44e76f3d37_hd.jpg" data-lazy-status="ok"></figure><p>而lasso呢？我们光看正则项的等高线。作一个垂线垂直于等高线，并一直延伸这条垂线，因为这条垂线就是梯度的方向，我们的解会沿着这个方向走下去。此时发生了什么呢？与坐标轴相交了！也就是说，在lasso的影响下，沿着梯度下降，我们的最优值求解会一头撞上坐标轴！其中一个变量的值就变成了0！而不是像ridge那样向原点前进。当然，与ridge类似，我们的最终解会在loss函数与lasso正则影响的一个均衡位置上。这个关系有点像两个恒星相距一段距离，行星在其中被万有引力拉扯，中间会有一个拉格朗日点。</p><p>希望能够给不太理解lasso回归的人一点参考～感谢阅读。</p></span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/38121173/answer/575506086"><span data-tooltip="发布于 2019-01-17 01:17">编辑于 2019-01-17</span></a></div></div><div class="ContentItem-actions RichContent-actions"><span><button aria-label="赞同 4" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 4</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>添加评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover85-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover85-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button><button data-zop-retract-question="true" type="button" class="Button ContentItem-action ContentItem-rightButton Button--plain"><span class="RichContent-collapsedText">收起</span><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--ArrowDown ContentItem-arrowIcon is-active" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M12 13L8.285 9.218a.758.758 0 0 0-1.064 0 .738.738 0 0 0 0 1.052l4.249 4.512a.758.758 0 0 0 1.064 0l4.246-4.512a.738.738 0 0 0 0-1.052.757.757 0 0 0-1.063 0L12.002 13z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="17" data-zop="{&quot;authorName&quot;:&quot;知乎用户&quot;,&quot;itemId&quot;:193794853,&quot;title&quot;:&quot;Linear least squares, Lasso,ridge regression有何本质区别？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="193794853" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="17" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;193794853&quot;,&quot;upvote_num&quot;:3,&quot;comment_num&quot;:0,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;38121173&quot;,&quot;author_member_hash_id&quot;:&quot;b5cdb995fb0703755eb2b7343ba81b3c&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="知乎用户"><meta itemprop="image" content="https://pic4.zhimg.com/da8e974dc_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/"><meta itemprop="zhihu:followerCount" content="4352"><span class="UserLink AuthorInfo-avatarWrapper"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/da8e974dc_xs.jpg" srcset="https://pic4.zhimg.com/da8e974dc_l.jpg 2x" alt="知乎用户"></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name">知乎用户</span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button type="button" class="Button Button--plain">3 人赞同了该回答</button></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="3"><meta itemprop="url" content="https://www.zhihu.com/question/38121173/answer/193794853"><meta itemprop="dateCreated" content="2017-07-05T03:18:29.000Z"><meta itemprop="dateModified" content="2017-07-05T03:18:29.000Z"><meta itemprop="commentCount" content="0"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text"><p>Lasso是一阶正则化，岭回归是二阶。lasso的出发点是减少overfit，而岭回归一般认为是处理多重共线性的一种做法，当然它也有降低overfit的作用。lasso实际上是挑选自变量的一种做法，岭回归是压缩某些系数。二者都会在训练集上放大误差，但是均能在测试集上减小估计误差（理论上）。</p></span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/38121173/answer/193794853"><span data-tooltip="发布于 2017-07-05 11:18">发布于 2017-07-05</span></a></div></div><div class="ContentItem-actions"><span><button aria-label="赞同 3" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 3</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>添加评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover83-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover83-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button></div></div></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="18" data-zop="{&quot;authorName&quot;:&quot;知乎用户&quot;,&quot;itemId&quot;:167342967,&quot;title&quot;:&quot;Linear least squares, Lasso,ridge regression有何本质区别？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="167342967" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="18" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;167342967&quot;,&quot;upvote_num&quot;:0,&quot;comment_num&quot;:0,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;38121173&quot;,&quot;author_member_hash_id&quot;:&quot;63d34c5510f805dfc28da7fa0fa8383e&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="知乎用户"><meta itemprop="image" content="https://pic4.zhimg.com/da8e974dc_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/"><meta itemprop="zhihu:followerCount" content="32"><span class="UserLink AuthorInfo-avatarWrapper"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/da8e974dc_xs.jpg" srcset="https://pic4.zhimg.com/da8e974dc_l.jpg 2x" alt="知乎用户"></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name">知乎用户</span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"></div></div></div></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="0"><meta itemprop="url" content="https://www.zhihu.com/question/38121173/answer/167342967"><meta itemprop="dateCreated" content="2017-05-10T03:49:27.000Z"><meta itemprop="dateModified" content="2017-05-10T04:19:07.000Z"><meta itemprop="commentCount" content="0"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text"><p>泻药。本质上并无区别。都是线性模型而已。</p></span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/38121173/answer/167342967"><span data-tooltip="发布于 2017-05-10 11:49">编辑于 2017-05-10</span></a></div></div><div class="ContentItem-actions"><span><button aria-label="赞同 0" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>添加评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover84-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover84-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button></div></div></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="19" data-zop="{&quot;authorName&quot;:&quot;张晋&quot;,&quot;itemId&quot;:310538819,&quot;title&quot;:&quot;Linear least squares, Lasso,ridge regression有何本质区别？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="310538819" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="19" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;310538819&quot;,&quot;upvote_num&quot;:4,&quot;comment_num&quot;:0,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;38121173&quot;,&quot;author_member_hash_id&quot;:&quot;835580d22343a4ff15541b8baea089fe&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="张晋"><meta itemprop="image" content="https://pic3.zhimg.com/v2-38e6633335e7551cc256486e10bc438b_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/zj_theigrams"><meta itemprop="zhihu:followerCount" content="1778"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover66-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover66-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/zj_theigrams"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-38e6633335e7551cc256486e10bc438b_xs.jpg" srcset="https://pic3.zhimg.com/v2-38e6633335e7551cc256486e10bc438b_l.jpg 2x" alt="张晋"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover67-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover67-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/zj_theigrams">张晋</a></div></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="ztext AuthorInfo-badgeText">叶公式数学爱好者</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button type="button" class="Button Button--plain">4 人赞同了该回答</button></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="4"><meta itemprop="url" content="https://www.zhihu.com/question/38121173/answer/310538819"><meta itemprop="dateCreated" content="2018-02-03T08:03:45.000Z"><meta itemprop="dateModified" content="2018-02-03T08:03:45.000Z"><meta itemprop="commentCount" content="0"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text"><ul><li>Lasso: <img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(68)" alt="[公式]" eeimg="1" data-formula="\mathop{\arg\min}_{W} \|y-Wx\|^2+\lambda\sum\|W_j\|_1"></li></ul><p>Lasso用到的是带L1约束的最小二乘法，期望得到的W是稀疏的，从贝叶斯观点来看，就是先给W假设了一个峰值很尖锐且接近0的分布，而L1惩罚项对应的是Laplace分布</p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/50/v2-e10dec07deb9e7b05cc2fa984abf7531_hd.jpg" data-caption="" data-size="normal" data-rawwidth="1300" data-rawheight="975" class="origin_image zh-lightbox-thumb" width="1300" data-original="https://pic2.zhimg.com/v2-e10dec07deb9e7b05cc2fa984abf7531_r.jpg"/></noscript><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-e10dec07deb9e7b05cc2fa984abf7531_hd.jpg" data-caption="" data-size="normal" data-rawwidth="1300" data-rawheight="975" class="origin_image zh-lightbox-thumb lazy" width="1300" data-original="https://pic2.zhimg.com/v2-e10dec07deb9e7b05cc2fa984abf7531_r.jpg" data-actualsrc="https://pic2.zhimg.com/50/v2-e10dec07deb9e7b05cc2fa984abf7531_hd.jpg" data-lazy-status="ok"></figure><p class="ztext-empty-paragraph"><br></p><ul><li>Ridge regression: <img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(69)" alt="[公式]" eeimg="1" data-formula="\mathop{\arg\min}_{W} \|y-Wx\|^2+\lambda\sum\|W_j\|_2^2"></li></ul><p>Lasso用到的是带L2约束的最小二乘法，只是期望W的均值能靠近0，所以先验分布的峰值也为0，但相比Laplace分布要更平缓一些，取的是高斯分布</p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/50/v2-b3fe0e21d3d2cee65c7af84174e98240_hd.jpg" data-caption="" data-size="normal" data-rawwidth="2000" data-rawheight="1278" class="origin_image zh-lightbox-thumb" width="2000" data-original="https://pic2.zhimg.com/v2-b3fe0e21d3d2cee65c7af84174e98240_r.jpg"/></noscript><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-b3fe0e21d3d2cee65c7af84174e98240_hd.jpg" data-caption="" data-size="normal" data-rawwidth="2000" data-rawheight="1278" class="origin_image zh-lightbox-thumb lazy" width="2000" data-original="https://pic2.zhimg.com/v2-b3fe0e21d3d2cee65c7af84174e98240_r.jpg" data-actualsrc="https://pic2.zhimg.com/50/v2-b3fe0e21d3d2cee65c7af84174e98240_hd.jpg" data-lazy-status="ok"></figure><ul><li>Linear least squares： <img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/equation(70)" alt="[公式]" eeimg="1" data-formula="\mathop{\arg\min}_{W} \|y-Wx\|^2"></li></ul><p>而Linear least squares用到的是最简单的最小二乘法，对W没有要求，因此也就没有取先验分布，仅仅在x,y的条件下取极大似然估计。</p><p></p></span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/38121173/answer/310538819"><span data-tooltip="发布于 2018-02-03 16:03">发布于 2018-02-03</span></a></div></div><div class="ContentItem-actions RichContent-actions"><span><button aria-label="赞同 4" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 4</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>添加评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover89-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover89-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button><button data-zop-retract-question="true" type="button" class="Button ContentItem-action ContentItem-rightButton Button--plain"><span class="RichContent-collapsedText">收起</span><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--ArrowDown ContentItem-arrowIcon is-active" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M12 13L8.285 9.218a.758.758 0 0 0-1.064 0 .738.738 0 0 0 0 1.052l4.249 4.512a.758.758 0 0 0 1.064 0l4.246-4.512a.738.738 0 0 0 0-1.052.757.757 0 0 0-1.063 0L12.002 13z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="20" data-zop="{&quot;authorName&quot;:&quot;狐狸卜&quot;,&quot;itemId&quot;:83575980,&quot;title&quot;:&quot;Linear least squares, Lasso,ridge regression有何本质区别？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="83575980" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="20" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;83575980&quot;,&quot;upvote_num&quot;:3,&quot;comment_num&quot;:2,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;38121173&quot;,&quot;author_member_hash_id&quot;:&quot;ff9577d05c893981df0d45dec5c241da&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="狐狸卜"><meta itemprop="image" content="https://pic1.zhimg.com/2d5b53af419a4b6377033e2d26a406e3_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/hu-li-bu"><meta itemprop="zhihu:followerCount" content="36"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover69-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover69-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/hu-li-bu"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/2d5b53af419a4b6377033e2d26a406e3_xs.jpg" srcset="https://pic1.zhimg.com/2d5b53af419a4b6377033e2d26a406e3_l.jpg 2x" alt="狐狸卜"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover70-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover70-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/hu-li-bu">狐狸卜</a></div></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="ztext AuthorInfo-badgeText">shallow statistic student </div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button type="button" class="Button Button--plain">3 人赞同了该回答</button></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="3"><meta itemprop="url" content="https://www.zhihu.com/question/38121173/answer/83575980"><meta itemprop="dateCreated" content="2016-01-27T06:05:30.000Z"><meta itemprop="dateModified" content="2016-01-28T06:24:29.000Z"><meta itemprop="commentCount" content="2"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text">从应用的角度讲两句。<br>linear regression应用在反应变量和观测量成线性关系加一个高斯误差的模型上。怎么估计参数？最基础常见的是least square，其结果的好处在于无偏。但是当观测量X接近singular的时候，least square估计量的方差大。<br>ridge是在最优化的函数上做l2的regularization，而lasso是l1的。ridge是修改优化函数后的升级版regression。而lasso最大的好处在于，curve常和数轴上的顶点相交，自动完成了feature selection，不用再看显著性的脸色。</span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/38121173/answer/83575980"><span data-tooltip="发布于 2016-01-27 14:05">编辑于 2016-01-28</span></a></div></div><div class="ContentItem-actions"><span><button aria-label="赞同 3" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 3</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>2 条评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover87-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover87-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button></div></div></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="21" data-zop="{&quot;authorName&quot;:&quot;Sunnyboy&quot;,&quot;itemId&quot;:404690248,&quot;title&quot;:&quot;Linear least squares, Lasso,ridge regression有何本质区别？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="404690248" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="21" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;404690248&quot;,&quot;upvote_num&quot;:1,&quot;comment_num&quot;:0,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;38121173&quot;,&quot;author_member_hash_id&quot;:&quot;f6cc5b87819349525e4546d997996621&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="Sunnyboy"><meta itemprop="image" content="https://pic3.zhimg.com/827811763275c5a8f1b1a46771c6f2b7_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/liao-luo-feng-83"><meta itemprop="zhihu:followerCount" content="7"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover72-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover72-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/liao-luo-feng-83"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/827811763275c5a8f1b1a46771c6f2b7_xs.jpg" srcset="https://pic3.zhimg.com/827811763275c5a8f1b1a46771c6f2b7_l.jpg 2x" alt="Sunnyboy"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover73-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover73-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/liao-luo-feng-83">Sunnyboy</a></div></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="ztext AuthorInfo-badgeText">Fdu</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button type="button" class="Button Button--plain">1 人赞同了该回答</button></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="1"><meta itemprop="url" content="https://www.zhihu.com/question/38121173/answer/404690248"><meta itemprop="dateCreated" content="2018-05-29T15:27:37.000Z"><meta itemprop="dateModified" content="2018-05-29T15:27:37.000Z"><meta itemprop="commentCount" content="0"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text"><p>可以参考statistical learning with sparsity.</p><br><p>关于lasso,之所以选q=1,是因为如果大于了就不能产生稀疏的解，小于了就是凸问题难以解决。q=1恰好是满足这两个性质的临界点。</p>至于这样得出的估计量虽然是稀疏的但是是有偏的。针对这个问题有scad和mcp两种稀疏性惩罚，并且有有较好的理论特点(oracle property )</span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/38121173/answer/404690248"><span data-tooltip="发布于 2018-05-29 23:27">发布于 2018-05-29</span></a></div></div><div class="ContentItem-actions"><span><button aria-label="赞同 1" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 1</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>添加评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover88-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover88-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button></div></div></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="22" data-zop="{&quot;authorName&quot;:&quot;萧萧白马&quot;,&quot;itemId&quot;:404616372,&quot;title&quot;:&quot;Linear least squares, Lasso,ridge regression有何本质区别？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="404616372" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="22" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;404616372&quot;,&quot;upvote_num&quot;:1,&quot;comment_num&quot;:0,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;38121173&quot;,&quot;author_member_hash_id&quot;:&quot;74a92eeef315a7dfa52d3ec871470808&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="萧萧白马"><meta itemprop="image" content="https://pic1.zhimg.com/v2-2c08fd5130dffe2b4876fc48ef631b6c_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/ma-ming-79-4"><meta itemprop="zhihu:followerCount" content="513"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover75-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover75-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/ma-ming-79-4"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-2c08fd5130dffe2b4876fc48ef631b6c_xs.jpg" srcset="https://pic1.zhimg.com/v2-2c08fd5130dffe2b4876fc48ef631b6c_l.jpg 2x" alt="萧萧白马"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover76-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover76-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/ma-ming-79-4">萧萧白马</a></div></div><a class="UserLink-badge" data-tooltip="已认证的个人" href="https://www.zhihu.com/question/48510028" target="_blank" rel="noopener noreferrer"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--BadgeCert" fill="currentColor" viewBox="0 0 24 24" width="18" height="18"><g fill="none" fill-rule="evenodd"><path fill="#0F88EB" d="M2.64 13.39c1.068.895 1.808 2.733 1.66 4.113l.022-.196c-.147 1.384.856 2.4 2.24 2.278l-.198.016c1.387-.122 3.21.655 4.083 1.734l-.125-.154c.876 1.084 2.304 1.092 3.195.027l-.127.152c.895-1.068 2.733-1.808 4.113-1.66l-.198-.022c1.386.147 2.402-.856 2.279-2.238l.017.197c-.122-1.388.655-3.212 1.734-4.084l-.154.125c1.083-.876 1.092-2.304.027-3.195l.152.127c-1.068-.895-1.808-2.732-1.66-4.113l-.022.198c.147-1.386-.856-2.4-2.24-2.279l.198-.017c-1.387.123-3.21-.654-4.083-1.733l.125.153c-.876-1.083-2.304-1.092-3.195-.027l.127-.152c-.895 1.068-2.733 1.808-4.113 1.662l.198.02c-1.386-.147-2.4.857-2.279 2.24L4.4 6.363c.122 1.387-.655 3.21-1.734 4.084l.154-.126c-1.083.878-1.092 2.304-.027 3.195l-.152-.127z"></path><path fill="#FFF" d="M9.78 15.728l-2.633-2.999s-.458-.705.242-1.362c.7-.657 1.328-.219 1.328-.219l1.953 2.132 4.696-4.931s.663-.348 1.299.198c.636.545.27 1.382.27 1.382s-3.466 3.858-5.376 5.782c-.98.93-1.778.017-1.778.017z"></path></g></svg></span></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="AuthorInfo-badgeText">清华大学 数学博士</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button type="button" class="Button Button--plain">1 人赞同了该回答</button></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="1"><meta itemprop="url" content="https://www.zhihu.com/question/38121173/answer/404616372"><meta itemprop="dateCreated" content="2018-05-29T13:39:53.000Z"><meta itemprop="dateModified" content="2018-05-30T07:25:27.000Z"><meta itemprop="commentCount" content="0"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text"><p>看到很多的回答都从优化、机器学习算法的角度考虑。 </p><p>我可以从经济学的角度讲讲。</p><p>大概涉及“边际效用”，明日更新。</p><p class="ztext-empty-paragraph"><br></p><p>对于ridga， 损失函数为：</p><p>sum (y - bx)^2 + gamma b^2,</p><p>根据KKT条件，b需要满足一阶导数为0，即：</p><p>sum 2 (y - bx) x = 2 gamma b,</p><p>其中，等式左边为b对loss的边际效用， 右边是b关于惩罚项的边际效用。</p><p>在ridge回归中，我们要求，如果参数b越大，它对loss的边际贡献应该更大（2 gamma b）。同样，如果参数b很小，我们只要求它对loss有较小的边际贡献。 因此，通常在ridge中，b不会被优化为0，因为对于较小的b，我们只期望它有较小的边际贡献。</p><p class="ztext-empty-paragraph"><br></p><p>对于lasso, 损失函数为：</p><p>sum (y - bx)^2 + gamma |b|,</p><p>对应的一阶条件是：</p><p>sum 2 (y - bx) x = gamma sign(b).</p><p>这种情况下，我们期待b的边际贡献的大小不因b的取值大小而改变。 所以，lasso会把ridge中较小的参数变为0，但也会比ridge容忍更大的参数。</p></span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/38121173/answer/404616372"><span data-tooltip="发布于 2018-05-29 21:39">编辑于 2018-05-30</span></a></div></div><div class="ContentItem-actions"><span><button aria-label="赞同 1" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 1</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>添加评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover90-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover90-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button></div></div></div></div><div class="Pc-word"><div class="Pc-word-card"><a target="_blank" href="http://www.togocareer.com/haiguiqiuzhi.html?sourcezhihu&amp;id=PC-1120"><div class="Pc-word-card-brand"><div class="Pc-word-card-brand-wrapper"><img width="20" height="20" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-b02efb32af2aada63f09e5d0a275df75_250x250.jpeg" alt="logo"><span>Togocareer</span></div></div></a><div class="Pc-word-card-sign"><div class="Pc-word-card-sign-label">广告​<svg class="Icon Icon--triangle Pc-word-card-sign-svg" viewBox="0 0 24 24"><path d="M9.218 16.78a.737.737 0 0 0 1.052 0l4.512-4.249a.758.758 0 0 0 0-1.063L10.27 7.22a.737.737 0 0 0-1.052 0 .759.759 0 0 0-.001 1.063L13 12l-3.782 3.716a.758.758 0 0 0 0 1.063z" fill-rule="evenodd"></path></svg></div><div class="Pc-word-card-sign-popup Pc-word-card-sign-popup--isHidden"><span class="Pc-word-card-sign-popup-arrow"></span><div class="Pc-word-card-sign-popup-menu"><button type="button">不感兴趣</button><a target="_blank" rel="noopener noreferrer" href="https://www.zhihu.com/promotion-intro">知乎广告介绍</a></div></div></div><a target="_blank" href="http://www.togocareer.com/haiguiqiuzhi.html?sourcezhihu&amp;id=PC-1120"><h2 class="Pc-word-card-title">作为应届留学生，回国后通过哪些途径找工作的？各行业招聘情况如何</h2><div class="Pc-word-card-content  "><span>近期热门招聘公司：金融，咨询，五百强外资，互联网等各大行业，从零到卓越，全流程专业化求职，一站式服务留学生。</span><span class="Pc-word-card-content-cta  ">查看详情</span></div></a></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="24" data-zop="{&quot;authorName&quot;:&quot;知乎用户&quot;,&quot;itemId&quot;:344219082,&quot;title&quot;:&quot;Linear least squares, Lasso,ridge regression有何本质区别？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="344219082" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="24" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;344219082&quot;,&quot;upvote_num&quot;:0,&quot;comment_num&quot;:0,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;38121173&quot;,&quot;author_member_hash_id&quot;:&quot;98a9114d6d9b01f232fb46169e868e8b&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="知乎用户"><meta itemprop="image" content="https://pic4.zhimg.com/da8e974dc_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/"><meta itemprop="zhihu:followerCount" content="99"><span class="UserLink AuthorInfo-avatarWrapper"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/da8e974dc_xs.jpg" srcset="https://pic4.zhimg.com/da8e974dc_l.jpg 2x" alt="知乎用户"></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name">知乎用户</span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"></div></div></div></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="0"><meta itemprop="url" content="https://www.zhihu.com/question/38121173/answer/344219082"><meta itemprop="dateCreated" content="2018-03-17T19:36:45.000Z"><meta itemprop="dateModified" content="2018-03-25T11:24:51.000Z"><meta itemprop="commentCount" content="0"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text">与诸位理解略有所不同，个人认为l1 和 l2的一个区别是他们对权重的整体效果不同。l1和l2都有一个效果，就是收缩权重，但是如何收缩却很有区别。l1在收缩的时候权重会集中，l2却是分散。比如原始权重为[0,0.5,0,0.5]，这里先不考虑收缩，l1会变为[0,0,0,1]，而l2为[0.25,0.25,0.25,0.25]，这也和他们本身的形式是对应的，l1会出现稀疏解，l2会让平方和变小。而l1往往是为了稀疏而提取特征。而elastic则是为了让l1也具有更好的过拟合性能而存在的。</span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/38121173/answer/344219082"><span data-tooltip="发布于 2018-03-18 03:36">编辑于 2018-03-25</span></a></div></div><div class="ContentItem-actions"><span><button aria-label="赞同 0" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>添加评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover91-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover91-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button></div></div></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="25" data-zop="{&quot;authorName&quot;:&quot;mathphobia&quot;,&quot;itemId&quot;:166875285,&quot;title&quot;:&quot;Linear least squares, Lasso,ridge regression有何本质区别？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="166875285" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="25" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;166875285&quot;,&quot;upvote_num&quot;:0,&quot;comment_num&quot;:0,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;38121173&quot;,&quot;author_member_hash_id&quot;:&quot;aa788e49eb00c40944d834106880ffac&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="mathphobia"><meta itemprop="image" content="https://pic3.zhimg.com/v2-e1a59729142d03e9d83146557bf288eb_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/bling-joe"><meta itemprop="zhihu:followerCount" content="6"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover79-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover79-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/bling-joe"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-e1a59729142d03e9d83146557bf288eb_xs.jpg" srcset="https://pic3.zhimg.com/v2-e1a59729142d03e9d83146557bf288eb_l.jpg 2x" alt="mathphobia"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover80-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover80-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/bling-joe">mathphobia</a></div></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="ztext AuthorInfo-badgeText">业余书法爱好者和业余的程序员</div></div></div></div></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="0"><meta itemprop="url" content="https://www.zhihu.com/question/38121173/answer/166875285"><meta itemprop="dateCreated" content="2017-05-09T04:29:52.000Z"><meta itemprop="dateModified" content="2017-05-09T04:29:53.000Z"><meta itemprop="commentCount" content="0"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text">本质就是有无罚项和什么形式的罚。<br>其实主要看l1罚的作用即可，对于无罚的约束问题，假设其存在一个靠近0的极值点x0，显然x0使其目标函数的导数接近于0。<br>加了l1罚之后，若只考虑l1罚的极值点，显然是0，弱导数也是其系数，起主导作用。<br>而l2的导数在x0附近依然很小，基本不影响原约束问题导数的值。</span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/38121173/answer/166875285"><span data-tooltip="发布于 2017-05-09 12:29">编辑于 2017-05-09</span></a></div></div><div class="ContentItem-actions"><span><button aria-label="赞同 0" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>添加评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover92-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover92-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button></div></div></div></div><div></div></div></div></div></div></div></div><div class="Card" data-za-detail-view-path-module="BottomBar" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null}}"><button type="button" class="Button QuestionAnswers-answerButton Button--blue Button--spread"><svg viewBox="0 0 12 12" class="Icon QuestionButton-icon Button-icon Icon--modify" width="14" height="16" aria-hidden="true" style="height: 16px; width: 14px;"><title></title><g><path d="M.423 10.32L0 12l1.667-.474 1.55-.44-2.4-2.33-.394 1.564zM10.153.233c-.327-.318-.85-.31-1.17.018l-.793.817 2.49 2.414.792-.814c.318-.328.312-.852-.017-1.17l-1.3-1.263zM3.84 10.536L1.35 8.122l6.265-6.46 2.49 2.414-6.265 6.46z" fill-rule="evenodd"></path></g></svg>写回答</button></div><div class="CollapsedAnswers-bar"><button type="button" class="Button Button--plain">1 个回答被折叠</button>（<a target="_blank" type="button" class="Button Button--plain" href="https://www.zhihu.com/question/20120168">为什么？</a>）</div></div></div><div class="Question-sideColumn Question-sideColumn--sticky" data-za-detail-view-path-module="RightSideBar" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null}}"><div><div class="Sticky is-fixed" style="width: 296px; top: -437px; left: 1155.5px;"><div class="Card AppBanner"><a class="AppBanner-link" href="http://zhi.hu/BDXoI"><div class="AppBanner-layout"><img class="AppBanner-qrcode" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/sidebar-download-qrcode.7caef4dd.png" alt="QR Code of Downloading Zhihu App"><div class="AppBanner-content"><div class="AppBanner-title">下载知乎客户端</div><div class="AppBanner-description">与世界分享知识、经验和见解</div></div></div></a></div><div class="Card"></div><div class="Card" data-za-detail-view-path-module="RelatedQuestions" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null}}"><div class="Card-header SimilarQuestions-title"><div class="Card-headerText">相关问题</div></div><div class="Card-section SimilarQuestions-list"><div class="SimilarQuestions-item" itemprop="zhihu:similarQuestion" itemtype="http://schema.org/Question" itemscope="" data-za-detail-view-path-module="QuestionItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;26768865&quot;}}}"><meta itemprop="name" content="Linear SVM 和 LR 有什么异同？"><meta itemprop="url" content="https://www.zhihu.com/question/26768865"><meta itemprop="answerCount" content="15"><meta itemprop="zhihu:followerCount" content="1258"><a target="_blank" type="button" class="Button Button--plain" href="https://www.zhihu.com/question/26768865">Linear SVM 和 LR 有什么异同？</a> 15 个回答</div><div class="SimilarQuestions-item" itemprop="zhihu:similarQuestion" itemtype="http://schema.org/Question" itemscope="" data-za-detail-view-path-module="QuestionItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;36915970&quot;}}}"><meta itemprop="name" content="Linear classifier 里的 bias 有什么用？"><meta itemprop="url" content="https://www.zhihu.com/question/36915970"><meta itemprop="answerCount" content="6"><meta itemprop="zhihu:followerCount" content="42"><a target="_blank" type="button" class="Button Button--plain" href="https://www.zhihu.com/question/36915970">Linear classifier 里的 bias 有什么用？</a> 6 个回答</div><div class="SimilarQuestions-item" itemprop="zhihu:similarQuestion" itemtype="http://schema.org/Question" itemscope="" data-za-detail-view-path-module="QuestionItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;26632694&quot;}}}"><meta itemprop="name" content="svm(支持向量机)的训练时用到了一些核函数(kernel function)，怎么样正确理解它们的作用?"><meta itemprop="url" content="https://www.zhihu.com/question/26632694"><meta itemprop="answerCount" content="11"><meta itemprop="zhihu:followerCount" content="183"><a target="_blank" type="button" class="Button Button--plain" href="https://www.zhihu.com/question/26632694">svm(支持向量机)的训练时用到了一些核函数(kernel function)，怎么样正确理解它们的作用?</a> 11 个回答</div><div class="SimilarQuestions-item" itemprop="zhihu:similarQuestion" itemtype="http://schema.org/Question" itemscope="" data-za-detail-view-path-module="QuestionItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;46784781&quot;}}}"><meta itemprop="name" content="基于树的adaboost和Gradient Tree Boosting区别？"><meta itemprop="url" content="https://www.zhihu.com/question/46784781"><meta itemprop="answerCount" content="7"><meta itemprop="zhihu:followerCount" content="144"><a target="_blank" type="button" class="Button Button--plain" href="https://www.zhihu.com/question/46784781">基于树的adaboost和Gradient Tree Boosting区别？</a> 7 个回答</div><div class="SimilarQuestions-item" itemprop="zhihu:similarQuestion" itemtype="http://schema.org/Question" itemscope="" data-za-detail-view-path-module="QuestionItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;27346308&quot;}}}"><meta itemprop="name" content="Linear Programming、Nonlinear Programming、convex optimization、Numerical Optimization之间的关系？"><meta itemprop="url" content="https://www.zhihu.com/question/27346308"><meta itemprop="answerCount" content="5"><meta itemprop="zhihu:followerCount" content="153"><a target="_blank" type="button" class="Button Button--plain" href="https://www.zhihu.com/question/27346308">Linear Programming、Nonlinear Programming、convex optimization、Numerical Optimization之间的关系？</a> 5 个回答</div></div></div><div class="Card" data-za-detail-view-path-module="ContentList" data-za-detail-view-path-module_name="相关推荐" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null}}"><div class="Card-header RelatedCommodities-title"><div class="Card-headerText">相关推荐</div></div><div class="Card-section RelatedCommodities-list"><a target="_blank" href="https://www.zhihu.com/lives/787681685134114816" type="button" class="Button RelatedCommodities-item Button--plain" data-za-detail-view-path-module="LiveItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Live&quot;,&quot;id&quot;:&quot;787681685134114816&quot;,&quot;author_member_hash_id&quot;:&quot;c9b28ce4b50bf0444d17d010224cb06f&quot;}}}"><img class="RelatedCommodities-image" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/bc9809df2fc0e97fd3748dddcf17dd05_250x0.jpg" alt="live"><div class="RelatedCommodities-content"><div class="RelatedCommodities-subject RelatedCommodities-subject-two" data-tooltip="从零学会数据分析: R语言的数据结构">从零学会数据分析: R语言的数据结构</div><div class="RelatedCommodities-meta"><div class="RelatedCommodities-scoreWrapper"><div class="Rating"><svg viewBox="0 0 15 15" class="Icon Icon--rating" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#ffab2e" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z">   </path></g></g></svg><svg viewBox="0 0 15 15" class="Icon Icon--rating" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#ffab2e" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z">   </path></g></g></svg><svg viewBox="0 0 15 15" class="Icon Icon--rating" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#ffab2e" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z">   </path></g></g></svg><svg viewBox="0 0 15 15" class="Icon Icon--rating" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#ffab2e" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z">   </path></g></g></svg><svg width="15" height="15" viewBox="0 0 15 15" class="Icon Icon--ratingHalf" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#d7d8d9" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z"></path>   </g></g></svg></div>3010 人参与</div></div></div></a><a target="_blank" href="https://www.zhihu.com/lives/909806712939249664" type="button" class="Button RelatedCommodities-item Button--plain" data-za-detail-view-path-module="LiveItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Live&quot;,&quot;id&quot;:&quot;909806712939249664&quot;,&quot;author_member_hash_id&quot;:&quot;7dce82d1f49ea92b780a8b668b03cbfc&quot;}}}"><img class="RelatedCommodities-image" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-5c463b5bef5cd0f2e1233604414e98a5_250x0.jpg" alt="live"><div class="RelatedCommodities-content"><div class="RelatedCommodities-subject RelatedCommodities-subject-two" data-tooltip="推荐算法(终):常用技能及日常工作">推荐算法(终):常用技能及日常工作</div><div class="RelatedCommodities-meta"><div class="RelatedCommodities-scoreWrapper"><div class="Rating"><svg viewBox="0 0 15 15" class="Icon Icon--rating" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#ffab2e" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z">   </path></g></g></svg><svg viewBox="0 0 15 15" class="Icon Icon--rating" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#ffab2e" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z">   </path></g></g></svg><svg viewBox="0 0 15 15" class="Icon Icon--rating" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#ffab2e" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z">   </path></g></g></svg><svg viewBox="0 0 15 15" class="Icon Icon--rating" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#ffab2e" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z">   </path></g></g></svg><svg viewBox="0 0 15 15" class="Icon Icon--rating" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#ffab2e" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z">   </path></g></g></svg></div>269 人参与</div></div></div></a><a target="_blank" href="https://www.zhihu.com/pub/book/119561064" type="button" class="Button RelatedCommodities-item Button--plain" data-za-detail-view-path-module="EBookItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;EBook&quot;,&quot;token&quot;:&quot;119561064&quot;}}}"><img class="RelatedCommodities-image" src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-d1ed3b8e7e4c253c47b52c395b20c3a8_250x0.jpg" alt="live"><div class="RelatedCommodities-content"><div class="RelatedCommodities-subject RelatedCommodities-subject-two" data-tooltip="scikit-learn 机器学习：常用算法原理及编程实战">scikit-learn 机器学习：常用算法原理及编程实战</div><div class="RelatedCommodities-meta"><div class="RelatedCommodities-bookMeta">1,244 人读过<span class="RelatedCommodities-bookRead"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Ebook" width="13" height="14" fill="currentColor" viewBox="0 0 24 24"><path d="M16 17.649V2.931a.921.921 0 0 0-.045-.283.943.943 0 0 0-1.182-.604L4.655 5.235A.932.932 0 0 0 4 6.122v14.947c0 .514.421.931.941.931H19.06c.52 0 .941-.417.941-.93V7.292a.936.936 0 0 0-.941-.931h-.773v12.834a.934.934 0 0 1-.83.924L6.464 21.416c-.02.002 2.94-.958 8.883-2.881a.932.932 0 0 0 .653-.886z" fill-rule="evenodd"></path></svg></span>阅读</span></div></div></div></a></div></div><div class="Card"></div><footer class="Footer"><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://liukanshan.zhihu.com/">刘看山</a><span class="Footer-dot"></span><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://www.zhihu.com/question/19581624">知乎指南</a><span class="Footer-dot"></span><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://www.zhihu.com/terms">知乎协议</a><span class="Footer-dot"></span><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://www.zhihu.com/term/privacy">知乎隐私保护指引</a><br><a class="Footer-item" target="_blank" href="https://www.zhihu.com/app">应用</a><span class="Footer-dot"></span><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://app.mokahr.com/apply/zhihu">工作</a><span class="Footer-dot"></span><button type="button" class="Button OrgCreateButton">申请开通知乎机构号</button><br><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://zhuanlan.zhihu.com/p/51068775">侵权举报</a><span class="Footer-dot"></span><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="http://www.12377.cn/">网上有害信息举报专区</a><br><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://tsm.miit.gov.cn/dxxzsp/">京 ICP 证 110745 号</a><br><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=11010802020088"><img src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/v2-d0289dc0a46fc5b15b3363ffa78cf6c7.png">京公网安备 11010802010035 号</a><br><span class="Footer-item">违法和不良信息举报：010-82716601</span><br><a class="Footer-item" target="_blank" href="https://www.zhihu.com/jubao">儿童色情信息举报专区</a><br><a class="Footer-item" target="_blank" href="https://www.zhihu.com/certificates">证照中心</a><br><a class="Footer-item" target="_blank" href="https://www.zhihu.com/contact">联系我们</a><span> © 2019 知乎</span></footer></div><div class="Sticky--holder" style="position: static; top: auto; right: auto; bottom: auto; left: auto; display: block; float: none; margin: 0px; height: 1078px;"></div></div></div></div></div></main><div data-zop-usertoken="{}"></div><div class="CornerButtons"><div class="CornerAnimayedFlex"><button data-tooltip="建议反馈" data-tooltip-position="left" data-tooltip-will-hide-on-click="true" aria-label="建议反馈" type="button" class="Button CornerButton Button--plain"><svg class="Zi Zi--Feedback" title="建议反馈" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M19.99 6.99L18 5s-1-1-2-1H8C7 4 6 5 6 5L4 7S3 8 3 9v9s0 2 2.002 2H19c2 0 2-2 2-2V9c0-1-1.01-2.01-1.01-2.01zM16.5 5.5L19 8H5l2.5-2.5h9zm-2 5.5s.5 0 .5.5-.5.5-.5.5h-5s-.5 0-.5-.5.5-.5.5-.5h5z"></path></svg></button></div><div class="CornerAnimayedFlex"><button data-tooltip="回到顶部" data-tooltip-position="left" data-tooltip-will-hide-on-click="true" aria-label="回到顶部" type="button" class="Button CornerButton Button--plain"><svg class="Zi Zi--BackToTop" title="回到顶部" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M16.036 19.59a1 1 0 0 1-.997.995H9.032a.996.996 0 0 1-.997-.996v-7.005H5.03c-1.1 0-1.36-.633-.578-1.416L11.33 4.29a1.003 1.003 0 0 1 1.412 0l6.878 6.88c.782.78.523 1.415-.58 1.415h-3.004v7.005z"></path></svg></button></div></div></div></div><script id="js-clientConfig" type="text/json">{"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","zhuanlanHost":"zhuanlan.zhihu.com","apiHost":"api.zhihu.com"}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"question\u002Fget\u002F":false,"question\u002FgetAnswers\u002F38121173":false}},"entities":{"users":{},"questions":{"38121173":{"type":"question","id":38121173,"title":"Linear least squares, Lasso,ridge regression有何本质区别？","questionType":"normal","created":1449061786,"updatedTime":1449061786,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F38121173","isMuted":false,"isNormal":true,"isEditable":false,"adminClosedComment":false,"hasPublishingDraft":false,"answerCount":23,"visitCount":101596,"commentCount":0,"followerCount":2004,"collapsedAnswerCount":1,"excerpt":"Linear least squares, Lasso,ridge regression有何本质区别？ 还有ridge regression uses L2 regularization; and Lasso uses L1 regularization. L1和L2一般如何选取？","commentPermission":"all","detail":"Linear least squares, Lasso,ridge regression有何本质区别？\u003Cbr\u002F\u003E还有\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FRidge_regression\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003Eridge regression\u003C\u002Fa\u003E uses L2 regularization; and \u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FLasso_%28statistics%29\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003ELasso\u003C\u002Fa\u003E uses L1 regularization. \u003Cbr\u002F\u003EL1和L2一般如何选取？","editableDetail":"Linear least squares, Lasso,ridge regression有何本质区别？\u003Cbr\u003E还有\u003Ca href=\"http:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FRidge_regression\"\u003E\u003Cem\u003Eridge regression\u003C\u002Fem\u003E\u003C\u002Fa\u003E uses L2 regularization; and \u003Ca href=\"http:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FLasso_(statistics)\"\u003E\u003Cem\u003ELasso\u003C\u002Fem\u003E\u003C\u002Fa\u003E uses L1 regularization. \u003Cbr\u003EL1和L2一般如何选取？","status":{"isLocked":false,"isClose":false,"isEvaluate":false,"isSuggest":false},"relationship":{"isAuthor":false,"isFollowing":false,"isAnonymous":false,"canLock":false,"canStickAnswers":false,"canCollapseAnswers":false},"topics":[{"id":"19553534","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19553534","name":"数据挖掘","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002F3d11abd48_is.jpg"},{"id":"19559450","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19559450","name":"机器学习","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fd3dd87a0feae0a3db82973157eee89c0_is.jpg"},{"id":"19591797","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19591797","name":"数据结构","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fc94ca74bf_is.jpg"}],"author":{"id":"2b114675d9026980843027dcab56949b","urlToken":"","name":"知乎用户","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fda8e974dc_is.jpg","avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fda8e974dc_{size}.jpg","isOrg":false,"type":"people","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fpeople\u002F0","userType":"people","headline":"","badge":[],"gender":1,"isAdvertiser":false,"isPrivacy":true},"canComment":{"status":true,"reason":""},"reviewInfo":{"type":"","tips":"","editTips":"","isReviewing":false},"relatedCards":[],"muteInfo":{"type":""},"showAuthor":false}},"answers":{"75158776":{"id":75158776,"type":"answer","answerType":"normal","question":{"type":"question","id":38121173,"title":"Linear least squares, Lasso,ridge regression有何本质区别？","questionType":"normal","created":1449061786,"updatedTime":1449061786,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F38121173","relationship":{}},"author":{"id":"17b5e9a781f9157bc7ee12bca15d6f36","urlToken":"","name":"知乎用户","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fda8e974dc_is.jpg","avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fda8e974dc_{size}.jpg","isOrg":false,"type":"people","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fpeople\u002F0","userType":"people","headline":"","badge":[],"gender":1,"isAdvertiser":false,"followerCount":313,"isFollowed":false,"isPrivacy":true},"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fanswers\u002F75158776","isCollapsed":false,"createdTime":1449188229,"updatedTime":1449188229,"extras":"","isCopyable":true,"isNormal":true,"voteupCount":564,"commentCount":21,"isSticky":false,"adminClosedComment":false,"commentPermission":"all","canComment":{"reason":"","status":true},"reshipmentSettings":"allowed","content":"Linear regression一般只对low dimension适用，比如n=50, p=5，而且这五个变量还不存在multicolinearity.\u003Cbr\u002F\u003E\u003Cbr\u002F\u003ERidge Regression的提出就是为了解决multicolinearity的，加一个L2 penalty term也是因为算起来方便。然而它并不能shrink parameters to 0.所以没法做variable selection。\u003Cbr\u002F\u003E\u003Cbr\u002F\u003ELASSO是针对Ridge Regression的没法做variable selection的问题提出来的，L1 penalty虽然算起来麻烦，没有解析解，但是可以把某些系数shrink到0啊。\u003Cbr\u002F\u003E\u003Cbr\u002F\u003E然而LASSO虽然可以做variable selection，但是不consistent啊，而且当n很小时至多只能选出n个变量；而且不能做group selection。\u003Cbr\u002F\u003E\u003Cbr\u002F\u003E于是有了在L1和L2 penalty之间做个权重就是elastic net, 针对不consistent有了adaptive lasso，针对不能做group selection有了group lasso, 在graphical models里有了graphical lasso。然后有人说unbiasedness, sparsity and continuity这三条都满足多好，于是有了MCP和SCAD同时满足这三条性质。penalized regression太多了，上面提到的都是比较popular的方法了。","editableContent":"","excerpt":"Linear regression一般只对low dimension适用，比如n=50, p=5，而且这五个变量还不存在multicolinearity. Ridge Regression的提出就是为了解决multicolinearity的，加一个L2 penalty term也是因为算起来方便。然而它并不能shrink parameters to 0.所以没法做variable selection。 LASSO是针对Ridge Regression的没法做variable selection的问题提出来的，L1 penalty虽然算起来麻烦，没有解析解，但是可以把某些系数shrink到0啊。…","collapsedBy":"nobody","collapseReason":"","annotationAction":null,"markInfos":[],"relevantInfo":{"isRelevant":false,"relevantType":"","relevantText":""},"suggestEdit":{"reason":"","status":false,"tip":"","title":"","unnormalDetails":{"status":"","description":"","reason":"","reasonId":0,"note":""},"url":""},"isLabeled":false,"rewardInfo":{"canOpenReward":false,"isRewardable":false,"rewardMemberCount":0,"rewardTotalMoney":0,"tagline":""},"relationship":{"isAuthor":false,"isAuthorized":false,"isNothelp":false,"isThanked":false,"isRecognized":false,"voting":0,"upvotedFollowees":[]}},"85813729":{"id":85813729,"type":"answer","answerType":"normal","question":{"type":"question","id":38121173,"title":"Linear least squares, Lasso,ridge regression有何本质区别？","questionType":"normal","created":1449061786,"updatedTime":1449061786,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F38121173","relationship":{}},"author":{"id":"2e4dfd1e401bed61d2d21fa6cb4f8600","urlToken":"yang-jun-14","name":"杨军","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-0a767e48ee617044b3898761e96da2d9_is.jpg","avatarUrlTemplate":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-0a767e48ee617044b3898761e96da2d9_{size}.jpg","isOrg":false,"type":"people","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fpeople\u002F2e4dfd1e401bed61d2d21fa6cb4f8600","userType":"people","headline":"从事大规模机器学习系统研发及应用相关工作","badge":[{"type":"best_answerer","description":"优秀回答者","topics":[{"id":"19813032","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19813032","name":"深度学习（Deep Learning）","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002F5d3c206139ca2124997418db09b0bb11_is.jpg"},{"id":"19559450","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19559450","name":"机器学习","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fd3dd87a0feae0a3db82973157eee89c0_is.jpg"}]}],"gender":1,"isAdvertiser":false,"followerCount":11605,"isFollowed":false,"isPrivacy":false},"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fanswers\u002F85813729","isCollapsed":false,"createdTime":1455198700,"updatedTime":1455201613,"extras":"","isCopyable":true,"isNormal":true,"voteupCount":123,"commentCount":12,"isSticky":false,"adminClosedComment":false,"commentPermission":"all","canComment":{"reason":"","status":true},"reshipmentSettings":"allowed","content":"线性回归问题是很经典的机器学习问题了。\n\u003Cbr\u002F\u003E\u003Cbr\u002F\u003E适用的方法也蛮多，有标准的Ordinary Least Squares，还有带了L2正则的Ridge Regression以及L1正则的Lasso Regression。\n\u003Cbr\u002F\u003E\u003Cbr\u002F\u003E这些不同的回归模型的差异和设计动机是什么？\n\u003Cbr\u002F\u003E\u003Cbr\u002F\u003E在本帖的一个高票回答[1]里，把这个问题讨论得其实已经相当清楚了。\n\u003Cbr\u002F\u003E\u003Cbr\u002F\u003E我在这里的回答更多是一个知识性的总结，在Scott Young的《如何高效学习》[6]里提到高效学习的几个环节： 获取、理解、拓展、纠错、应用、测试。 \u003Cbr\u002F\u003E在我来看，用自己的语言对来整理对一个问题的认识，就是理解和扩展的一种形式，而发在这里也算是一种应用、测试兼顾纠错的形式了。\n\u003Cbr\u002F\u003E\u003Cbr\u002F\u003E首先来看什么是回归问题，直白来说，就是给定 \u003Cbr\u002F\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cvec+X+%5Cin+%5Cmathcal+R_D+%281.1%29\" alt=\"\\vec X \\in \\mathcal R_D (1.1)\" eeimg=\"1\"\u002F\u003E\u003Cbr\u002F\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Y+%5Cin+%5Cmathcal+R+%281.2%29\" alt=\"Y \\in \\mathcal R (1.2)\" eeimg=\"1\"\u002F\u003E\u003Cbr\u002F\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Y%3D%5Cboldsymbol+f%28%5Cvec+X%29+%281.3%29\" alt=\"Y=\\boldsymbol f(\\vec X) (1.3)\" eeimg=\"1\"\u002F\u003E\u003Cbr\u002F\u003E其中映射函数\n\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cboldsymbol+f\" alt=\"\\boldsymbol f\" eeimg=\"1\"\u002F\u003E未知，但是我们手上有一堆数据样本\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal+T\" alt=\"\\mathcal T\" eeimg=\"1\"\u002F\u003E，形式如下：\n\u003Cbr\u002F\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28%5Cvec+X_1%2C+Y_1%29%2C+%28%5Cvec+X_2%2C+Y_2%29%2C+...+%28%5Cvec+X_n%2C+Y_n%29++++++%282.1%29\" alt=\"(\\vec X_1, Y_1), (\\vec X_2, Y_2), ... (\\vec X_n, Y_n)      (2.1)\" eeimg=\"1\"\u002F\u003E\u003Cbr\u002F\u003E我们期望从数据样本里推断出映射函数\n\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cboldsymbol+f\" alt=\"\\boldsymbol f\" eeimg=\"1\"\u002F\u003E，满足\u003Cbr\u002F\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=argmin_fE%28f%28%5Cvec+X_i%29+-+Y_i%29%5E2+++%283.1%29\" alt=\"argmin_fE(f(\\vec X_i) - Y_i)^2   (3.1)\" eeimg=\"1\"\u002F\u003E\u003Cbr\u002F\u003E即期望推断出的映射函数在数据样本上与真实目标的期望差异尽可能最小化。\n\u003Cbr\u002F\u003E\u003Cbr\u002F\u003E通常来说，数据样本中每个样本的出现频率都可以认为是1，而我们要推断的映射函数可以认为是\n\u003Cbr\u002F\u003E一个线性函数\u003Cbr\u002F\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=f%28%5Cvec+X%29%3Dw_1X_1+%2B+w_2X_2+%2B+...+%2B+w_nX_n+%2B+%5Cbeta%284.1%29\" alt=\"f(\\vec X)=w_1X_1 + w_2X_2 + ... + w_nX_n + \\beta(4.1)\" eeimg=\"1\"\u002F\u003E\u003Cbr\u002F\u003E其中\n\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=w_1%2C+w_2%2C+...+w_n%2C+%5Cbeta+%284.2%29\" alt=\"w_1, w_2, ... w_n, \\beta (4.2)\" eeimg=\"1\"\u002F\u003E就是我们要推断的关键参数了。\n\u003Cbr\u002F\u003E这样的问题就是线性回归(Linear Regression)问题。\n\u003Cbr\u002F\u003E\u003Cbr\u002F\u003EOrdinary Linear Square的求解方法很直白，结合上面的描述，我们可以将 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=argmin_fE%28f%28%5Cvec+X%29+-+Y%29%5E2+%285.1%29\" alt=\"argmin_fE(f(\\vec X) - Y)^2 (5.1)\" eeimg=\"1\"\u002F\u003E\u003Cbr\u002F\u003E具像化为求解函数\u003Cbr\u002F\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csum_%7Bi%3D1%7D%5E%7BN%7D%28f%28%5Cvec+X_i%29+-+Y_i%29%5E2+%285.2%29\" alt=\"\\sum_{i=1}^{N}(f(\\vec X_i) - Y_i)^2 (5.2)\" eeimg=\"1\"\u002F\u003E\u003Cbr\u002F\u003E的最小值以及对应的关键参数。\u003Cbr\u002F\u003E\u003Cbr\u002F\u003E对于这个目标函数，我们可以通过求导计算[2]，直接得出解析解如下 ：\n\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cvec+w+%3D+%28X_TX%29%5E%7B-1%7DX%5ET%5Cvec+Y%285.3%29\" alt=\"\\vec w = (X_TX)^{-1}X^T\\vec Y(5.3)\" eeimg=\"1\"\u002F\u003E\u003Cbr\u002F\u003E\u003Cbr\u002F\u003E当然，这是一个典型的Convex优化问题，也可以通过迭代求优的算法来进行求解，比如Gradient Descent或者Newton法[2]。\n\u003Cbr\u002F\u003E\u003Cbr\u002F\u003E看起来不错，那么为什么我们还要在OLS的基础上提供了Ridge Regression(L2正则)和Lasso Regression(L1正则)呢?\n\u003Cbr\u002F\u003E\u003Cbr\u002F\u003E如果说得笼统一些的话，是为了避免over-fit，如果再深入一些，则可以这样来理解：\n\u003Cbr\u002F\u003E      不引入正则项的OLS的解很可能会不stable，具体来说，两次不同的采样所采集到的训练数据，用于训练同一个线性回归模型，训练数据的些微差异，最终学出的模型差异可能是巨大的。在[3]里有一个例子：\n\u003Cbr\u002F\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002F3720ff6c0dc84fff5000e24cd939cc88_hd.jpg\" data-rawwidth=\"871\" data-rawheight=\"393\" class=\"origin_image zh-lightbox-thumb\" width=\"871\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002F3720ff6c0dc84fff5000e24cd939cc88_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;871&#39; height=&#39;393&#39;&gt;&lt;\u002Fsvg&gt;\" data-rawwidth=\"871\" data-rawheight=\"393\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"871\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002F3720ff6c0dc84fff5000e24cd939cc88_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002F3720ff6c0dc84fff5000e24cd939cc88_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cbr\u002F\u003E还是在[3]里，提供了一个定量的证明：\n\u003Cbr\u002F\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fa2f38e0dad18e77666e30f6f2798e99c_hd.jpg\" data-rawwidth=\"878\" data-rawheight=\"162\" class=\"origin_image zh-lightbox-thumb\" width=\"878\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fa2f38e0dad18e77666e30f6f2798e99c_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;878&#39; height=&#39;162&#39;&gt;&lt;\u002Fsvg&gt;\" data-rawwidth=\"878\" data-rawheight=\"162\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"878\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fa2f38e0dad18e77666e30f6f2798e99c_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fa2f38e0dad18e77666e30f6f2798e99c_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cbr\u002F\u003E结果的证明细节，有兴趣的同学可以自己去查阅，这里直接把关键点引用如下：\n\u003Cbr\u002F\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fa2f38e0dad18e77666e30f6f2798e99c_hd.jpg\" data-rawwidth=\"878\" data-rawheight=\"162\" class=\"origin_image zh-lightbox-thumb\" width=\"878\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fa2f38e0dad18e77666e30f6f2798e99c_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;878&#39; height=&#39;162&#39;&gt;&lt;\u002Fsvg&gt;\" data-rawwidth=\"878\" data-rawheight=\"162\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"878\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fa2f38e0dad18e77666e30f6f2798e99c_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fa2f38e0dad18e77666e30f6f2798e99c_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cbr\u002F\u003E一个有名的病态矩阵是Hilbert矩阵[4]，其形如下：\n\u003Cbr\u002F\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fed68d8679bc49283f46b9da45e4a7d61_hd.jpg\" data-rawwidth=\"436\" data-rawheight=\"166\" class=\"origin_image zh-lightbox-thumb\" width=\"436\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fed68d8679bc49283f46b9da45e4a7d61_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;436&#39; height=&#39;166&#39;&gt;&lt;\u002Fsvg&gt;\" data-rawwidth=\"436\" data-rawheight=\"166\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"436\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fed68d8679bc49283f46b9da45e4a7d61_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fed68d8679bc49283f46b9da45e4a7d61_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cbr\u002F\u003E\u003Cbr\u002F\u003E再回到我们关于OLS的讨论，我们不难看出，随着训练样本采样次数的增加，采样到病态阵的概率会增多，这样一来学出的模型的稳定性就比较\n\u003Cbr\u002F\u003E差。想象一下，今天训练出来的模型跟明天训练出的模型，存在明显的模型权值差异，这看起来并不是一件非常好的事情。\n\u003Cbr\u002F\u003E\u003Cbr\u002F\u003E那么病态阵的根本原因是什么呢？条件数的描述还是相对有些抽象，所以这里又引入了奇异阵的概念。\n\u003Cbr\u002F\u003E\u003Cbr\u002F\u003E什么是奇异阵呢？\n\u003Cbr\u002F\u003E形式化来说，不存在逆矩阵的方阵（因为OLS的闭式解里，需要求逆的矩阵是\n\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=X%5ETX%286.1%29\" alt=\"X^TX(6.1)\" eeimg=\"1\"\u002F\u003E\u003Cbr\u002F\u003E一定是一个方阵，所以这里仅讨论方阵这一特殊形态）就是奇异阵，即\n\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A%5E%7B-1%7DA%3D%5Cmathcal+I%2C+%5Cnot+%5Cexists+A%5E%7B-1%7D%286.2%29\" alt=\"A^{-1}A=\\mathcal I, \\not \\exists A^{-1}(6.2)\" eeimg=\"1\"\u002F\u003E\u003Cbr\u002F\u003E那么再具体一些，什么样的方阵会不存在逆呢？\n\u003Cbr\u002F\u003E     非满秩、矩阵的特征值之和为0、矩阵的行列式为0。\n\u003Cbr\u002F\u003E满足这三个条件中的任意一个条件，即可推出方阵为奇异阵，不可求逆，实际上这三个条件是可以互相推导出来的。\n\u003Cbr\u002F\u003E而我们又知道，一个方阵的逆矩阵可以通过其伴随矩阵和行列式求出来[7]\n\u003Cbr\u002F\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A%5E%7B-1%7D%3D%5Cfrac+%7B1%7D+%7B%7CA%7C%7D+adj%28A%29%3B+adj%28A%29+%5Cin+R%5E%7Bn%2An%7D%2C+%28adj%28A%29%29_%7Bij%7D%3D%28-1%29%5E%7Bi%2Bj%7D%7CA_%7B%5Csetminus+j%2C%5Csetminus+i%7D%7C%286.3%29\" alt=\"A^{-1}=\\frac {1} {|A|} adj(A); adj(A) \\in R^{n*n}, (adj(A))_{ij}=(-1)^{i+j}|A_{\\setminus j,\\setminus i}|(6.3)\" eeimg=\"1\"\u002F\u003E\u003Cbr\u002F\u003E\u003Cbr\u002F\u003E从这里，其实可以看的出来，对于近似奇异阵，其行列式非常接近于0，所以\u003Cbr\u002F\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cfrac+1+%7B%7CA%7C%7D+%286.4%29\" alt=\"\\frac 1 {|A|} (6.4)\" eeimg=\"1\"\u002F\u003E\u003Cbr\u002F\u003E会是一个非常大的数，这其实反映出来就是在计算A的逆矩阵时，伴随矩阵上的些微变化，会被很大程度上放大，就会导致多次采样出来的训练数据，学出的模型差异很大，这是除了上面提到的条件数以外的另一种比较形象的解释了。 \n\u003Cbr\u002F\u003E如果类比普通代数的话，奇异阵就好比是0，0不存在倒数，越接近0的数，其倒数越大，同样，奇异阵不存在逆矩阵，而接近奇异阵的逆矩阵的波动也会比较大。\n\u003Cbr\u002F\u003E所以跟奇异阵有多像(数学语言里称为near singularity)，决定了我们的线性回归模型有多么不稳定。\n\u003Cbr\u002F\u003E既然知道了奇异阵的性质并且了解到是奇异阵使得从训练数据中学出来的线性回归模型不那么稳定，那么我们是不是可以人为地通过引入一些transformation让\n\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=X%5ETX%287.1%29\" alt=\"X^TX(7.1)\" eeimg=\"1\"\u002F\u003E长得跟奇异阵不那么像呢？\n\u003Cbr\u002F\u003E我们知道\n\u003Cbr\u002F\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%7CA%7C%3D%5CPi_i%5En+%5Clambda_i%287.2%29\" alt=\"|A|=\\Pi_i^n \\lambda_i(7.2)\" eeimg=\"1\"\u002F\u003E\u003Cbr\u002F\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=trA%3D%5Csum_%7Bi%3D1%7D%7Bn%7D%5Clambda_i%287.3%29\" alt=\"trA=\\sum_{i=1}{n}\\lambda_i(7.3)\" eeimg=\"1\"\u002F\u003E\u003Cbr\u002F\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=trA%3D%5Csum_%7Bi%3D1%7D%5EnA_%7Bii%7D%287.4%29\" alt=\"trA=\\sum_{i=1}^nA_{ii}(7.4)\" eeimg=\"1\"\u002F\u003E\u003Cbr\u002F\u003E于是一个直观的思路是在方阵\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=X%5ETX\" alt=\"X^TX\" eeimg=\"1\"\u002F\u003E的对角线元素上施加如下的线性变换 \u003Cbr\u002F\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=X%5ETX%2B%5Clambda%5Cmathcal++I+%287.5%29\" alt=\"X^TX+\\lambda\\mathcal  I (7.5)\" eeimg=\"1\"\u002F\u003E\u003Cbr\u002F\u003E其中\n\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal+I\" alt=\"\\mathcal I\" eeimg=\"1\"\u002F\u003E是单位阵。\n\u003Cbr\u002F\u003E有了上面的变换以后，对角线的元素为全零的概率就可以有效降低，也就达到了减少\n\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=X%5ETX\" alt=\"X^TX\" eeimg=\"1\"\u002F\u003E\u003Cbr\u002F\u003Enear singularity的程度，进而减少线性回归模型不稳定性的目的了。\n\u003Cbr\u002F\u003E那么这个变换跟Ridge或是Lasso有什么关系呢？\n\u003Cbr\u002F\u003E实际上，\n\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cvec+w%3D%28X%5ETX%2B%5Clambda+%5Cmathcal+I%29%5E%7B-1%7DX%5Cvec+Y%287.6%29\" alt=\"\\vec w=(X^TX+\\lambda \\mathcal I)^{-1}X\\vec Y(7.6)\" eeimg=\"1\"\u002F\u003E(7.6)\n正是对下面的\n\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csum_%7Bi%3D1%7D%5E%7BN%7D%28%5Cvec+w%5Cvec+X_i+-+Y_i%29%5E2+%2B+%5Clambda+%7C%7C%5Cvec+w%7C%7C_2%5E2%287.7%29\" alt=\"\\sum_{i=1}^{N}(\\vec w\\vec X_i - Y_i)^2 + \\lambda ||\\vec w||_2^2(7.7)\" eeimg=\"1\"\u002F\u003E\u003Cbr\u002F\u003E这个优化问题为\n\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cvec+w\" alt=\"\\vec w\" eeimg=\"1\"\u002F\u003E计算闭式解得到的结果(在[2]里OLS的闭式解的推导过程里加入二范数正则项，蛮自然地就会得到7.6里的结果，[8]里也有类似的推论)。\n\u003Cbr\u002F\u003E而(7.7)正是Ridge Regression的标准写法。\n\u003Cbr\u002F\u003E进一步，Lasso Regression的写法是\n\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csum_%7Bi%3D1%7D%5E%7BN%7D%28%5Cvec+w%5Cvec+X_i+-+Y_i%29%5E2+%2B+%5Clambda+%7C%7C%5Cvec+w%7C%7C_1\" alt=\"\\sum_{i=1}^{N}(\\vec w\\vec X_i - Y_i)^2 + \\lambda ||\\vec w||_1\" eeimg=\"1\"\u002F\u003E\u003Cbr\u002F\u003E这实际上也是在原始矩阵上施加了一些变换，期望离奇异阵远一些，另外1范数的引入，使得模型训练的过程本身包含了model selection的功能，在上面的回复里都举出了很多的例子，在一本像样些的ML\u002FDM的教材里也大抵都有着比形象的示例图，在这里我就不再重复了。\n\u003Cbr\u002F\u003E一个略微想提一下的是，对于2范数，本质上其实是对误差的高斯先验，而1范数则对应于误差的Laplace先验，这算是另一个理解正则化的视角了。\n\u003Cbr\u002F\u003E只不过1范数的引入导致优化目标不再处处连续不能直接求取闭式解，而不得不resort到迭代求优的方法上了，而因为其非处处连续的特点，\n即便是在迭代求优的过程中，也变得有些特殊了，这个我们可以在以后讨论OWLQN和LBFGS算法的时后再详细引出来。\n\u003Cbr\u002F\u003E\u003Cbr\u002F\u003E[1]. \u003Ca href=\"https:\u002F\u002Fwww.zhihu.com\u002Fquestion\u002F38121173\" class=\"internal\"\u003ELinear least squares, Lasso,ridge regression有何本质区别？ - 数据挖掘\u003C\u002Fa\u003E\u003Cbr\u002F\u003E[2]. \u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fcs229.stanford.edu\u002Fnotes\u002Fcs229-notes1.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttp:\u002F\u002F\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Ecs229.stanford.edu\u002Fnote\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003Es\u002Fcs229-notes1.pdf\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003Cbr\u002F\u003E[3]. 《数值分析导论》(\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fbook.douban.com\u002Fsubject\u002F4089812\u002F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E数值分析导论 (豆瓣)\u003C\u002Fa\u003E )的 例6.5.3\n\u003Cbr\u002F\u003E[4]. 《数值分析导论》(\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fbook.douban.com\u002Fsubject\u002F4089812\u002F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E数值分析导论 (豆瓣)\u003C\u002Fa\u003E )的例6.5.5\n\u003Cbr\u002F\u003E[5]. 关于奇异阵的资料。\n\u003Cbr\u002F\u003E[6].  \u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fbook.douban.com\u002Fsubject\u002F25783654\u002F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E如何高效学习 (豆瓣)\u003C\u002Fa\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fbook.douban.com\u002Fsubject\u002F25783654\u002F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E如何高效学习 (豆瓣)\u003C\u002Fa\u003E\u003Cbr\u002F\u003E[7]. \u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fcs229.stanford.edu\u002Fsection\u002Fcs229-linalg.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttp:\u002F\u002F\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Ecs229.stanford.edu\u002Fsect\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003Eion\u002Fcs229-linalg.pdf\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003Cbr\u002F\u003E[8]. PRML 3.28","editableContent":"","excerpt":"线性回归问题是很经典的机器学习问题了。 适用的方法也蛮多，有标准的Ordinary Least Squares，还有带了L2正则的Ridge Regression以及L1正则的Lasso Regression。 这些不同的回归模型的差异和设计动机是什么？ 在本帖的一个高票回答[1]里，把这个问题讨论得其实已经相当清楚了。 我在这里的回答更多是一个知识性的总结，在Scott Young的《如何高效学习》[6]里提到高效学习的几个环节： 获取、理解、拓展、纠错、应用、测试。 在…","collapsedBy":"nobody","collapseReason":"","annotationAction":null,"markInfos":[],"relevantInfo":{"isRelevant":false,"relevantType":"","relevantText":""},"suggestEdit":{"reason":"","status":false,"tip":"","title":"","unnormalDetails":{"status":"","description":"","reason":"","reasonId":0,"note":""},"url":""},"isLabeled":false,"rewardInfo":{"canOpenReward":false,"isRewardable":false,"rewardMemberCount":0,"rewardTotalMoney":0,"tagline":""},"relationship":{"isAuthor":false,"isAuthorized":false,"isNothelp":false,"isThanked":false,"isRecognized":false,"voting":0,"upvotedFollowees":[]}},"104955389":{"id":104955389,"type":"answer","answerType":"normal","question":{"type":"question","id":38121173,"title":"Linear least squares, Lasso,ridge regression有何本质区别？","questionType":"normal","created":1449061786,"updatedTime":1449061786,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F38121173","relationship":{}},"author":{"id":"3262be16282302c2060deb22faa2766c","urlToken":"yiorfun","name":"Yeung Evan","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002F02c4a4fbc_is.jpg","avatarUrlTemplate":"https:\u002F\u002Fpic3.zhimg.com\u002F02c4a4fbc_{size}.jpg","isOrg":false,"type":"people","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fpeople\u002F3262be16282302c2060deb22faa2766c","userType":"people","headline":"Hell, it&#39;s about time.","badge":[],"gender":1,"isAdvertiser":false,"followerCount":1450,"isFollowed":false,"isPrivacy":false},"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fanswers\u002F104955389","isCollapsed":false,"createdTime":1465376443,"updatedTime":1493504036,"extras":"","isCopyable":true,"isNormal":true,"voteupCount":35,"commentCount":4,"isSticky":false,"adminClosedComment":false,"commentPermission":"all","canComment":{"reason":"","status":true},"reshipmentSettings":"allowed","content":"\u003Cp\u003E不太赞同高票答案关于Lasso产生的motivation，高票答案说：“你可能又要问了，多加的那一项凭什么是模长呢？不能把2-norm改成1-norm吗？”实际上，Tibshirani (1996) 原文中对此也有颇多阐述，Lasso产生的初因并非只是简单地把2修改到1（虽然表现出来确实如此，后面对此详叙）。我尝试在楼上各个答案的基础上，添加一些历史线索。\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\u003Cp\u003E先回答题主问题：\u003C\u002Fp\u003E\u003Cp\u003E最小二乘法的一些讨论可参见 \u003Ca href=\"https:\u002F\u002Fwww.zhihu.com\u002Fquestion\u002F23817253\u002Fanswer\u002F85998617?from=profile_answer_card\" class=\"internal\"\u003ELogistic 回归模型的参数估计为什么不能采用最小二乘法？ - Yeung Evan 的回答\u003C\u002Fa\u003E ；Linear least squares是对线性回归模型进行参数估计的一种选择，同时我们当然可以从非最小二乘的方法对线性回归模型的参数进行估计。在使用最小二乘法进行求解时，我们会对线性回归模型进行假设，当这些假设在实际中并不满足时，最小二乘法是有问题的。其中很重要的一个假设是要求各个变量要相互独立，而实际样本可能会有较大的共线性（multicollinearity）。从数学上来说，这种情况会造成\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctextbf%7BX%7D%5E%5Ctop+%5Ctextbf%7BX%7D\" alt=\"\\textbf{X}^\\top \\textbf{X}\" eeimg=\"1\"\u002F\u003E（至少一个）特征值很小，从而估计量的MSE会很大（*）。所以ridge regression就很粗暴，因为\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctextbf%7BX%7D%5E%5Ctop+%5Ctextbf%7BX%7D\" alt=\"\\textbf{X}^\\top \\textbf{X}\" eeimg=\"1\"\u002F\u003E（至少一个）特征值很小，所以就强行加上一个\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda+%5Ctextbf%7BI%7D\" alt=\"\\lambda \\textbf{I}\" eeimg=\"1\"\u002F\u003E把特征值“掰”大。ridge regression 大约是在1970年Hoerl提出。\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\u003Cp\u003E后来（或同时）人们考虑回归方程的选择问题，比如说，不一定所有的自变量的数据都需要放入模型，因为有些自变量本身对模型的影响不大，但引入之后却会增加对其他参数的估计所需的计算量以及减少其精度（那个年代计算机还远未普及）。但到底哪些变量需要选，哪些变量需要弃，就是一个颇为棘手的问题。后来比较经典的处理方法就是逐步回归法，又细分为向前逐步回归和向后逐步回归。这一方法的成熟大约也是在70年代左右。再然后，此问题的发展是1993年Frank和Friedman提出的bridge regression，他们提出的对优化目标函数的限制已经是形如\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csum_j%7C%5Cbeta_j%7C+%5E+%7B%5Cgamma%7D\" alt=\"\\sum_j|\\beta_j| ^ {\\gamma}\" eeimg=\"1\"\u002F\u003E的样子了，但可惜他们没有能力进一步对\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"\u002F\u003E取特殊值（**）时的情况进行讨论。\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\u003Cp\u003E最终在1996年Tibshirani把上述两个问题同时解决了：\u003C\u002Fp\u003E\u003Cp\u003E（*）MSE偏大时估计的精确度会降低，因为方差偏大；\u003C\u002Fp\u003E\u003Cp\u003E（**）\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cgamma%3D1\" alt=\"\\gamma=1\" eeimg=\"1\"\u002F\u003E时模型具有“挑选”自变量的能力，并将这个情况下的优化问题讲清楚了；\n\u003C\u002Fp\u003E\u003Cp\u003E这里回到本文开头，看上去只是把 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cgamma+%3D+2\" alt=\"\\gamma = 2\" eeimg=\"1\"\u002F\u003E 改成了 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cgamma+%3D+1\" alt=\"\\gamma = 1\" eeimg=\"1\"\u002F\u003E ，但实际上背后的工作原理相当复杂，这也是CS的一些研究在严密性上，相对容易忽视的地方：即对变量的选择性从何而来？换句话说，怎么证明把 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"\u002F\u003E 数值改了，就把变量压缩了呢？\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\u003Cp\u003E要回答这个问题是相当难的，仅以ridge回归为例，在考虑 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cgamma+%3D+2\" alt=\"\\gamma = 2\" eeimg=\"1\"\u002F\u003E 时，可以通过相当的篇幅证明，在此约束下的回归结果 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Chat%7B%5Cbeta%7D_%5Ctext%7Bridge%7D+%3C+%5Chat%7B%5Cbeta%7D_%5Ctext%7BOLS%7D\" alt=\"\\hat{\\beta}_\\text{ridge} &lt; \\hat{\\beta}_\\text{OLS}\" eeimg=\"1\"\u002F\u003E 。这意味着，估计向量的长度缩短了——当然其中的某些分量一定变小了——这就是最初的变量被压缩的雏形。而更一般的变量选择性，需要更艰深的证明，而不是简单一句“把 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"\u002F\u003E 值改了，模型就能避免over fitting”——中间的因果逻辑需要相当多的数学推导。\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\u003Cp\u003ELasso后续的发展请关注千面人的答案，当然包括真正把Lasso变得“好算”的Lars算法也是该问题的里程碑之一。\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\u003Cp\u003E上述即是Lasso和ridge regression的一些历史线索，是最自然、最符合一个研究发展脉络的逻辑。在机器学习中还有一些和此问题相关的概念，比如正则化，它们本质上都是相似的，在机器学习中引入正则化这一概念的渊源非我能置喙，但至少说引入正则因子或者惩罚项是为了避免overfitting，是不太自然的。\u003C\u002Fp\u003E","editableContent":"","excerpt":"不太赞同高票答案关于Lasso产生的motivation，高票答案说：“你可能又要问了，多加的那一项凭什么是模长呢？不能把2-norm改成1-norm吗？”实际上，Tibshirani (1996) 原文中对此也有颇多阐述，Lasso产生的初因并非只是简单地把2修改到1（虽然表现出来确实如此，后面对此详叙）。我尝试在楼上各个答案的基础上，添加一些历史线索。 先回答题主问题： 最小二乘法的一些讨论可参见 Logistic 回归模型的参数估计为什么不能采用最小二…","collapsedBy":"nobody","collapseReason":"","annotationAction":null,"markInfos":[],"relevantInfo":{"isRelevant":false,"relevantType":"","relevantText":""},"suggestEdit":{"reason":"","status":false,"tip":"","title":"","unnormalDetails":{"status":"","description":"","reason":"","reasonId":0,"note":""},"url":""},"isLabeled":false,"rewardInfo":{"canOpenReward":false,"isRewardable":false,"rewardMemberCount":0,"rewardTotalMoney":0,"tagline":""},"relationship":{"isAuthor":false,"isAuthorized":false,"isNothelp":false,"isThanked":false,"isRecognized":false,"voting":0,"upvotedFollowees":[]}},"166238142":{"id":166238142,"type":"answer","answerType":"normal","question":{"type":"question","id":38121173,"title":"Linear least squares, Lasso,ridge regression有何本质区别？","questionType":"normal","created":1449061786,"updatedTime":1449061786,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F38121173","relationship":{}},"author":{"id":"edcb3f84977d280287bc025bdbe86465","urlToken":"","name":"知乎用户","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fda8e974dc_is.jpg","avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fda8e974dc_{size}.jpg","isOrg":false,"type":"people","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fpeople\u002F0","userType":"people","headline":"","badge":[],"gender":1,"isAdvertiser":false,"followerCount":7873,"isFollowed":false,"isPrivacy":true},"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fanswers\u002F166238142","isCollapsed":false,"createdTime":1494185658,"updatedTime":1494200346,"extras":"","isCopyable":false,"isNormal":true,"voteupCount":184,"commentCount":18,"isSticky":false,"adminClosedComment":false,"commentPermission":"all","canComment":{"reason":"","status":true},"reshipmentSettings":"need_payment","content":"\u003Cp\u003E很多回答都很全面了，大意就是lasso在优化过程的目标函数中使用如下的L1 penalty：\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002Fv2-6640aa578fb80502a3548a133e7b42db_hd.jpg\" data-rawwidth=\"764\" data-rawheight=\"154\" class=\"origin_image zh-lightbox-thumb\" width=\"764\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-6640aa578fb80502a3548a133e7b42db_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;764&#39; height=&#39;154&#39;&gt;&lt;\u002Fsvg&gt;\" data-rawwidth=\"764\" data-rawheight=\"154\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"764\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-6640aa578fb80502a3548a133e7b42db_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002Fv2-6640aa578fb80502a3548a133e7b42db_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E从而把一些线性回归项的系数“逼成”零；ridge是用L2 penalty，旨在把系数变得小一些，但非完全成零。两者原理上的区别可由下图表示：\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002Fv2-2a88e2acc009fa4de3edeb51e683ca02_hd.jpg\" data-rawwidth=\"602\" data-rawheight=\"399\" class=\"origin_image zh-lightbox-thumb\" width=\"602\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2a88e2acc009fa4de3edeb51e683ca02_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;602&#39; height=&#39;399&#39;&gt;&lt;\u002Fsvg&gt;\" data-rawwidth=\"602\" data-rawheight=\"399\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"602\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2a88e2acc009fa4de3edeb51e683ca02_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002Fv2-2a88e2acc009fa4de3edeb51e683ca02_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E不难看出由于L1 penalty规定的范围“四四方方、有棱有角”，所以最优解的系数会被刚好缩成零，因此lasso可以实现对变量的选择（系数为零的变量就被筛掉了）。\u003C\u002Fp\u003E\u003Cp\u003E有趣的是，我们还可以将所有变量分组，然后在目标函数中惩罚每一组的L2范数，这样达到的效果就是可以将一整组的系数同时消成零，即抹掉一整组的变量，这种手法叫做\u003Cb\u003Egroup lasso\u003C\u002Fb\u003E，其目标函数如下：\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002Fv2-4221797634d200611e9aff12e8242761_hd.jpg\" data-rawwidth=\"957\" data-rawheight=\"164\" class=\"origin_image zh-lightbox-thumb\" width=\"957\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-4221797634d200611e9aff12e8242761_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;957&#39; height=&#39;164&#39;&gt;&lt;\u002Fsvg&gt;\" data-rawwidth=\"957\" data-rawheight=\"164\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"957\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-4221797634d200611e9aff12e8242761_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002Fv2-4221797634d200611e9aff12e8242761_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E其中我们把所有变量分为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=m\" alt=\"m\" eeimg=\"1\"\u002F\u003E 组，第一项是通常的OLS，第二项是每一组系数的L2范数之和。这里， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E控制整体惩罚的力度，\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csqrt%7B%5Crho_l%7D\" alt=\"\\sqrt{\\rho_l}\" eeimg=\"1\"\u002F\u003E是每一组的加权，可以按需调节。\u003C\u002Fp\u003E\u003Cp\u003E比如一个regression若有10个系数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbeta_1%2C+%5Cbeta_2%2C+...%2C+%5Cbeta_%7B10%7D\" alt=\"\\beta_1, \\beta_2, ..., \\beta_{10}\" eeimg=\"1\"\u002F\u003E，我们如果选择将其分成2组：其中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbeta_1%2C+...%2C+%5Cbeta_5\" alt=\"\\beta_1, ..., \\beta_5\" eeimg=\"1\"\u002F\u003E 一组， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbeta_6%2C+...%2C+%5Cbeta_%7B10%7D\" alt=\"\\beta_6, ..., \\beta_{10}\" eeimg=\"1\"\u002F\u003E 一组。那么group lasso的惩罚项目将会是：\u003C\u002Fp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Crho%28%5Csqrt%7Bp_1%7D%5Csqrt%7B%5Csum_%7Bi+%3D+1%7D%5E5%5Cbeta_i%5E2%7D+%2B+%5Csqrt%7Bp_2%7D%5Csqrt%7B%5Csum_%7Bj+%3D+6%7D%5E%7B10%7D%5Cbeta_j%5E2%7D%29\" alt=\"\\rho(\\sqrt{p_1}\\sqrt{\\sum_{i = 1}^5\\beta_i^2} + \\sqrt{p_2}\\sqrt{\\sum_{j = 6}^{10}\\beta_j^2})\" eeimg=\"1\"\u002F\u003E\u003Cp\u003E通过施加group-wise的L2 penalty，我们有可能促使 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbeta_1+%3D+%5Cbeta_2+%3D+...+%3D+%5Cbeta_5+%3D+0\" alt=\"\\beta_1 = \\beta_2 = ... = \\beta_5 = 0\" eeimg=\"1\"\u002F\u003E 或者 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbeta_6+%3D+%5Cbeta_7+%3D+...+%3D+%5Cbeta_%7B10%7D+%3D+0\" alt=\"\\beta_6 = \\beta_7 = ... = \\beta_{10} = 0\" eeimg=\"1\"\u002F\u003E 。 \u003C\u002Fp\u003E\u003Cp\u003E最后，还有一种lasso和group lasso的奇葩结合，叫做\u003Cb\u003Esparse group lasso\u003C\u002Fb\u003E，由 Simon et al 在2013年提出，sparse group lasso的目标函数（如下）的惩罚项中，既有所有系数的L1范数，又有每一组系数的L2范数\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fv2-63a3bd6361cf5420a4c0c8878729c41f_hd.jpg\" data-rawwidth=\"1024\" data-rawheight=\"154\" class=\"origin_image zh-lightbox-thumb\" width=\"1024\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-63a3bd6361cf5420a4c0c8878729c41f_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1024&#39; height=&#39;154&#39;&gt;&lt;\u002Fsvg&gt;\" data-rawwidth=\"1024\" data-rawheight=\"154\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1024\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-63a3bd6361cf5420a4c0c8878729c41f_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fv2-63a3bd6361cf5420a4c0c8878729c41f_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E其中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E 依然控制总体的惩罚力度，有新引入 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"\u002F\u003E 控制两个惩罚项之间的相互强弱。所以sparse group lasso既可以把系数和变量一组一组地筛掉，又可以在剩下的组中筛掉一些单个的系数，原理图如下：\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-0382de0682555ee4e73f0e63dbd2abe9_hd.jpg\" data-rawwidth=\"443\" data-rawheight=\"146\" class=\"origin_image zh-lightbox-thumb\" width=\"443\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0382de0682555ee4e73f0e63dbd2abe9_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;443&#39; height=&#39;146&#39;&gt;&lt;\u002Fsvg&gt;\" data-rawwidth=\"443\" data-rawheight=\"146\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"443\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0382de0682555ee4e73f0e63dbd2abe9_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-0382de0682555ee4e73f0e63dbd2abe9_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E当然了，这只是在简单OLS背景下的lasso、ridge、和group lasso和sparse group lasso，更常用的目标函数的第一项一般是log likelihood（用于maximum likelihood手法）。相似的概念也可以迁移到其他场景，比如因子分析模型（factor analysis model），其中group lasso可以帮助进行对可被观测的变量选取，而sparse group lasso可以选取隐藏因子，我统计的thesis做的就是这个啦。\u003C\u002Fp\u003E","editableContent":"","excerpt":"很多回答都很全面了，大意就是lasso在优化过程的目标函数中使用如下的L1 penalty： [图片] 从而把一些线性回归项的系数“逼成”零；ridge是用L2 penalty，旨在把系数变得小一些，但非完全成零。两者原理上的区别可由下图表示： [图片] 不难看出由于L1 penalty规定的范围“四四方方、有棱有角”，所以最优解的系数会被刚好缩成零，因此lasso可以实现对变量的选择（系数为零的变量就被筛掉了）。 有趣的是，我们还可以将所有变量分组，然后在目标…","collapsedBy":"nobody","collapseReason":"","annotationAction":null,"markInfos":[],"relevantInfo":{"isRelevant":false,"relevantType":"","relevantText":""},"suggestEdit":{"reason":"","status":false,"tip":"","title":"","unnormalDetails":{"status":"","description":"","reason":"","reasonId":0,"note":""},"url":""},"isLabeled":false,"rewardInfo":{"canOpenReward":false,"isRewardable":false,"rewardMemberCount":0,"rewardTotalMoney":0,"tagline":""},"relationship":{"isAuthor":false,"isAuthorized":false,"isNothelp":false,"isThanked":false,"isRecognized":false,"voting":0,"upvotedFollowees":[]}},"403986652":{"id":403986652,"type":"answer","answerType":"normal","question":{"type":"question","id":38121173,"title":"Linear least squares, Lasso,ridge regression有何本质区别？","questionType":"normal","created":1449061786,"updatedTime":1449061786,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F38121173","relationship":{}},"author":{"id":"2aabe2f784afcc242b18cad062235cca","urlToken":"larry-LJY","name":"童话李","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-116898dc61a60b17b597e10cf47f3fa1_is.jpg","avatarUrlTemplate":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-116898dc61a60b17b597e10cf47f3fa1_{size}.jpg","isOrg":false,"type":"people","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fpeople\u002F2aabe2f784afcc242b18cad062235cca","userType":"people","headline":"你哭着对我说：","badge":[],"gender":1,"isAdvertiser":false,"followerCount":480,"isFollowed":false,"isPrivacy":false},"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fanswers\u002F403986652","isCollapsed":false,"createdTime":1527545064,"updatedTime":1527565285,"extras":"","isCopyable":true,"isNormal":true,"voteupCount":416,"commentCount":37,"isSticky":false,"adminClosedComment":false,"commentPermission":"all","canComment":{"reason":"","status":true},"reshipmentSettings":"allowed","content":"\u003Cp\u003E这两天的项目就是LASSO跟RIDGE，讲讲我的体会。\u003C\u002Fp\u003E\u003Cp\u003ELASSO这两年莫名的非常火，不管什么经济学话题，\u003Cb\u003E只要涉及到解释变量有效性的问题，评论人或者审稿人都想看看，你这玩意儿做的LASSO有啥结果没有。潜台词就是，如果LASSO做不出结果是不是你这个话题本身就有问题呢。\u003C\u002Fb\u003E过两年这阵风过去可能大家的想法又变了，不过身处其中普通人别无选择只能适应，无论如何都不能在潮流中落了下风。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E公式之前好多人都写了，我就直接摆几个别人总结最简单的结果，这对我理解背后的机制帮助非常大，图懒得画，都是从网上贴的。\u003C\u002Fp\u003E\u003Cp\u003E先随便生成一组数\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=y%3Dsin+%28x%29%2B%5Cepsilon%5C%5C+x+%5Cin+%5B1%2F3%5Cpi%2C5%2F3%5Cpi%5D\" alt=\"y=sin (x)+\\epsilon\\\\ x \\in [1\u002F3\\pi,5\u002F3\\pi]\" eeimg=\"1\"\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E本质上是三角函数加上一个正太分布的随机扰动，大概长这样\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002Fv2-d87347a2af705634b5572bf36d446cba_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"300\" data-rawheight=\"245\" data-default-watermark-src=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-33c0bce2a63972904b39ff9084fd6ffe_hd.jpg\" class=\"content_image\" width=\"300\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;300&#39; height=&#39;245&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"300\" data-rawheight=\"245\" data-default-watermark-src=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-33c0bce2a63972904b39ff9084fd6ffe_hd.jpg\" class=\"content_image lazy\" width=\"300\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002Fv2-d87347a2af705634b5572bf36d446cba_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E接着再跑15个OLS回归，里面加上1到15阶的x作为回归变量，\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=y%3Dx%5E%7B0%7D%2Bx%5E%7B1%7D%2B...%2Bx%5E%7Bn%7D%5C%5C+n%3D1%2C...%2C15\" alt=\"y=x^{0}+x^{1}+...+x^{n}\\\\ n=1,...,15\" eeimg=\"1\"\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E拟合结果大概长这样\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-ae7bc7c2e8cd1ddf1c43ba2f31fbb983_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1024\" data-rawheight=\"854\" data-default-watermark-src=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fv2-929ff4f7a3dcc02beb48aa527044cd64_hd.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1024\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-ae7bc7c2e8cd1ddf1c43ba2f31fbb983_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1024&#39; height=&#39;854&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1024\" data-rawheight=\"854\" data-default-watermark-src=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fv2-929ff4f7a3dcc02beb48aa527044cd64_hd.jpg\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1024\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-ae7bc7c2e8cd1ddf1c43ba2f31fbb983_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-ae7bc7c2e8cd1ddf1c43ba2f31fbb983_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E上图可以看出，从1到15阶的x，确实拟合的越来越精确，可是与背后真正的函数sin(x)的距离，也经历了先接近后远离的情况。原因是OLS为了把拟合精确度提高，会尽力把噪音也拟合上，最终造成了所谓的过度拟合(overfitting)，catch the noise, not the signal。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E那么一个合理的问题就是：如何才能避免过度拟合？\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E为了回答这个问题，就先看一眼刚刚回归出来的系数，大概长这样。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-ce63b4c6904500a93ab00afbb82ec7ca_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"759\" data-default-watermark-src=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fv2-f3c8655f5455b0d62a997f059a427c2c_hd.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1600\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-ce63b4c6904500a93ab00afbb82ec7ca_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1600&#39; height=&#39;759&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"759\" data-default-watermark-src=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fv2-f3c8655f5455b0d62a997f059a427c2c_hd.jpg\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1600\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-ce63b4c6904500a93ab00afbb82ec7ca_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-ce63b4c6904500a93ab00afbb82ec7ca_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E有看到什么趋势没有？\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E一个很明显的趋势就是，系数的数量级从个位数奔到了十的5次方到6次方！\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E直觉就是大的系数可以把X微小的变动放大，通过多个正负项的叠加尽量把每个点都拟合上。\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E这就是在日常实践中判断过度拟合的一个重要标准，系数如果大的离谱，多半是过度拟合了。至于多少是离谱，需要根据经验判断。单变量股票收益率预测回归，在采用百分比收益率，并把X标准化到(0,1)之后如果出来个几十的系数，多半是过度拟合了。就像R^2如果做到20%以上多半是有look ahead bias，其他结果我根本就不想往下看。。。\u003C\u002Fp\u003E\u003Cp\u003E那么好怎么解决这个问题？\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E一个非常直觉的解决方法就是，在目标函数里面把过大的系数进行惩罚(\u003Cu\u003Epenalty\u003C\u002Fu\u003E)。\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003ELASSO与RIDGE的区别就是怎么进行这个惩罚。\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E先说LASSO，\u003C\u002Fp\u003E\u003Cp\u003E它是这样做惩罚的，在OLS拟合的基础上，对其系数的绝对值进行惩罚，目标函数长这样\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=argmin%28y-wx%29%5E2%2B%5Calpha+%7Cw%7C\" alt=\"argmin(y-wx)^2+\\alpha |w|\" eeimg=\"1\"\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E这样写目标函数就是想达到一个平衡，第一拟合的误差要小，第二\u003Cb\u003E系数的绝对值\u003C\u002Fb\u003E不能太大。\u003C\u002Fp\u003E\u003Cp\u003E拟合的图像跟系数分别如下\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-184f95e9329f8e71597d2b62724eb21f_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1024\" data-rawheight=\"833\" data-default-watermark-src=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fv2-8ba8e565a8cdc9728cd57e7760f9a536_hd.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1024\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-184f95e9329f8e71597d2b62724eb21f_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1024&#39; height=&#39;833&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1024\" data-rawheight=\"833\" data-default-watermark-src=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fv2-8ba8e565a8cdc9728cd57e7760f9a536_hd.jpg\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1024\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-184f95e9329f8e71597d2b62724eb21f_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-184f95e9329f8e71597d2b62724eb21f_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-09d4817a8332a69800d58f6469deee21_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1024\" data-rawheight=\"334\" data-default-watermark-src=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fv2-5f96838e4b4ed52b1f42d7cea8e42019_hd.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1024\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-09d4817a8332a69800d58f6469deee21_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1024&#39; height=&#39;334&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1024\" data-rawheight=\"334\" data-default-watermark-src=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fv2-5f96838e4b4ed52b1f42d7cea8e42019_hd.jpg\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1024\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-09d4817a8332a69800d58f6469deee21_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-09d4817a8332a69800d58f6469deee21_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E有以下几个特点，\u003C\u002Fp\u003E\u003Col\u003E\u003Cli\u003E随着惩罚力度alpha的增加越来越多的系数变成了0\u003C\u002Fli\u003E\u003Cli\u003E系数确实很好的控制在合理区间，以截距项跟一阶项为例，大致都在1到0.X之间。\u003C\u002Fli\u003E\u003Cli\u003E随着惩罚力度的增加拟合越来越差，到最后RSS到了37，出现了拟合不足（underfitting）的问题。\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E再说RIDGE，目标函数长这样\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=argmin%28y-wx%29%5E2%2B%5Clambda+w%5E2\" alt=\"argmin(y-wx)^2+\\lambda w^2\" eeimg=\"1\"\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E也是想达到一个类似的平衡，第一拟合的误差要小，第二\u003Cb\u003E系数的平方\u003C\u002Fb\u003E不能太大。\u003C\u002Fp\u003E\u003Cp\u003E结果如下，\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-f0b57ab6ffc0361c121a4c25958ba22d_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1024\" data-rawheight=\"847\" data-default-watermark-src=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fv2-fddb92b97a3880b239bbac11c8e4daf1_hd.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1024\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-f0b57ab6ffc0361c121a4c25958ba22d_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1024&#39; height=&#39;847&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1024\" data-rawheight=\"847\" data-default-watermark-src=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fv2-fddb92b97a3880b239bbac11c8e4daf1_hd.jpg\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1024\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-f0b57ab6ffc0361c121a4c25958ba22d_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-f0b57ab6ffc0361c121a4c25958ba22d_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-a82e351373b93e2b8b749cff0cae3d17_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1024\" data-rawheight=\"334\" data-default-watermark-src=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-9193be2f228736c914cb7e6cf18e75da_hd.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1024\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-a82e351373b93e2b8b749cff0cae3d17_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1024&#39; height=&#39;334&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1024\" data-rawheight=\"334\" data-default-watermark-src=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-9193be2f228736c914cb7e6cf18e75da_hd.jpg\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1024\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-a82e351373b93e2b8b749cff0cae3d17_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-a82e351373b93e2b8b749cff0cae3d17_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E有以下几个特点，\u003C\u002Fp\u003E\u003Col\u003E\u003Cli\u003E随着惩罚力度alpha的增加越来越多的系数变得很小，但不会到0。\u003C\u002Fli\u003E\u003Cli\u003E随着惩罚力度的增加拟合越来越差，到最后RSS到了23，出现了拟合不足（underfitting）的问题。但拟合不足的问题似乎比相同条件下LASSO来的轻一些。\u003C\u002Fli\u003E\u003Cli\u003E由于系数一直不到0，便没办法做变量选择。\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cp\u003E\u003Cb\u003E接下来的一个问题是，既然惩罚力度alpha太大了容易拟合不足，太低了容易过度拟合。究竟多大的惩罚力度是合适的？\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E这个问题对于OLS，LASSO，和RIDGE，有一个相对标准的做法，用赤池信息准则(AIC)或贝叶斯信息准则(BIC)进行判断。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E多余的公式就不写了，直觉是这样的，AIC大致都是关于惩罚力度的U型函数，条件形同的情况下AIC越小越好，直接选取AIC最低点对应的惩罚力度alpha。\u003C\u002Fb\u003E一个例子就是下图。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-90a4685daecaf241520a09ea74540abf_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"480\" data-default-watermark-src=\"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002Fv2-c28462770bd8c163990a7ae009db01fb_hd.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-90a4685daecaf241520a09ea74540abf_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;640&#39; height=&#39;480&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"480\" data-default-watermark-src=\"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002Fv2-c28462770bd8c163990a7ae009db01fb_hd.jpg\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-90a4685daecaf241520a09ea74540abf_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-90a4685daecaf241520a09ea74540abf_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E最后用一幅图总结三者的关系\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002Fv2-9a7fa8a60918d49e329eb35fd731c59f_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"300\" data-rawheight=\"169\" data-default-watermark-src=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fv2-2a9ca445446e05168abf222dc4a9e4cf_hd.jpg\" class=\"content_image\" width=\"300\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;300&#39; height=&#39;169&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"300\" data-rawheight=\"169\" data-default-watermark-src=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fv2-2a9ca445446e05168abf222dc4a9e4cf_hd.jpg\" class=\"content_image lazy\" width=\"300\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002Fv2-9a7fa8a60918d49e329eb35fd731c59f_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E如果以OLS的系数作为横轴，OLS, LASSO, RIDGE的系数作为纵轴的话，可以画一幅大致如上的图。\u003C\u002Fp\u003E\u003Col\u003E\u003Cli\u003ELASSO本质上对OLS的系数做了一个固定数值的惩罚这个数值大致是1\u002F2alpha，这一点是可以严格证明的。但最终其变动的趋势和OLS是一样的，用图中实例就是红线与蓝线其实是平行的。\u003C\u002Fli\u003E\u003Cli\u003ERIDGE本质上对OLS的系数做了一个比例上的缩减。可以从图中看出，绿线的斜率变低了。\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E这几日睁眼到闭眼琢磨的都是这俩东西，希望对你有用。公式是要好好看，但别太纠结于公式，推了半个笔记本最后可能还是不知道该怎么用LASSO。真正做data的时候，难点肯定都不在这里，有空再写吧。\u003C\u002Fp\u003E","editableContent":"","excerpt":"这两天的项目就是LASSO跟RIDGE，讲讲我的体会。 LASSO这两年莫名的非常火，不管什么经济学话题， 只要涉及到解释变量有效性的问题，评论人或者审稿人都想看看，你这玩意儿做的LASSO有啥结果没有。潜台词就是，如果LASSO做不出结果是不是你这个话题本身就有问题呢。过两年这阵风过去可能大家的想法又变了，不过身处其中普通人别无选择只能适应，无论如何都不能在潮流中落了下风。 公式之前好多人都写了，我就直接摆几个别人总结最…","collapsedBy":"nobody","collapseReason":"","annotationAction":null,"markInfos":[],"relevantInfo":{"isRelevant":false,"relevantType":"","relevantText":""},"suggestEdit":{"reason":"","status":false,"tip":"","title":"","unnormalDetails":{"status":"","description":"","reason":"","reasonId":0,"note":""},"url":""},"isLabeled":false,"rewardInfo":{"canOpenReward":false,"isRewardable":false,"rewardMemberCount":0,"rewardTotalMoney":0,"tagline":""},"relationship":{"isAuthor":false,"isAuthorized":false,"isNothelp":false,"isThanked":false,"isRecognized":false,"voting":0,"upvotedFollowees":[]}}},"articles":{},"columns":{},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"clubs":{}},"currentUser":"","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false}},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{}},"env":{"ab":{"config":{"experiments":[{"expId":"launch-us_foltopic_user-10","expPrefix":"us_foltopic_user","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false}],"params":[{"id":"web_answer_list_ad","type":"String","value":"1"},{"id":"se_likebutton","type":"String","value":"0","chainId":"_all_"},{"id":"se_dnn_cpyramid","type":"String","value":"0","chainId":"_all_"},{"id":"se_go_ztext","type":"String","value":"0","chainId":"_all_"},{"id":"se_billboardsearch","type":"String","value":"0","chainId":"_all_"},{"id":"web_sem_ab","type":"String","value":"0"},{"id":"li_qa_new_cover","type":"String","value":"0","chainId":"_all_"},{"id":"li_ts_sample","type":"String","value":"old","chainId":"_all_"},{"id":"soc_update","type":"String","value":"1","chainId":"_all_"},{"id":"ls_fmp4","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_answer","type":"String","value":"open_cvr","chainId":"_all_"},{"id":"se_site_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"soc_special","type":"String","value":"0","chainId":"_all_"},{"id":"web_n_web_msg","type":"String","value":"0"},{"id":"li_se_paid_answer","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_xgb_model","type":"String","value":"new_xgb","chainId":"_all_"},{"id":"se_college","type":"String","value":"default","chainId":"_all_"},{"id":"se_ios_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"se_featured","type":"String","value":"1","chainId":"_all_"},{"id":"tp_topic_head","type":"String","value":"1","chainId":"_all_"},{"id":"se_backsearch","type":"String","value":"0","chainId":"_all_"},{"id":"top_recall_exp_v2","type":"String","value":"1","chainId":"_all_"},{"id":"li_qa_cover","type":"String","value":"old","chainId":"_all_"},{"id":"zw_sameq_sorce","type":"String","value":"999","chainId":"_all_"},{"id":"zr_ans_rec","type":"String","value":"gbrank","chainId":"_all_"},{"id":"top_quality","type":"String","value":"0","chainId":"_all_"},{"id":"se_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"se_ltr_gc","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sft_v2","type":"String","value":"d","chainId":"_all_"},{"id":"soc_notification","type":"String","value":"0","chainId":"_all_"},{"id":"li_hot_score_ab","type":"String","value":"0","chainId":"_all_"},{"id":"li_pay_banner_type","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_item_cf","type":"String","value":"close","chainId":"_all_"},{"id":"top_root","type":"String","value":"0","chainId":"_all_"},{"id":"se_topiclabel","type":"String","value":"1","chainId":"_all_"},{"id":"tp_sticky_android","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_metacard_top","type":"String","value":"top","chainId":"_all_"},{"id":"se_payconsult","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_lastread","type":"String","value":"0","chainId":"_all_"},{"id":"ls_new_upload","type":"String","value":"0","chainId":"_all_"},{"id":"li_android_vip","type":"String","value":"0","chainId":"_all_"},{"id":"se_zu_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"se_agency","type":"String","value":" 0","chainId":"_all_"},{"id":"se_movietab","type":"String","value":"1","chainId":"_all_"},{"id":"tp_meta_card","type":"String","value":"0","chainId":"_all_"},{"id":"tp_header_style","type":"String","value":"1","chainId":"_all_"},{"id":"tsp_hotctr","type":"String","value":"1","chainId":"_all_"},{"id":"pf_fuceng","type":"String","value":"1","chainId":"_all_"},{"id":"zr_video_recall","type":"String","value":"current_recall","chainId":"_all_"},{"id":"se_dnn_slabel","type":"String","value":"0","chainId":"_all_"},{"id":"se_p_slideshow","type":"String","value":"0","chainId":"_all_"},{"id":"zr_intervene","type":"String","value":"0","chainId":"_all_"},{"id":"se_famous","type":"String","value":"1","chainId":"_all_"},{"id":"ug_zero_follow","type":"String","value":"0","chainId":"_all_"},{"id":"se_zu_recommend","type":"String","value":"0","chainId":"_all_"},{"id":"top_hotcommerce","type":"String","value":"1","chainId":"_all_"},{"id":"soc_bignew","type":"String","value":"1","chainId":"_all_"},{"id":"pf_creator_card","type":"String","value":"1","chainId":"_all_"},{"id":"web_answer_update","type":"String","value":"0"},{"id":"zr_slot_cold_start","type":"String","value":"default","chainId":"_all_"},{"id":"se_ri","type":"String","value":"0","chainId":"_all_"},{"id":"se_webtimebox","type":"String","value":"0","chainId":"_all_"},{"id":"top_test_4_liguangyi","type":"String","value":"1","chainId":"_all_"},{"id":"ug_fw_answ_aut_1","type":"String","value":"0","chainId":"_all_"},{"id":"qa_test","type":"String","value":"0","chainId":"_all_"},{"id":"zr_video_rank","type":"String","value":"new_rank","chainId":"_all_"},{"id":"se_webmajorob","type":"String","value":"0","chainId":"_all_"},{"id":"li_search_answer","type":"String","value":"0","chainId":"_all_"},{"id":"se_limit","type":"String","value":"0","chainId":"_all_"},{"id":"se_waterfall","type":"String","value":"0","chainId":"_all_"},{"id":"ug_goodcomment_0","type":"String","value":"1","chainId":"_all_"},{"id":"se_subtext","type":"String","value":"0","chainId":"_all_"},{"id":"tp_m_intro_re_topic","type":"String","value":"1","chainId":"_all_"},{"id":"tsp_childbillboard","type":"String","value":"1","chainId":"_all_"},{"id":"zr_km_tag","type":"String","value":"open","chainId":"_all_"},{"id":"top_new_feed","type":"String","value":"5","chainId":"_all_"},{"id":"ls_videoad","type":"String","value":"2","chainId":"_all_"},{"id":"gue_new_special_page","type":"String","value":"0"},{"id":"top_v_album","type":"String","value":"1","chainId":"_all_"},{"id":"soc_bigone","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_xgb","type":"String","value":"0","chainId":"_all_"},{"id":"zr_rel_search","type":"String","value":"base","chainId":"_all_"},{"id":"se_auto_syn","type":"String","value":"0","chainId":"_all_"},{"id":"se_rr","type":"String","value":"0","chainId":"_all_"},{"id":"se_dnn_muli_task","type":"String","value":"0","chainId":"_all_"},{"id":"se_ad_index","type":"String","value":"10","chainId":"_all_"},{"id":"se_college_cm","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sft","type":"String","value":"a","chainId":"_all_"},{"id":"zr_article_rec_rank","type":"String","value":"close","chainId":"_all_"},{"id":"zr_art_rec","type":"String","value":"base","chainId":"_all_"},{"id":"zr_search_xgb","type":"String","value":"0","chainId":"_all_"},{"id":"pf_foltopic_usernum","type":"String","value":"50","chainId":"_all_"},{"id":"ug_follow_answerer","type":"String","value":"0","chainId":"_all_"},{"id":"se_mclick1","type":"String","value":"0","chainId":"_all_"},{"id":"se_lottery","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_slot_style","type":"String","value":"event_card","chainId":"_all_"},{"id":"zr_km_style","type":"String","value":"base","chainId":"_all_"},{"id":"se_col_boost","type":"String","value":"0","chainId":"_all_"},{"id":"web_column_auto_invite","type":"String","value":"0"},{"id":"ug_follow_topic_1","type":"String","value":"2","chainId":"_all_"},{"id":"se_time_threshold","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_qa","type":"String","value":"0","chainId":"_all_"},{"id":"qa_answerlist_ad","type":"String","value":"0","chainId":"_all_"},{"id":"se_mclick","type":"String","value":"0","chainId":"_all_"},{"id":"se_ltr_ck","type":"String","value":"0","chainId":"_all_"},{"id":"top_rank","type":"String","value":"0","chainId":"_all_"},{"id":"li_tjys_ec_ab","type":"String","value":"0","chainId":"_all_"},{"id":"se_topicdirect","type":"String","value":"2","chainId":"_all_"},{"id":"tp_qa_metacard","type":"String","value":"1","chainId":"_all_"},{"id":"se_expired_ob","type":"String","value":"0","chainId":"_all_"},{"id":"zr_man_intervene","type":"String","value":"0","chainId":"_all_"},{"id":"se_search_feed","type":"String","value":"N","chainId":"_all_"},{"id":"top_vipconsume","type":"String","value":"1","chainId":"_all_"},{"id":"li_back","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_album_card","type":"String","value":"0","chainId":"_all_"},{"id":"zr_infinity_small","type":"String","value":"256","chainId":"_all_"},{"id":"gue_zhuantikapian","type":"String","value":"0"},{"id":"zr_rec_answer_cp","type":"String","value":"close","chainId":"_all_"},{"id":"zr_video_rank_nn","type":"String","value":"new_rank","chainId":"_all_"},{"id":"se_wannasearch","type":"String","value":"0","chainId":"_all_"},{"id":"top_recall_deep_user","type":"String","value":"1","chainId":"_all_"},{"id":"web_heifetz_grow_ad","type":"String","value":"1"},{"id":"li_album_liutongab","type":"String","value":"0","chainId":"_all_"},{"id":"li_price_test","type":"String","value":"1","chainId":"_all_"},{"id":"pf_noti_entry_num","type":"String","value":"0","chainId":"_all_"},{"id":"ug_follow_answerer_0","type":"String","value":"0","chainId":"_all_"},{"id":"ug_newtag","type":"String","value":"0","chainId":"_all_"},{"id":"se_amovietab","type":"String","value":"1","chainId":"_all_"},{"id":"se_new_topic","type":"String","value":"0","chainId":"_all_"},{"id":"gue_anonymous","type":"String","value":"show"},{"id":"web_question_invite","type":"String","value":"B"},{"id":"se_colorfultab","type":"String","value":"1","chainId":"_all_"},{"id":"se_preset_tech","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_newchild","type":"String","value":"1","chainId":"_all_"},{"id":"tsp_vote","type":"String","value":"1","chainId":"_all_"},{"id":"pf_newguide_vertical","type":"String","value":"0","chainId":"_all_"},{"id":"top_ydyq","type":"String","value":"X","chainId":"_all_"},{"id":"zr_km_paid_answer","type":"String","value":"0","chainId":"_all_"},{"id":"se_ltr_nn","type":"String","value":"1","chainId":"_all_"},{"id":"li_se_kv","type":"String","value":"0","chainId":"_all_"},{"id":"zr_answer_rec_cp","type":"String","value":"open","chainId":"_all_"},{"id":"top_recall_exp_v1","type":"String","value":"1","chainId":"_all_"},{"id":"top_universalebook","type":"String","value":"1","chainId":"_all_"},{"id":"top_native_answer","type":"String","value":"1","chainId":"_all_"},{"id":"top_gr_ab","type":"String","value":"0","chainId":"_all_"},{"id":"web_answerlist_ad","type":"String","value":"0"},{"id":"se_whitelist","type":"String","value":"0","chainId":"_all_"},{"id":"se_mobileweb","type":"String","value":"1","chainId":"_all_"},{"id":"tp_qa_toast","type":"String","value":"1","chainId":"_all_"},{"id":"top_ebook","type":"String","value":"0","chainId":"_all_"},{"id":"ug_zero_follow_0","type":"String","value":"0","chainId":"_all_"},{"id":"zr_test_aa1","type":"String","value":"0","chainId":"_all_"},{"id":"se_webrs","type":"String","value":"1","chainId":"_all_"},{"id":"se_websearch","type":"String","value":"3","chainId":"_all_"}],"chains":[{"chainId":"_all_"}]},"triggers":{}},"userAgent":{"Edge":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":false,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"isWebView":false,"origin":"Mozilla\u002F5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F76.0.3809.100 Safari\u002F537.36"},"ctx":{"path":"\u002Fquestion\u002F38121173"},"trafficSource":"production","edition":{"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false},"theme":"light","enableShortcut":true,"referer":"https:\u002F\u002Fai100-2.cupoy.com\u002Fmission\u002FD39","conf":{},"ipInfo":{},"logged":false,"tdkInfo":{}},"me":{"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"active":{"sendDigitsError":null,"activeConfirmSucceeded":null,"activeConfirmError":null},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false,"captchaBase64String":null,"captchaValidationMessage":null,"loginCaptchaExpires":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"currentCreatorUrlToken":null,"homeData":{"recommendQuestions":[]},"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0},"goodatTopics":[]},"customPromotion":{"itemLists":{}},"recommend":{"recommendTimes":{}}},"explore":{"academy":{"tabs":[],"article":{}}},"rights":[],"rightsStatus":{},"levelUpperLimit":10,"account":{"growthLevel":{}},"applyStatus":{}},"question":{"followers":{},"concernedFollowers":{},"answers":{"38121173":{"isFetching":false,"isDrained":false,"ids":[403986652,75158776,166238142,85813729,104955389],"newIds":[403986652,75158776,166238142,85813729,104955389],"totals":23,"isPrevDrained":true,"previous":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F38121173\u002Fanswers?include=data%5B%2A%5D.is_normal%2Cadmin_closed_comment%2Creward_info%2Cis_collapsed%2Cannotation_action%2Cannotation_detail%2Ccollapse_reason%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Ccreated_time%2Cupdated_time%2Creview_info%2Crelevant_info%2Cquestion%2Cexcerpt%2Crelationship.is_authorized%2Cis_author%2Cvoting%2Cis_thanked%2Cis_nothelp%2Cis_labeled%2Cis_recognized%2Cpaid_info%2Cpaid_info_content%3Bdata%5B%2A%5D.mark_infos%5B%2A%5D.url%3Bdata%5B%2A%5D.author.follower_count%2Cbadge%5B%2A%5D.topics&limit=5&offset=0&platform=desktop&sort_by=default","next":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F38121173\u002Fanswers?include=data%5B%2A%5D.is_normal%2Cadmin_closed_comment%2Creward_info%2Cis_collapsed%2Cannotation_action%2Cannotation_detail%2Ccollapse_reason%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Ccreated_time%2Cupdated_time%2Creview_info%2Crelevant_info%2Cquestion%2Cexcerpt%2Crelationship.is_authorized%2Cis_author%2Cvoting%2Cis_thanked%2Cis_nothelp%2Cis_labeled%2Cis_recognized%2Cpaid_info%2Cpaid_info_content%3Bdata%5B%2A%5D.mark_infos%5B%2A%5D.url%3Bdata%5B%2A%5D.author.follower_count%2Cbadge%5B%2A%5D.topics&limit=5&offset=5&platform=desktop&sort_by=default"}},"hiddenAnswers":{},"updatedAnswers":{},"collapsedAnswers":{},"notificationAnswers":{},"invitedQuestions":{"total":{"count":null,"isEnd":false,"isLoading":false,"questions":[]},"followees":{"count":null,"isEnd":false,"isLoading":false,"questions":[]}},"laterQuestions":{"count":null,"globalWriteAnimate":false,"isEnd":false,"isLoading":false,"questions":[]},"waitingQuestions":{"hot":{"isEnd":false,"isLoading":false,"questions":[]},"value":{"isEnd":false,"isLoading":false,"questions":[]},"newest":{"isEnd":false,"isLoading":false,"questions":[]},"easy":{"isEnd":false,"isLoading":false,"questions":[]}},"invitationCandidates":{},"inviters":{},"invitees":{},"similarQuestions":{},"relatedCommodities":{},"recommendReadings":{},"bio":{},"brand":{},"permission":{},"adverts":{"3":{"ad":{"adVerb":"","brand":{"id":0,"logo":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-b02efb32af2aada63f09e5d0a275df75_250x250.jpeg","name":"Togocareer"},"category":1,"clickTracks":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?nt=0&ed=CjEEfh4wM313HGRQCGItUUZxX3hebm5xK0B_XlNodR1dIQgsCiVueHlDZV4XMTYNWXYPbFkqPn14FWtUA2RqQAJ4CHoIem15aEY7WgdidAdYdx8pTH5qZi1Kb1QAYWpSG3gKewllNjVzFXdMNhNqhIWG&tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805&pf=4&ts=1566402237&ar=0.0000004253745926961948&ut=a9a0b520e346491fb5be9950c12fc2ae&pdi=1523617890848616&ui=1.171.46.33&idi=2006&au=4280"],"closeTrack":"https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?au=4280&pf=4&ed=CjEEfR4wM313HGRQCGItUUZxX3hebm5xK0B_XlNodR1dIQgsCiVueHlDZV4XMTYNWXYPbFkqPn14FWtUA2RqQAJ4CHoIem15aEY7WgdidAdYdx8pTH5qZi1Kb1QAYWpSG3gKewllNjVzFWWpvLFu_vd8&idi=2006&ui=1.171.46.33&ut=a9a0b520e346491fb5be9950c12fc2ae&ts=1566402237&nt=0&ar=0.0000004253745926961948&tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805&pdi=1523617890848616","closeTracks":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?au=4280&pf=4&ed=CjEEfR4wM313HGRQCGItUUZxX3hebm5xK0B_XlNodR1dIQgsCiVueHlDZV4XMTYNWXYPbFkqPn14FWtUA2RqQAJ4CHoIem15aEY7WgdidAdYdx8pTH5qZi1Kb1QAYWpSG3gKewllNjVzFWWpvLFu_vd8&idi=2006&ui=1.171.46.33&ut=a9a0b520e346491fb5be9950c12fc2ae&ts=1566402237&nt=0&ar=0.0000004253745926961948&tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805&pdi=1523617890848616"],"conversionTracks":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?ar=0.0000004253745926961948&pdi=1523617890848616&ed=CjEEfx4wM313HGRQCGItUUZxX3hebm5xK0B_XlNodR1dIQgsCiVueHlDZV4XMTYNWXYPbFkqPn14FWtUA2RqQAJ4CHoIem15aEY7WgdidAdYdx8pTH5qZi1Kb1QAYWpSG3gKewllNjVzFUAXekwVnhnC&ts=1566402237&nt=0&pf=4&idi=2006&au=4280&ui=1.171.46.33&tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805&ut=a9a0b520e346491fb5be9950c12fc2ae"],"creatives":[{"appPromotionUrl":"","brand":{"id":0,"logo":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-b02efb32af2aada63f09e5d0a275df75_250x250.jpeg","name":"Togocareer"},"cta":{"value":"查看详情"},"description":"留学生回国，在国内的发展情况如何？2019年校招、社招信息、500强名企内推名额预约，实习，求职更快捷！","footer":{"value":""},"landingUrl":"https:\u002F\u002Fwww.togocareer.com\u002Fservices.html?tgcChannel==zhihu&tuwen0805","title":"对于留学生来说，回国求职，如何进入名企工作？实习机会怎么找？"}],"debugTracks":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?pf=4&ts=1566402237&idi=2006&ut=a9a0b520e346491fb5be9950c12fc2ae&au=4280&ui=1.171.46.33&ed=CjEEcx4wM313HGRQCGItUUZxX3hebm5xK0B_XlNodR1dIQgsCiVueHlDZV4XMTYNWXYPbFkqPn14FWtUA2RqQAJ4CHoIem15aEY7WgdidAdYdx8pTH5qZi1Kb1QAYWpSG3gKewllNjVzFbXF_RKDTDZ2&pdi=1523617890848616&ar=0.0000004253745926961948&nt=0&tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805"],"displayAdvertisingTag":true,"experimentInfo":"{\"ad_card_close_hotspot\":{\"close_area\":\"old\"},\"ad_follow\":{\"is_new_page\":0},\"canvas_form\":{\"new_privacy\":\"1\"},\"fe\":{\"fp_click\":0,\"fp_desc_isshow\":0,\"fp_video_isactive\":0,\"fp_video_isplay\":0,\"fp_video_style\":\"nil\",\"fp_word_isactive\":0},\"feed_new_page\":{\"new_page\":\"none\"},\"l_a_p\":{\"p_as_prefetch\":\"0\"},\"l_ad_30\":{\"new_30\":\"0\"},\"l_ad_314_d\":{\"p_ad_314_d\":5},\"l_c_color\":{\"p_c_color\":\"gray\"},\"l_c_st\":{\"p_c_st\":\"0\"},\"l_c_st_31\":{\"p_c_st_31\":\"none\"},\"l_c_st_bi_30\":{\"p_c_st_bi_30\":\"0\"},\"l_c_st_pw_30\":{\"p_c_st_pw_30\":\"0\"},\"l_c_st_pw_8_r\":{\"p_c_st_pw_8_r\":\"0\"},\"l_c_st_v_30\":{\"p_c_st_v_30\":\"0\"},\"l_c_style\":{\"p_c_style\":\"1\"},\"l_creative_st_bi_8\":{\"p_creative_st_bi_8\":\"0\"},\"l_creative_st_v_8\":{\"p_creative_st_v_8\":\"0\"},\"l_f_h\":{\"p_f_h\":\"0\"},\"l_m_bfa\":{\"p_m_bfa\":\"a\"},\"l_m_cf_2\":{\"p_m_cf_2\":\"0\"},\"l_m_cf_3\":{\"p_m_cf_3\":\"0\"},\"l_m_cf_4\":{\"p_m_cf_4\":\"0\"},\"l_m_cf_5\":{\"p_m_cf_5\":\"0\"},\"l_m_cv_3\":{\"p_m_cv_3\":\"0\"},\"l_m_cv_5\":{\"p_m_cv_5\":\"0\"},\"l_native_ad\":{\"label\":\"old\"},\"lps_switch\":{\"p_lps_switch\":0,\"p_lps_switch_android\":0},\"lps_switch2\":{\"p_lps_switch2\":\"0\"},\"morph_switch\":{\"p_morph_switch\":\"on\"}}","id":609324,"impressionTracks":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805&pf=4&nt=0&au=4280&ts=1566402237&ut=a9a0b520e346491fb5be9950c12fc2ae&pdi=1523617890848616&ar=0.0000004253745926961948&ui=1.171.46.33&idi=2006&ed=CjEEeB4wM313HGRQCGItUUZxX3hebm5xK0B_XlNodR1dIQgsCiVueHlDZV4XMTYNWXYPbFkqPn14FWtUA2RqQAJ4CHoIem15aEY7WgdidAdYdx8pTH5qZi1Kb1QAYWpSG3gKewllNjVzFR_KjgCW5F55"],"isEvergreen":false,"isNewWebview":true,"isSpeeding":false,"isWebp":false,"landPrefetch":false,"name":"","nativePrefetch":true,"partyId":-2,"revertCloseTrack":"https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?pdi=1523617890848616&nt=0&au=4280&ar=0.0000004253745926961948&ui=1.171.46.33&idi=2006&ut=a9a0b520e346491fb5be9950c12fc2ae&pf=4&ts=1566402237&ed=CjEEch4wM313HGRQCGItUUZxX3hebm5xK0B_XlNodR1dIQgsCiVueHlDZV4XMTYNWXYPbFkqPn14FWtUA2RqQAJ4CHoIem15aEY7WgdidAdYdx8pTH5qZi1Kb1QAYWpSG3gKewllNjVzFZdhnCXEvzc2&tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805","template":"web_word","viewTrack":"https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?ts=1566402237&ar=0.0000004253745926961948&ut=a9a0b520e346491fb5be9950c12fc2ae&ed=CjEEeR4wM313HGRQCGItUUZxX3hebm5xK0B_XlNodR1dIQgsCiVueHlDZV4XMTYNWXYPbFkqPn14FWtUA2RqQAJ4CHoIem15aEY7WgdidAdYdx8pTH5qZi1Kb1QAYWpSG3gKewllNjVzFVfMOyNe6Adf&nt=0&pf=4&pdi=1523617890848616&idi=2006&au=4280&tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805&ui=1.171.46.33","viewTracks":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?ts=1566402237&ar=0.0000004253745926961948&ut=a9a0b520e346491fb5be9950c12fc2ae&ed=CjEEeR4wM313HGRQCGItUUZxX3hebm5xK0B_XlNodR1dIQgsCiVueHlDZV4XMTYNWXYPbFkqPn14FWtUA2RqQAJ4CHoIem15aEY7WgdidAdYdx8pTH5qZi1Kb1QAYWpSG3gKewllNjVzFVfMOyNe6Adf&nt=0&pf=4&pdi=1523617890848616&idi=2006&au=4280&tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805&ui=1.171.46.33"],"zaAdInfo":"CKyYJRDsASIBMV3NurpOYPyvJg==","zaAdInfoJson":"{\"ad_id\":609324,\"ad_zone_id\":236,\"category\":\"1\",\"timestamp\":1566402200,\"creative_id\":628732}"},"adjson":"{\"ads\":[{\"id\":609324,\"ad_zone_id\":236,\"template\":\"web_word\",\"impression_tracks\":[\"https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?ed=CjEEeB4wM313HGRQCGItUUZxX3hebm5xK0B_XlNodR1dIQgsCiVueHlDZV4XMTYNWXYPbFkqPn14FWtUA2RqQAJ4CHoIem15aEY7WgdidAdYdx8pTH5qZi1Kb1QAYWpSG3gKewllNjVzFR_KjgCW5F55\\u0026pdi=1523617890848616\\u0026ts=1566402237\\u0026idi=2006\\u0026pf=4\\u0026au=4280\\u0026nt=0\\u0026ut=a9a0b520e346491fb5be9950c12fc2ae\\u0026ui=1.171.46.33\\u0026tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805\\u0026ar=0.0000004253745926961948\"],\"view_tracks\":[\"https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?idi=2006\\u0026ed=CjEEeR4wM313HGRQCGItUUZxX3hebm5xK0B_XlNodR1dIQgsCiVueHlDZV4XMTYNWXYPbFkqPn14FWtUA2RqQAJ4CHoIem15aEY7WgdidAdYdx8pTH5qZi1Kb1QAYWpSG3gKewllNjVzFVfMOyNe6Adf\\u0026tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805\\u0026pf=4\\u0026ui=1.171.46.33\\u0026ut=a9a0b520e346491fb5be9950c12fc2ae\\u0026nt=0\\u0026ts=1566402237\\u0026ar=0.0000004253745926961948\\u0026pdi=1523617890848616\\u0026au=4280\"],\"click_tracks\":[\"https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?idi=2006\\u0026au=4280\\u0026ut=a9a0b520e346491fb5be9950c12fc2ae\\u0026ts=1566402237\\u0026ar=0.0000004253745926961948\\u0026pdi=1523617890848616\\u0026nt=0\\u0026pf=4\\u0026tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805\\u0026ui=1.171.46.33\\u0026ed=CjEEfh4wM313HGRQCGItUUZxX3hebm5xK0B_XlNodR1dIQgsCiVueHlDZV4XMTYNWXYPbFkqPn14FWtUA2RqQAJ4CHoIem15aEY7WgdidAdYdx8pTH5qZi1Kb1QAYWpSG3gKewllNjVzFXdMNhNqhIWG\"],\"close_tracks\":[\"https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?idi=2006\\u0026ut=a9a0b520e346491fb5be9950c12fc2ae\\u0026tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805\\u0026au=4280\\u0026nt=0\\u0026pf=4\\u0026ts=1566402237\\u0026pdi=1523617890848616\\u0026ed=CjEEfR4wM313HGRQCGItUUZxX3hebm5xK0B_XlNodR1dIQgsCiVueHlDZV4XMTYNWXYPbFkqPn14FWtUA2RqQAJ4CHoIem15aEY7WgdidAdYdx8pTH5qZi1Kb1QAYWpSG3gKewllNjVzFWWpvLFu_vd8\\u0026ui=1.171.46.33\\u0026ar=0.0000004253745926961948\"],\"debug_tracks\":[\"https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?ut=a9a0b520e346491fb5be9950c12fc2ae\\u0026ts=1566402237\\u0026au=4280\\u0026pf=4\\u0026ed=CjEEcx4wM313HGRQCGItUUZxX3hebm5xK0B_XlNodR1dIQgsCiVueHlDZV4XMTYNWXYPbFkqPn14FWtUA2RqQAJ4CHoIem15aEY7WgdidAdYdx8pTH5qZi1Kb1QAYWpSG3gKewllNjVzFbXF_RKDTDZ2\\u0026idi=2006\\u0026ui=1.171.46.33\\u0026ar=0.0000004253745926961948\\u0026pdi=1523617890848616\\u0026nt=0\\u0026tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805\"],\"conversion_tracks\":[\"https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?ed=CjEEfx4wM313HGRQCGItUUZxX3hebm5xK0B_XlNodR1dIQgsCiVueHlDZV4XMTYNWXYPbFkqPn14FWtUA2RqQAJ4CHoIem15aEY7WgdidAdYdx8pTH5qZi1Kb1QAYWpSG3gKewllNjVzFUAXekwVnhnC\\u0026tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805\\u0026au=4280\\u0026idi=2006\\u0026pdi=1523617890848616\\u0026nt=0\\u0026pf=4\\u0026ts=1566402237\\u0026ut=a9a0b520e346491fb5be9950c12fc2ae\\u0026ui=1.171.46.33\\u0026ar=0.0000004253745926961948\"],\"za_ad_info\":\"CKyYJRDsASIBMV3NurpOYPyvJg==\",\"za_ad_info_json\":\"{\\\"ad_id\\\":609324,\\\"ad_zone_id\\\":236,\\\"category\\\":\\\"1\\\",\\\"timestamp\\\":1566402200,\\\"creative_id\\\":628732}\",\"creatives\":[{\"id\":628732,\"asset\":{\"brand_name\":\"Togocareer\",\"brand_logo\":\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-b02efb32af2aada63f09e5d0a275df75_250x250.jpeg\",\"title\":\"对于留学生来说，回国求职，如何进入名企工作？实习机会怎么找？\",\"desc\":\"留学生回国，在国内的发展情况如何？2019年校招、社招信息、500强名企内推名额预约，实习，求职更快捷！\",\"landing_url\":\"https:\u002F\u002Fwww.togocareer.com\u002Fservices.html?tgcChannel==zhihu\\u0026tuwen0805\",\"img_size\":0,\"cta\":\"查看详情\"}}],\"expand\":{\"display_advertising_tag\":true,\"is_new_webview\":true,\"is_cdn_speeding\":false},\"experiment_info\":\"{\\\"ad_card_close_hotspot\\\":{\\\"close_area\\\":\\\"old\\\"},\\\"ad_follow\\\":{\\\"is_new_page\\\":0},\\\"canvas_form\\\":{\\\"new_privacy\\\":\\\"1\\\"},\\\"fe\\\":{\\\"fp_click\\\":0,\\\"fp_desc_isshow\\\":0,\\\"fp_video_isactive\\\":0,\\\"fp_video_isplay\\\":0,\\\"fp_video_style\\\":\\\"nil\\\",\\\"fp_word_isactive\\\":0},\\\"feed_new_page\\\":{\\\"new_page\\\":\\\"none\\\"},\\\"l_a_p\\\":{\\\"p_as_prefetch\\\":\\\"0\\\"},\\\"l_ad_30\\\":{\\\"new_30\\\":\\\"0\\\"},\\\"l_ad_314_d\\\":{\\\"p_ad_314_d\\\":5},\\\"l_c_color\\\":{\\\"p_c_color\\\":\\\"gray\\\"},\\\"l_c_st\\\":{\\\"p_c_st\\\":\\\"0\\\"},\\\"l_c_st_31\\\":{\\\"p_c_st_31\\\":\\\"none\\\"},\\\"l_c_st_bi_30\\\":{\\\"p_c_st_bi_30\\\":\\\"0\\\"},\\\"l_c_st_pw_30\\\":{\\\"p_c_st_pw_30\\\":\\\"0\\\"},\\\"l_c_st_pw_8_r\\\":{\\\"p_c_st_pw_8_r\\\":\\\"0\\\"},\\\"l_c_st_v_30\\\":{\\\"p_c_st_v_30\\\":\\\"0\\\"},\\\"l_c_style\\\":{\\\"p_c_style\\\":\\\"1\\\"},\\\"l_creative_st_bi_8\\\":{\\\"p_creative_st_bi_8\\\":\\\"0\\\"},\\\"l_creative_st_v_8\\\":{\\\"p_creative_st_v_8\\\":\\\"0\\\"},\\\"l_f_h\\\":{\\\"p_f_h\\\":\\\"0\\\"},\\\"l_m_bfa\\\":{\\\"p_m_bfa\\\":\\\"a\\\"},\\\"l_m_cf_2\\\":{\\\"p_m_cf_2\\\":\\\"0\\\"},\\\"l_m_cf_3\\\":{\\\"p_m_cf_3\\\":\\\"0\\\"},\\\"l_m_cf_4\\\":{\\\"p_m_cf_4\\\":\\\"0\\\"},\\\"l_m_cf_5\\\":{\\\"p_m_cf_5\\\":\\\"0\\\"},\\\"l_m_cv_3\\\":{\\\"p_m_cv_3\\\":\\\"0\\\"},\\\"l_m_cv_5\\\":{\\\"p_m_cv_5\\\":\\\"0\\\"},\\\"l_native_ad\\\":{\\\"label\\\":\\\"old\\\"},\\\"lps_switch\\\":{\\\"p_lps_switch\\\":0,\\\"p_lps_switch_android\\\":0},\\\"lps_switch2\\\":{\\\"p_lps_switch2\\\":\\\"0\\\"},\\\"morph_switch\\\":{\\\"p_morph_switch\\\":\\\"on\\\"}}\",\"view_x_tracks\":[\"https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?pdi=1523617890848616\\u0026nt=0\\u0026ts=1566402237\\u0026ui=1.171.46.33\\u0026idi=2006\\u0026tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805\\u0026ut=a9a0b520e346491fb5be9950c12fc2ae\\u0026pf=4\\u0026ed=CjEEewhlKSlzHGtRBml-UQpoDSwKJXd0f0A3SggydAlGc117XnE8dHYSNFAIdi1KVncKfB4iMyRzE2JeAmJ4FhssBHsIc2N3dwMxDgxmfghcdgtsWzdncGhGPVoCYX0WCTUEeQlyfCw7GGLfwQkz6G5W5A%3D%3D\\u0026ar=0.0000004253745926961948\\u0026au=4280\"]}]}"}},"advancedStyle":{},"commonAnswerCount":0,"hiddenAnswerCount":0,"meta":{},"autoInvitation":{},"simpleConcernedFollowers":{},"draftStatus":{},"disclaimers":{}},"shareTexts":{},"answers":{"voters":{},"copyrightApplicants":{},"favlists":{},"newAnswer":{},"concernedUpvoters":{},"simpleConcernedUpvoters":{},"paidContent":{},"settings":{}},"banner":{},"topic":{"bios":{},"hot":{},"newest":{},"top":{},"unanswered":{},"questions":{},"followers":{},"contributors":{},"parent":{},"children":{},"bestAnswerers":{},"wikiMeta":{},"index":{},"intro":{},"meta":{},"schema":{},"creatorWall":{},"wikiEditInfo":{},"committedWiki":{}},"explore":{"recommendations":{},"specials":{},"roundtables":{},"collections":{},"columns":{}},"articles":{"voters":{}},"favlists":{"relations":{}},"pins":{"voters":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotListCategories":[],"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]}},"upload":{},"video":{"data":{},"shareVideoDetail":{},"last":{}},"guide":{"guide":{"isFetching":false,"isShowGuide":false}},"reward":{"answer":{},"article":{},"question":{}},"search":{"recommendSearch":[],"topSearch":{},"searchValue":{},"suggestSearch":{},"attachedInfo":{},"nextOffset":{},"topicReview":{},"generalByQuery":{},"generalByQueryInADay":{},"generalByQueryInAWeek":{},"generalByQueryInThreeMonths":{},"peopleByQuery":{},"topicByQuery":{},"columnByQuery":{},"liveByQuery":{},"albumByQuery":{},"eBookByQuery":{}},"publicEditPermission":{},"readStatus":{},"draftHistory":{"history":{},"drafts":{}},"notifications":{"recent":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"history":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"notificationActors":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"recentNotificationEntry":"all"},"specials":{"entities":{},"all":{"data":[],"paging":{},"isLoading":false}},"collections":{"hot":{"data":[],"paging":{},"isLoading":false}}},"subAppName":"main"}</script><script src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/vendor.7842b402f56b92d57f3e.js.下載"></script><script src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/main.app.7c8634e8d9de8fd5d961.js.下載"></script><script src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/main.question-routes.cd051ae1fd8b5a422985.js.下載"></script><script src="./Linear least squares, Lasso,ridge regression有何本质区别？ - 知乎_files/zap.js.下載"></script><div><div style="display: none;">想来知乎工作？请发送邮件到 jobs@zhihu.com</div></div></body></html>