<!DOCTYPE html>
<!-- saved from url=(0039)https://www.zhihu.com/question/61607442 -->
<html lang="zh" data-hairline="true" data-theme="light"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>怎么选取训练神经网络时的Batch size? - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="renderer" content="webkit"><meta name="force-rendering" content="webkit"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"><meta data-react-helmet="true" name="description" property="og:description" content="batch_size的选择和训练数据规模、神经网络层数、单元数有什么关系？谢谢！"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png" sizes="152x152"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.b3e6278d.png" sizes="120x120"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7a750095.png" sizes="76x76"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.a4a761d4.png" sizes="60x60"><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/static/search.xml" title="知乎"><link rel="dns-prefetch" href="https://static.zhimg.com/"><link rel="dns-prefetch" href="https://pic1.zhimg.com/"><link rel="dns-prefetch" href="https://pic2.zhimg.com/"><link rel="dns-prefetch" href="https://pic3.zhimg.com/"><link rel="dns-prefetch" href="https://pic4.zhimg.com/"><style>
.u-safeAreaInset-top {
  height: constant(safe-area-inset-top) !important;
  height: env(safe-area-inset-top) !important;
  
}
.u-safeAreaInset-bottom {
  height: constant(safe-area-inset-bottom) !important;
  height: env(safe-area-inset-bottom) !important;
  
}
</style><link href="./怎么选取训练神经网络时的Batch size_ - 知乎_files/app.8e85635046f207102080.css" rel="stylesheet"><link href="./怎么选取训练神经网络时的Batch size_ - 知乎_files/question-routes.042095ae5a957a9ac73c.css" rel="stylesheet"><script defer="" crossorigin="anonymous" src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/init.js.下載" data-sentry-config="{&quot;dsn&quot;:&quot;https://65e244586890460588f00f2987137aa8@crash2.zhihu.com/193&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;2250-11349395&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrors&quot;:[&quot;origin message&quot;,&quot;Network request failed&quot;,&quot;Loading chunk&quot;,&quot;这个系统不支持该功能。&quot;,&quot;Can&#39;t find variable: webkit&quot;,&quot;Can&#39;t find variable: $&quot;,&quot;内存不足&quot;,&quot;out of memory&quot;,&quot;DOM Exception 18&quot;,&quot;The operation is insecure&quot;,&quot;[object Event]&quot;,&quot;[object FileError]&quot;,&quot;[object DOMError]&quot;,&quot;[object Object]&quot;,&quot;拒绝访问。&quot;,&quot;Maximum call stack size exceeded&quot;,&quot;UploadError&quot;,&quot;无法 fetch&quot;,&quot;draft-js&quot;,&quot;缺少 JavaScript 对象&quot;,&quot;componentWillEnter&quot;,&quot;componentWillLeave&quot;,&quot;componentWillAppear&quot;,&quot;getInlineStyleAt&quot;,&quot;getCharacterList&quot;],&quot;whitelistUrls&quot;:[&quot;static.zhihu.com&quot;]}"></script><link rel="stylesheet" type="text/css" href="./怎么选取训练神经网络时的Batch size_ - 知乎_files/richinput.6c6fcb145ace96c2ea0c.css"><script charset="utf-8" src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/main.richinput.f29007bc75519b296be3.js.下載"></script><link rel="stylesheet" type="text/css" href="./怎么选取训练神经网络时的Batch size_ - 知乎_files/modals.06b0b2abf7f99561b746.css"><script charset="utf-8" src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/main.modals.7cc66fee260fa5275fe4.js.下載"></script><link rel="stylesheet" type="text/css" href="./怎么选取训练神经网络时的Batch size_ - 知乎_files/signflow.a9fd05c5b833fa59ed49.css"><script charset="utf-8" src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/main.signflow.2776198f0c51deaad605.js.下載"></script></head><body class="Entry-body"><div id="root"><div><div class="LoadingBar"></div><div class="AdblockBanner"><div class="AdblockBanner-inner">我们检测到你可能使用了 AdBlock 或 Adblock Plus，它的部分策略可能会影响到正常功能的使用（如关注）。<br>你可以设定特殊规则或将知乎加入白名单，以便我们更好地提供服务。 （<a href="https://www.zhihu.com/question/54919485" target="_blank">为什么？</a>）</div><button type="button" class="Button AdblockBanner-close Button--plain"><svg viewBox="0 0 14 14" class="Icon Icon--remove" width="16" height="16" aria-hidden="true" style="height: 16px; width: 16px;"><title></title><g><path d="M8.486 7l5.208-5.207c.408-.408.405-1.072-.006-1.483-.413-.413-1.074-.413-1.482-.005L7 5.515 1.793.304C1.385-.103.72-.1.31.31-.103.724-.103 1.385.305 1.793L5.515 7l-5.21 5.207c-.407.408-.404 1.072.007 1.483.413.413 1.074.413 1.482.005L7 8.485l5.207 5.21c.408.407 1.072.404 1.483-.007.413-.413.413-1.074.005-1.482L8.485 7z"></path></g></svg></button></div><div><header role="banner" class="Sticky AppHeader is-hidden is-fixed" data-za-module="TopNavBar" style="width: 1903px; top: 0px; left: 0px;"><div class="AppHeader-inner"><a href="https://www.zhihu.com/" aria-label="知乎"><svg viewBox="0 0 200 91" class="Icon ZhihuLogo ZhihuLogo--blue Icon--logo" width="64" height="30" aria-hidden="true" style="height: 30px; width: 64px;"><title></title><g><path d="M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z" fill-rule="evenodd"></path></g></svg></a><ul role="navigation" class="Tabs AppHeader-Tabs"><li role="tab" class="Tabs-item AppHeader-Tab Tabs-item--noMeta"><a class="Tabs-link AppHeader-TabsLink" href="https://www.zhihu.com/" data-za-not-track-link="true">首页</a></li><li role="tab" class="Tabs-item AppHeader-Tab Tabs-item--noMeta"><a class="Tabs-link AppHeader-TabsLink" href="https://www.zhihu.com/explore" data-za-not-track-link="true">发现</a></li><li role="tab" class="Tabs-item AppHeader-Tab Tabs-item--noMeta"><a class="Tabs-link AppHeader-TabsLink" href="https://www.zhihu.com/question/waiting" data-za-not-track-link="true">等你来答</a></li></ul><div class="SearchBar" role="search" data-za-module="PresetWordItem"><div class="SearchBar-toolWrapper"><form class="SearchBar-tool"><div><div class="Popover"><div class="SearchBar-input Input-wrapper Input-wrapper--grey"><input type="text" maxlength="100" autocomplete="off" role="combobox" aria-expanded="false" aria-autocomplete="list" aria-activedescendant="AutoComplete3--1" id="Popover2-toggle" aria-haspopup="true" aria-owns="Popover2-content" class="Input" placeholder="《蜘蛛侠》将退出漫威宇宙" value=""><div class="Input-after"><button aria-label="搜索" type="button" class="Button SearchBar-searchIcon Button--primary"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Search" fill="currentColor" viewBox="0 0 24 24" width="18" height="18"><path d="M17.068 15.58a8.377 8.377 0 0 0 1.774-5.159 8.421 8.421 0 1 0-8.42 8.421 8.38 8.38 0 0 0 5.158-1.774l3.879 3.88c.957.573 2.131-.464 1.488-1.49l-3.879-3.878zm-6.647 1.157a6.323 6.323 0 0 1-6.316-6.316 6.323 6.323 0 0 1 6.316-6.316 6.323 6.323 0 0 1 6.316 6.316 6.323 6.323 0 0 1-6.316 6.316z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></form></div></div><div class="AppHeader-userInfo"><div class="AppHeader-profile"><div><button type="button" class="Button AppHeader-login Button--blue">登录</button><button type="button" class="Button Button--primary Button--blue">加入知乎</button></div></div></div></div><div><div class="PageHeader is-shown"><div class="QuestionHeader-content"><div class="QuestionHeader-main"><h1 class="QuestionHeader-title">怎么选取训练神经网络时的Batch size?</h1></div><div class="QuestionHeader-side" data-za-detail-view-path-module="ToolBar" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;61607442&quot;}}}"><div class="QuestionButtonGroup"><button type="button" class="Button FollowButton Button--primary Button--blue">关注问题</button><button type="button" class="Button Button--blue"><svg viewBox="0 0 12 12" class="Icon QuestionButton-icon Button-icon Icon--modify" width="14" height="16" aria-hidden="true" style="height: 16px; width: 14px;"><title></title><g><path d="M.423 10.32L0 12l1.667-.474 1.55-.44-2.4-2.33-.394 1.564zM10.153.233c-.327-.318-.85-.31-1.17.018l-.793.817 2.49 2.414.792-.814c.318-.328.312-.852-.017-1.17l-1.3-1.263zM3.84 10.536L1.35 8.122l6.265-6.46 2.49 2.414-6.265 6.46z" fill-rule="evenodd"></path></g></svg>写回答</button></div></div></div></div></div></header><div class="Sticky--holder" style="position: relative; top: 0px; right: 0px; bottom: 0px; left: 0px; display: block; float: none; margin: 0px; height: 52px;"></div></div><main role="main" class="App-main"><div class="QuestionPage" itemscope="" itemtype="http://schema.org/Question" data-za-detail-view-path-module="QuestionItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;61607442&quot;}}}"><meta itemprop="name" content="怎么选取训练神经网络时的Batch size?"><meta itemprop="url" content="https://www.zhihu.com/question/61607442"><meta itemprop="keywords" content="神经网络,深度学习（Deep Learning）,TensorFlow"><meta itemprop="answerCount" content="13"><meta itemprop="commentCount" content="0"><meta itemprop="dateCreated" content="2017-06-26T00:24:04.000Z"><meta itemprop="dateModified" content="2017-07-27T07:49:58.000Z"><meta itemprop="zhihu:visitsCount"><meta itemprop="zhihu:followerCount" content="978"><div data-zop-question="{&quot;title&quot;:&quot;怎么选取训练神经网络时的Batch size?&quot;,&quot;topics&quot;:[{&quot;name&quot;:&quot;神经网络&quot;,&quot;id&quot;:&quot;19607065&quot;},{&quot;name&quot;:&quot;深度学习（Deep Learning）&quot;,&quot;id&quot;:&quot;19813032&quot;},{&quot;name&quot;:&quot;TensorFlow&quot;,&quot;id&quot;:&quot;20032249&quot;}],&quot;id&quot;:61607442,&quot;isEditable&quot;:false}"><div class="QuestionStatus"></div><div class="QuestionHeader" data-za-detail-view-path-module="QuestionDescription" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;61607442&quot;}}}"><div class="QuestionHeader-content"><div class="QuestionHeader-main"><div class="QuestionHeader-tags"><div class="QuestionHeader-topics"><div class="Tag QuestionTopic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19607065&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/19607065" target="_blank"><div class="Popover"><div id="Popover4-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover4-content">神经网络</div></div></a></span></div><div class="Tag QuestionTopic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19813032&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/19813032" target="_blank"><div class="Popover"><div id="Popover5-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover5-content">深度学习（Deep Learning）</div></div></a></span></div><div class="Tag QuestionTopic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;20032249&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/20032249" target="_blank"><div class="Popover"><div id="Popover6-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover6-content">TensorFlow</div></div></a></span></div></div></div><h1 class="QuestionHeader-title">怎么选取训练神经网络时的Batch size?</h1><div><div class="QuestionHeader-detail"><div class="QuestionRichText QuestionRichText--collapsed"><div><span class="RichText ztext" itemprop="text">batch_size的选择和训练数据规模、神经网络层数、单元数有什么关系？谢谢！</span></div></div></div></div></div><div class="QuestionHeader-side"><div class="QuestionHeader-follow-status"><div class="QuestionFollowStatus"><div class="NumberBoard QuestionFollowStatus-counts NumberBoard--divider"><div class="NumberBoard-item"><div class="NumberBoard-itemInner"><div class="NumberBoard-itemName">关注者</div><strong class="NumberBoard-itemValue" title="978">978</strong></div></div><div class="NumberBoard-item"><div class="NumberBoard-itemInner"><div class="NumberBoard-itemName">被浏览</div><strong class="NumberBoard-itemValue" title="86829">86,829</strong></div></div></div></div></div></div></div><div class="QuestionHeader-footer"><div class="QuestionHeader-footer-inner"><div class="QuestionHeader-main QuestionHeader-footer-main"><div class="QuestionButtonGroup"><button type="button" class="Button FollowButton Button--primary Button--blue">关注问题</button><button type="button" class="Button Button--blue"><svg viewBox="0 0 12 12" class="Icon QuestionButton-icon Button-icon Icon--modify" width="14" height="16" aria-hidden="true" style="height: 16px; width: 14px;"><title></title><g><path d="M.423 10.32L0 12l1.667-.474 1.55-.44-2.4-2.33-.394 1.564zM10.153.233c-.327-.318-.85-.31-1.17.018l-.793.817 2.49 2.414.792-.814c.318-.328.312-.852-.017-1.17l-1.3-1.263zM3.84 10.536L1.35 8.122l6.265-6.46 2.49 2.414-6.265 6.46z" fill-rule="evenodd"></path></g></svg>写回答</button></div><div class="QuestionHeaderActions"><button type="button" class="Button Button--grey Button--withIcon Button--withLabel" style="margin-right: 16px;"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Invite Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M4 10V8a1 1 0 1 1 2 0v2h2a1 1 0 0 1 0 2H6v2a1 1 0 0 1-2 0v-2H2a1 1 0 0 1 0-2h2zm10.455 2c-2.21 0-4-1.79-4-4s1.79-4 4-4 4 1.79 4 4-1.79 4-4 4zm-7 6c0-2.66 4.845-4 7.272-4C17.155 14 22 15.34 22 18v1.375c0 .345-.28.625-.625.625H8.08a.625.625 0 0 1-.625-.625V18z" fill-rule="evenodd"></path></svg></span>邀请回答</button><div class="QuestionHeader-Comment"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>添加评论</button></div><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="Popover7-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover7-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><div class="Popover"><button aria-label="更多" type="button" id="Popover8-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover8-content" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div><div class="QuestionHeader-actions"></div></div></div></div></div><div><div class="Sticky is-fixed" style="width: 1903px; top: 52px; left: 0px;"></div><div class="Sticky--holder" style="position: static; top: auto; right: auto; bottom: auto; left: auto; display: block; float: none; margin: 0px; height: 0px;"></div></div></div><div class="Question-main"><div class="Question-mainColumn"><div><div id="QuestionAnswers-answers" class="QuestionAnswers-answers" data-zop-feedlistmap="0,0,1,0" data-za-detail-view-path-module="ContentList" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null}}"><div class="Card AnswersNavWrapper"><div class="ListShortcut"><div class="List"><div class="List-header"><h4 class="List-headerText"><span>13 个回答</span></h4><div class="List-headerOptions"><div class="Popover"><button role="combobox" aria-expanded="false" type="button" id="Popover9-toggle" aria-haspopup="true" aria-owns="Popover9-content" class="Button Select-button Select-plainButton Button--plain">默认排序<span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Select Select-arrow" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M12 16.183l2.716-2.966a.757.757 0 0 1 1.064.001.738.738 0 0 1 0 1.052l-3.247 3.512a.758.758 0 0 1-1.064 0L8.22 14.27a.738.738 0 0 1 0-1.052.758.758 0 0 1 1.063 0L12 16.183zm0-9.365L9.284 9.782a.758.758 0 0 1-1.064 0 .738.738 0 0 1 0-1.052l3.248-3.512a.758.758 0 0 1 1.065 0L15.78 8.73a.738.738 0 0 1 0 1.052.757.757 0 0 1-1.063.001L12 6.818z" fill-rule="evenodd"></path></svg></span></button></div></div></div><div><div class=""><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="0" data-zop="{&quot;authorName&quot;:&quot;YJango&quot;,&quot;itemId&quot;:440401209,&quot;title&quot;:&quot;怎么选取训练神经网络时的Batch size?&quot;,&quot;type&quot;:&quot;answer&quot;}" name="440401209" itemprop="acceptedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="0" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;440401209&quot;,&quot;upvote_num&quot;:583,&quot;comment_num&quot;:47,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;61607442&quot;,&quot;author_member_hash_id&quot;:&quot;4eedf55cf1de9f94173b352f9c5a8d2b&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="YJango"><meta itemprop="image" content="https://pic3.zhimg.com/v2-ff764e30ee9e36f4b357eea4b925bb22_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/YJango"><meta itemprop="zhihu:followerCount" content="229420"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover10-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover10-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/YJango"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/v2-ff764e30ee9e36f4b357eea4b925bb22_xs.jpg" srcset="https://pic3.zhimg.com/v2-ff764e30ee9e36f4b357eea4b925bb22_l.jpg 2x" alt="YJango"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover11-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover11-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/YJango">YJango</a></div></div><a class="UserLink-badge" data-tooltip="优秀回答者 · 已认证的个人" href="https://www.zhihu.com/question/48510028" target="_blank" rel="noopener noreferrer"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--BadgeCG" fill="currentColor" viewBox="0 0 24 24" width="18" height="18"><g fill="none" fill-rule="evenodd"><path fill="#0F88EB" d="M12.055 3.172c.21-.165.397-.344.555-.532l-.127.152c.891-1.065 2.319-1.056 3.195.027l-.125-.153c.872 1.08 2.696 1.856 4.083 1.733l-.197.017c1.383-.122 2.386.893 2.239 2.279l.021-.198c-.147 1.381.593 3.218 1.661 4.113l-.152-.127c1.065.891 1.056 2.319-.027 3.195l.154-.125c-1.08.872-1.856 2.696-1.734 4.084l-.017-.197c.123 1.382-.893 2.385-2.279 2.238l.198.021c-1.38-.147-3.218.593-4.113 1.661l.127-.152c-.891 1.065-2.319 1.057-3.195-.027l.125.154a3.716 3.716 0 0 0-.503-.506c.975-.77 2.422-1.25 3.559-1.13l-.198-.021c1.386.147 2.402-.856 2.279-2.238l.017.197c-.122-1.388.655-3.212 1.734-4.084l-.154.125c1.083-.876 1.092-2.304.027-3.195l.152.127c-1.068-.895-1.808-2.732-1.66-4.113l-.022.198c.147-1.386-.856-2.4-2.24-2.279l.198-.017c-1.159.103-2.622-.422-3.58-1.227z"></path><path fill="#FF9500" d="M19.21 10.483l.151.127c-1.069-.895-1.809-2.732-1.662-4.113l-.02.197c.146-1.386-.857-2.4-2.24-2.278l.197-.018c-1.387.124-3.21-.653-4.083-1.732l.125.153c-.876-1.083-2.304-1.092-3.195-.028l.127-.152c-.894 1.068-2.733 1.808-4.113 1.663l.198.02c-1.386-.147-2.4.857-2.279 2.24L2.4 6.363c.122 1.387-.655 3.21-1.734 4.084l.154-.126c-1.083.877-1.092 2.304-.027 3.194L.64 13.39c1.068.894 1.808 2.733 1.661 4.112l.021-.196c-.147 1.385.856 2.401 2.24 2.28l-.198.015c1.387-.122 3.211.655 4.083 1.734l-.124-.154c.184.228.396.397.62.53a1.89 1.89 0 0 0 1.972 0c.215-.127.421-.287.602-.503l-.127.152c.895-1.068 2.733-1.808 4.113-1.66l-.198-.022c1.387.147 2.402-.856 2.28-2.238l.016.197c-.122-1.389.655-3.212 1.734-4.084l-.154.124c1.083-.876 1.092-2.303.028-3.194"></path><path fill="#FFF" d="M14.946 11.082l-2.362 2.024.721 3.025c.128.534-.144.738-.617.45l-2.654-1.623L7.38 16.58c-.468.286-.746.09-.617-.449l.721-3.025-2.362-2.024c-.417-.357-.317-.68.236-.726l3.101-.248 1.194-2.872c.211-.507.55-.512.763 0l1.195 2.872 3.1.248c.547.044.657.365.236.726"></path></g></svg></span></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="AuthorInfo-badgeText"><span><span><a href="https://www.zhihu.com/people/YJango/creations/19559450">机器学习</a>、</span><span><a href="https://www.zhihu.com/people/YJango/creations/19813032">深度学习（Deep Learning）</a> </span>话题</span>的优秀回答者</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button type="button" class="Button Button--plain">583 人赞同了该回答</button></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="583"><meta itemprop="url" content="https://www.zhihu.com/question/61607442/answer/440401209"><meta itemprop="dateCreated" content="2018-07-12T03:28:12.000Z"><meta itemprop="dateModified" content="2018-10-09T07:27:11.000Z"><meta itemprop="commentCount" content="47"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text"><h2>目录</h2><ul><li>batch size 需要调参</li><li>个人经验分享</li><li>为什么 batch size 会影响训练结果</li><li><a href="https://link.zhihu.com/?target=http%3A//playground.tensorflow.org/" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">Tensorflow Playground</a> 演示</li></ul><hr><h2>batch size 需要<b>调参</b></h2><p>可以确定的是，<b>batch size 绝非越大越好</b>。</p><p>因为 batch size 的极限是训练集样本总个数，而这是当初神经网络还未如此之火时的训练方式 Gradient Descent (GD)。</p><p>2014 年初我上的人工神经网络 (还不叫深度学习，非常旧的教学材料) 课程，老师留的作业就是用自己写个神经网络去实现 4 输入<a href="https://link.zhihu.com/?target=https%3A//wenku.baidu.com/view/bba0bf1090c69ec3d5bb7537.html" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">异或门</a>。</p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/50/v2-2ba1c4a00fa00982dd27b4f879e31211_hd.jpg" data-size="normal" data-rawwidth="218" data-rawheight="203" data-default-watermark-src="https://pic1.zhimg.com/50/v2-03a4161ef84e407b821ba9568d9ea200_hd.jpg" class="content_image" width="218"/></noscript><img src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/v2-2ba1c4a00fa00982dd27b4f879e31211_hd.jpg" data-size="normal" data-rawwidth="218" data-rawheight="203" data-default-watermark-src="https://pic1.zhimg.com/50/v2-03a4161ef84e407b821ba9568d9ea200_hd.jpg" class="content_image lazy" width="218" data-actualsrc="https://pic3.zhimg.com/50/v2-2ba1c4a00fa00982dd27b4f879e31211_hd.jpg" data-lazy-status="ok"><figcaption>3 输入异或门</figcaption></figure><p>中间层的 <img src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/equation" alt="[公式]" eeimg="1" data-formula="W"> 的梯度是纯粹用解析公式计算而得 (不是如今的计算图)，激活函数是 sigmoid (不是 Relu)，而且每次更新都使用所有的 16 个样本，就是单纯的去完全拟合训练集，只是为了查看神经网络拟合任意函数的能力 (这很不现实，因为如果知道所有样本，压根就不需要去学习。现实的任务，是需要从有限的训练样本中训练出可以用于预测测试数据的模型)。</p><p>下面是我当时交作业的 loss 下降图。可以看到 loss 呈阶梯式下降。当时被解读为是陷入局部最小值。22000次 (横坐标) 迭代才能够拟合 (纵坐标是 loss 大小)，用全部样本作为 batch size 的 Gradient Descent (GD) 效果是很差的。</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/50/v2-4f584772442b50916a5fcc71e5394dde_hd.jpg" data-caption="" data-size="normal" data-rawwidth="406" data-rawheight="361" data-default-watermark-src="https://pic4.zhimg.com/50/v2-239c27a037fdc88f8ea020c07bbb618d_hd.jpg" class="content_image" width="406"/></noscript><img src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/v2-4f584772442b50916a5fcc71e5394dde_hd.jpg" data-caption="" data-size="normal" data-rawwidth="406" data-rawheight="361" data-default-watermark-src="https://pic4.zhimg.com/50/v2-239c27a037fdc88f8ea020c07bbb618d_hd.jpg" class="content_image lazy" width="406" data-actualsrc="https://pic1.zhimg.com/50/v2-4f584772442b50916a5fcc71e5394dde_hd.jpg" data-lazy-status="ok"></figure><p>深度学习和以前的人工神经网络差在哪里了？为什么近几年突然变成各个媒体吹上天的神话？事实上深度学习的核心技术在二三十年前就有，是各个技术组合方式的才造成了今天的差异 (也因如此，刚入门的伙伴很容易因某个细节不同甚至无法收敛)。这可以从 keras, lasagne 等库包的文档分类注意到<b>深度学习的跃进来源于不同技术的组合</b>：<b>层</b>、<b>梯度更新方式</b>、<b>初始化方式</b>、<b>非线性</b>、<b>目标函数</b>、<b>正规项</b>等。(研究时也会分别去提升不同的技术细节)</p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/50/v2-062251e9c131082c263b8a6eb1bb7f14_hd.jpg" data-size="normal" data-rawwidth="157" data-rawheight="175" class="content_image" width="157"/></noscript><img src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/v2-062251e9c131082c263b8a6eb1bb7f14_hd.jpg" data-size="normal" data-rawwidth="157" data-rawheight="175" class="content_image lazy" width="157" data-actualsrc="https://pic2.zhimg.com/50/v2-062251e9c131082c263b8a6eb1bb7f14_hd.jpg" data-lazy-status="ok"><figcaption>lasagne 文档分类</figcaption></figure><p>现在人们分析深度学习崛起原因时人们常会说：因为大数据、GPU 的高并行计算能力、ReLU，等等，但还有一个常被忽略，简单却又意外强大的因素就是 Gradient Descent (GD) 替换成了 Stochastic Gradient Descent (SGD)。随后又有大批的弥补 朴素SGD 不足的更新算法，也就是在 <b>梯度更新方式</b> 这一类。可以参考这篇文章 <a href="https://link.zhihu.com/?target=http%3A//ruder.io/optimizing-gradient-descent/" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">An overview of gradient descent optimization algorithms</a>。</p><ul><li>Gradient Descent：所有样本算出的梯度的平均值来更新每一步</li><li>Stochastic Gradient Descent：一个样本算出的梯度来更新每一步</li></ul><p>虽然从<b>直觉上</b>，Gradient Descent 可以快速准确的将训练集的 loss 降低到最小，但实际上很容易陷入局部最小值或鞍点 (不知道什么是鞍点的看 <span><span class="UserLink"><div class="Popover"><div id="Popover23-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover23-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/02c1927a26f1fa47d3d43ec304b81015" data-za-detail-view-id="1045">夕小瑶Elsa</a></div></div></span></span> 的 <a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzIwNzc2NTk0NQ%3D%3D%26mid%3D2247484570%26idx%3D1%26sn%3D4c0b6b76a7f2518d77818535b677e087%26chksm%3D970c2c4ca07ba55ad5cfe6b46f72dbef85a159574fb60b9932404e45747c95eed8c6c0f66d62%23rd" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">batch</a>) 无法继续下降，反而达不到最小。而且 GD 每一步的计算量巨大。即使是可以成功将训练集的 loss 降到最低，可我们并不关心训练集的表现，我们关心的是<b>测试集</b>的表现。训练集表现好，测试集表现差的过拟合 (overfitting) 并不是我们想要的。</p><p>Stochastic Gradient Descent 的随机性有利于跳出鞍点，又具有加强普遍性 (测试集上表现优秀) 的作用。可观看以下视频感受 <b>SGD 如何缓解陷入局部最小值</b></p><div><div class="RichText-video" data-za-detail-view-path-module="VideoItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Video&quot;,&quot;sub_type&quot;:&quot;SelfHosted&quot;,&quot;video_id&quot;:&quot;1031175825587978240&quot;,&quot;is_playable&quot;:true}}}"><div class="VideoCard VideoCard--interactive"><div class="VideoCard-layout"><div class="VideoCard-video"><div class="VideoCard-video-content"><div class="VideoCard-player"><iframe frameborder="0" allowfullscreen="" src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/1031175825587978240.html"></iframe></div></div></div><div class="VideoCard-content"><div class="VideoCard-title">缓解陷入局部最小值</div></div></div><div class="VideoCard-mask"></div></div></div></div><p class="ztext-empty-paragraph"><br></p><p>可惜训练耗时，同时过大的样本差异会使训练比较震荡，所以有了 Minibatch Gradient Descent 方法，同时具有二者的特点，是以  <img src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/equation(1)" alt="[公式]" eeimg="1" data-formula="n">  个样本算出的梯度的平均值来更新每一步。然而不得不苦逼的根据不同的任务去寻找最优的 <img src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/equation(1)" alt="[公式]" eeimg="1" data-formula="n"> 。不过在计算机视觉中，由于 batch normalization 的帮助，可以使用极大的 batch size，这时的 batch size 往往是受限于 GPU 的显存大小。</p><hr><h2>个人经验分享</h2><p>个人习惯从 batch size 以 128 为分界线。向下 (x0.5) 和向上 (x2) 训练后比较测试结果。若向下更好则再 x0.5，直到结果不再提升。</p><p>我遇到过很多情况。</p><p>比如同样的 cnn+rnn 模型结构，adam 更新法，learning rate 和其他参数全部相同。</p><ul><li><b>例1：</b>遇到某个语音、画面、自然语言不同任务时，最好的 batch size 分别为 8, 32, 16。</li><li><b>例2：</b>遇到某个多信号语音识别任务时， <b>batch size 为 1 最好</b>，可是训练时间让人抓狂，尤其是使用循环网络时。而将这些多个信号之间做 inversion mapping 时，结果却是 batch size 设为 32 后效果依然提升，但由于 GPU 显存限制无法继续加大 batch size。</li></ul><p>可以看出来，调参也有时间和空间的限制：</p><blockquote><b>大 batch size 限于空间，小 batch size 苦于时间。</b></blockquote><hr><h2>为什么 batch size 会影响训练结果</h2><p><span><span class="UserLink"><div class="Popover"><div id="Popover24-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover24-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/c4cfdf10563bff74ebf3e58d9f1c8592" data-za-detail-view-id="1045">Unstoppable</a></div></div></span></span> 有问到“一直很难理解为什么 batch size 会影响训练结果，答主能解读一下么”。</p><p>举一个特别简化的例子。</p><p>假如要学习一个 <img src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/equation(2)" alt="[公式]" eeimg="1" data-formula="f:x\rightarrow y"> 去拟合下面这样的一维数据，如下图所示。有 6 个训练样本 (编号 1,2,3,4,5,6)。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/50/v2-7ffb5d7ee7b6d582a2be9768dc6fa4b1_hd.jpg" data-caption="" data-size="normal" data-rawwidth="396" data-rawheight="324" data-default-watermark-src="https://pic4.zhimg.com/50/v2-6b8ead921a3bc78883a287bd6a75a4ea_hd.jpg" class="content_image" width="396"/></noscript><img src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/v2-7ffb5d7ee7b6d582a2be9768dc6fa4b1_hd.jpg" data-caption="" data-size="normal" data-rawwidth="396" data-rawheight="324" data-default-watermark-src="https://pic4.zhimg.com/50/v2-6b8ead921a3bc78883a287bd6a75a4ea_hd.jpg" class="content_image lazy" width="396" data-actualsrc="https://pic2.zhimg.com/50/v2-7ffb5d7ee7b6d582a2be9768dc6fa4b1_hd.jpg" data-lazy-status="ok"></figure><p>如果使用 Gradient Descent，那么每一步的更新都会向类似于黄线那种同时满足 6 个样本的 <img src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/equation(3)" alt="[公式]" eeimg="1" data-formula="f"> 去贴近 (不是一步到位，而是一点点更新变形)。</p><p>如果使用 Stochastic Gradient Descent，那么每一步的更新都会向能穿过该样本点的<img src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/equation(3)" alt="[公式]" eeimg="1" data-formula="f"> 去贴近。</p><p>如果使用 3-Batch Gradient Descent，那么每一步都会向同时满足 3 个样本点的形状更新，比如 1,2,3 样本组成的 batch 会往黑色虚线的形状变化。下一步可能是向同时满足 1,5,6 的 batch 的形状去变化。</p><p>同理，若使用 2-Batch Gradient Descent，绿色虚线是例子。</p><p>打个比方。这好比素描绘画，不同的 batch 策略决定了用什么样的线条 (线段、曲线段、角度等) 去描绘 (允许覆盖已画的线条)，最后画出的图形是由这些线条构成的。神经网络训练所得的 <img src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/equation(3)" alt="[公式]" eeimg="1" data-formula="f"> 也是根据梯度 (类比线条) 一点点更新出来的。所以能够影响梯度的 batch size 会影响神经网络训练出的 <img src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/equation(3)" alt="[公式]" eeimg="1" data-formula="f"> 。</p><p>诚然用 Gradient Descent 同时参考所有的训练样本去选择绘画线条的话，感觉上最能够画出拟合所有训练集样本的形状，但是这样会使每次绘画的线条从绘画开始到结束都是一样的 (对应神经网络陷入鞍点)，仅仅是不断的描粗线条，反而容易画不出想要的形状。即便是能够成功画出，但别忘了，我们真正想要画的<b>形状是连同测试样本在内的 (编号 a,b) 也同时满足的 </b><img src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/equation(3)" alt="[公式]" eeimg="1" data-formula="f">。</p><p>可以把神经网络的训练想象成一种特殊的绘画。绘画过程中不允许使用测试样本作为参考，只允许用<b>有限的</b>训练样本作为参考，还要求所画的形状覆盖测试样本。</p><hr><h2><a href="https://link.zhihu.com/?target=http%3A//playground.tensorflow.org/" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">Tensorflow Playground</a> 演示</h2><p>我们可以用<a href="https://link.zhihu.com/?target=http%3A//playground.tensorflow.org/" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">Tensorflow — Neural Network Playground</a>来实际的体验一下这种“绘画”过程。</p><ul><li>下图是 batch size 30 时，拟合 300 epoch 的情况 (每个 epoch 是指遍历完所有训练集样本)。拟合很容易卡在某个形状不动。</li></ul><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/50/v2-ab3bf9b9c3e4cf669bf51686752f8374_hd.jpg" data-size="normal" data-rawwidth="262" data-rawheight="329" data-default-watermark-src="https://pic4.zhimg.com/50/v2-ad67d04db7c01d98fee5d4c42dd227f6_hd.jpg" class="content_image" width="262"/></noscript><img src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/v2-ab3bf9b9c3e4cf669bf51686752f8374_hd.jpg" data-size="normal" data-rawwidth="262" data-rawheight="329" data-default-watermark-src="https://pic4.zhimg.com/50/v2-ad67d04db7c01d98fee5d4c42dd227f6_hd.jpg" class="content_image lazy" width="262" data-actualsrc="https://pic3.zhimg.com/50/v2-ab3bf9b9c3e4cf669bf51686752f8374_hd.jpg" data-lazy-status="ok"><figcaption>batch size 30 的拟合</figcaption></figure><ul><li>下图是 batch size 1 时，拟合 150 epoch 的情况，很快的就拟合成功了。但注意 loss 曲线的下降。比 batch size 30 时，有上下波动的情况。</li></ul><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/50/v2-405ab493008fdbd788d350162a830cd9_hd.jpg" data-size="normal" data-rawwidth="262" data-rawheight="330" data-default-watermark-src="https://pic2.zhimg.com/50/v2-13e5472fe93fcf62557c76b468876941_hd.jpg" class="content_image" width="262"/></noscript><img src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/v2-405ab493008fdbd788d350162a830cd9_hd.jpg" data-size="normal" data-rawwidth="262" data-rawheight="330" data-default-watermark-src="https://pic2.zhimg.com/50/v2-13e5472fe93fcf62557c76b468876941_hd.jpg" class="content_image lazy" width="262" data-actualsrc="https://pic3.zhimg.com/50/v2-405ab493008fdbd788d350162a830cd9_hd.jpg" data-lazy-status="ok"><figcaption>batch size 1 的拟合</figcaption></figure><p>从 <a href="https://link.zhihu.com/?target=http%3A//playground.tensorflow.org/" class=" wrap external" target="_blank" rel="nofollow noreferrer">Playground</a> 的例子中就可以看出， batch size 并非越大越好。</p></span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/61607442/answer/440401209"><span data-tooltip="发布于 2018-07-12 11:28">编辑于 2018-10-09</span></a></div></div><div><div class="ContentItem-actions Sticky RichContent-actions is-fixed is-bottom" style="width: 694px; bottom: 0px; left: 451.5px;"><span><button aria-label="赞同 583" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 583</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>47 条评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover26-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover26-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button><button data-zop-retract-question="true" type="button" class="Button ContentItem-action ContentItem-rightButton Button--plain"><span class="RichContent-collapsedText">收起</span><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--ArrowDown ContentItem-arrowIcon is-active" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M12 13L8.285 9.218a.758.758 0 0 0-1.064 0 .738.738 0 0 0 0 1.052l4.249 4.512a.758.758 0 0 0 1.064 0l4.246-4.512a.738.738 0 0 0 0-1.052.757.757 0 0 0-1.063 0L12.002 13z" fill-rule="evenodd"></path></svg></span></button></div><div class="Sticky--holder" style="position: static; top: auto; right: auto; bottom: auto; left: auto; display: flex; float: none; margin: 0px -20px -10px; height: 54px; width: 694px;"></div></div></div></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="1" data-zop="{&quot;authorName&quot;:&quot;夕小瑶&quot;,&quot;itemId&quot;:204525634,&quot;title&quot;:&quot;怎么选取训练神经网络时的Batch size?&quot;,&quot;type&quot;:&quot;answer&quot;}" name="204525634" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="1" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;204525634&quot;,&quot;upvote_num&quot;:218,&quot;comment_num&quot;:20,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;61607442&quot;,&quot;author_member_hash_id&quot;:&quot;02c1927a26f1fa47d3d43ec304b81015&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="夕小瑶"><meta itemprop="image" content="https://pic1.zhimg.com/v2-70ae276a4834426ad670577b953f4fbb_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/tsxiyao"><meta itemprop="zhihu:followerCount" content="14346"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover13-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover13-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/tsxiyao"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/v2-70ae276a4834426ad670577b953f4fbb_xs.jpg" srcset="https://pic1.zhimg.com/v2-70ae276a4834426ad670577b953f4fbb_l.jpg 2x" alt="夕小瑶"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover14-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover14-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/tsxiyao">夕小瑶</a></div></div><a class="UserLink-badge" data-tooltip="优秀回答者" href="https://www.zhihu.com/question/48509984" target="_blank" rel="noopener noreferrer"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--BadgeGlorious" fill="currentColor" viewBox="0 0 24 24" width="18" height="18"><g fill="none" fill-rule="evenodd"><path fill="#FF9500" d="M2.64 13.39c1.068.895 1.808 2.733 1.66 4.113l.022-.196c-.147 1.384.856 2.4 2.24 2.278l-.198.016c1.387-.122 3.21.655 4.083 1.734l-.125-.154c.876 1.084 2.304 1.092 3.195.027l-.127.152c.895-1.068 2.733-1.808 4.113-1.66l-.198-.022c1.386.147 2.402-.856 2.279-2.238l.017.197c-.122-1.388.655-3.212 1.734-4.084l-.154.125c1.083-.876 1.092-2.304.027-3.195l.152.127c-1.068-.895-1.808-2.732-1.66-4.113l-.022.198c.147-1.386-.856-2.4-2.24-2.279l.198-.017c-1.387.123-3.21-.654-4.083-1.733l.125.153c-.876-1.083-2.304-1.092-3.195-.027l.127-.152c-.895 1.068-2.733 1.808-4.113 1.662l.198.02c-1.386-.147-2.4.857-2.279 2.24L4.4 6.363c.122 1.387-.655 3.21-1.734 4.084l.154-.126c-1.083.878-1.092 2.304-.027 3.195l-.152-.127z"></path><path fill="#FFF" d="M12.034 14.959L9.379 16.58c-.468.286-.746.09-.617-.449l.721-3.025-2.362-2.024c-.417-.357-.317-.681.236-.725l3.1-.249 1.195-2.872c.21-.507.55-.512.763 0l1.195 2.872 3.1.249c.547.043.657.365.236.725l-2.362 2.024.721 3.025c.128.534-.144.738-.617.449l-2.654-1.621z"></path></g></svg></span></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="AuthorInfo-badgeText"><span><span><a href="https://www.zhihu.com/people/tsxiyao/creations/19559450">机器学习</a> </span>话题</span>的优秀回答者</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button type="button" class="Button Button--plain">218 人赞同了该回答</button></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="218"><meta itemprop="url" content="https://www.zhihu.com/question/61607442/answer/204525634"><meta itemprop="dateCreated" content="2017-07-27T07:48:02.000Z"><meta itemprop="dateModified" content="2017-07-27T07:48:03.000Z"><meta itemprop="commentCount" content="20"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text"><p>谢邀。邀请我的一定是订阅号的粉丝，2333。</p><p>这个问题我已经详细写过了，搬过来。</p><p>原链接：<a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzIwNzc2NTk0NQ%3D%3D%26mid%3D2247484570%26idx%3D1%26sn%3D4c0b6b76a7f2518d77818535b677e087%26chksm%3D970c2c4ca07ba55ad5cfe6b46f72dbef85a159574fb60b9932404e45747c95eed8c6c0f66d62%23rd" class=" wrap external" target="_blank" rel="nofollow noreferrer">训练神经网络时如何确定batch的大小？</a></p><p class="ztext-empty-paragraph"><br></p><h2>前言</h2><p>当我们要训练一个已经写好的神经网络时，我们就要直面诸多的超参数了。这些超参数一旦选不好，那么很有可能让神经网络跑的还不如感知机。因此在面对神经网络这种容量很大的model前，是很有必要深刻的理解一下各个超参数的意义及其对model的影响的。</p><h2>回顾</h2><p>简单回顾一下神经网络的<b>一次</b>迭代过程：</p><figure><noscript><img src="https://pic2.zhimg.com/50/v2-2e9e673571e3a56728798e3b15777afd_hd.jpg" data-rawwidth="1729" data-rawheight="1120" class="origin_image zh-lightbox-thumb" width="1729" data-original="https://pic2.zhimg.com/v2-2e9e673571e3a56728798e3b15777afd_r.jpg"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1729&#39; height=&#39;1120&#39;&gt;&lt;/svg&gt;" data-rawwidth="1729" data-rawheight="1120" class="origin_image zh-lightbox-thumb lazy" width="1729" data-original="https://pic2.zhimg.com/v2-2e9e673571e3a56728798e3b15777afd_r.jpg" data-actualsrc="https://pic2.zhimg.com/50/v2-2e9e673571e3a56728798e3b15777afd_hd.jpg"></figure><p>即，首先选择n个样本组成一个batch，然后将batch丢进神经网络，得到输出结果。再将输出结果与样本label丢给loss函数算出本轮的loss，而后就可以愉快的跑BP算法了（从后往前逐层计算参数之于loss的导数）。最后将每个参数的导数配合步长参数来进行参数更新。这就是训练过程的一次迭代。</p><h2>Batch Size</h2><p>由此，最直观的超参数就是batch的大小——我们可以一次性将整个数据集喂给神经网络，让神经网络利用全部样本来计算迭代时的梯度（即传统的梯度下降法），也可以一次只喂一个样本（即随机梯度下降法，也称在线梯度下降法），也可以取个折中的方案，即每次喂一部分样本让其完成本轮迭代（即batch梯度下降法）。</p><p>数学基础不太好的初学者可能在这里犯迷糊——一次性喂500个样本并迭代一次，跟一次喂1个样本迭代500次相比，有区别吗？</p><p>其实这两个做法就相当于：</p><p>第一种：<br>total = 旧参下计算更新值1+旧参下计算更新值2+...+旧参下计算更新值500 ;<br>新参数 = 旧参数 + total</p><p>第二种：<br>新参数1 = 旧参数 + 旧参数下计算更新值1；<br>新参数2 = 新参数1 + 新参数1下计算更新值1；<br>新参数3 = 新参数2 + 新参数2下计算更新值1；<br>...<br>新参数500 = 新参数500 + 新参数500下计算更新值1；</p><p>也就是说，第一种是将参数一次性更新500个样本的量，第二种是迭代的更新500次参数。当然是不一样的啦。<br></p><p>那么问题来了，哪个更好呢？<br></p><h2>Which one？</h2><p>我们首先分析最简单的影响，哪种做法收敛更快呢？</p><p>我们假设每个样本相对于大自然真实分布的标准差为σ，那么根据概率统计的知识，很容易推出n个样本的标准差为 <img src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/equation(4)" alt="[公式]" eeimg="1" data-formula="\sigma/\sqrt{n}"> （有疑问的同学快翻开概率统计的课本看一下推导过程）。从这里可以看出，我们使用样本来估计梯度的时候，1个样本带来σ的标准差，但是使用n个样本区估计梯度并不能让标准差线性降低（也就是并不能让误差降低为原来的1/n，即无法达到σ/n），而n个样本的计算量却是线性的（每个样本都要平等的跑一遍前向算法）。</p><p>由此看出，显然在同等的计算量之下（一定的时间内），使用整个样本集的收敛速度要远慢于使用少量样本的情况。换句话说，要想收敛到同一个最优点，使用整个样本集时，虽然迭代次数少，但是每次迭代的时间长，耗费的总时间是大于使用少量样本多次迭代的情况的。<br></p><p>那么是不是样本越少，收敛越快呢？</p><p>理论上确实是这样的，使用单个单核cpu的情况下也确实是这样的。但是我们要与工程实际相结合呀~实际上，工程上在使用GPU训练时，跑一个样本花的时间与跑几十个样本甚至几百个样本的时间是一样的！当然得益于GPU里面超多的核，超强的并行计算能力啦。因此，在工程实际中，从收敛速度的角度来说，小批量的样本集是最优的，也就是我们所说的mini-batch。这时的batch size往往从几十到几百不等，但一般不会超过几千（你有土豪显卡的话，当我没说）。<br></p><p>那么，如果我真有一个怪兽级显卡，使得一次计算10000个样本跟计算1个样本的时间相同的话，是不是设置10000就一定是最好的呢？虽然从收敛速度上来说是的，但！是！</p><p>我们知道，神经网络是个复杂的model，它的损失函数也不是省油的灯，在实际问题中，神经网络的loss曲面（以model参数为自变量，以loss值为因变量画出来的曲面）往往是非凸的，这意味着很可能有多个局部最优点，而且很可能有鞍点！</p><p>插播一下，鞍点就是loss曲面中像马鞍一样形状的地方的中心点，如下图：</p><p class="ztext-empty-paragraph"><br></p><figure><noscript><img src="https://pic1.zhimg.com/50/v2-817cf83b6e9b5da3859457cee2d70215_hd.jpg" data-rawwidth="1003" data-rawheight="549" class="origin_image zh-lightbox-thumb" width="1003" data-original="https://pic1.zhimg.com/v2-817cf83b6e9b5da3859457cee2d70215_r.jpg"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1003&#39; height=&#39;549&#39;&gt;&lt;/svg&gt;" data-rawwidth="1003" data-rawheight="549" class="origin_image zh-lightbox-thumb lazy" width="1003" data-original="https://pic1.zhimg.com/v2-817cf83b6e9b5da3859457cee2d70215_r.jpg" data-actualsrc="https://pic1.zhimg.com/50/v2-817cf83b6e9b5da3859457cee2d70215_hd.jpg"></figure><p>（图片来自《Deep Learning》）</p><p>想象一下，在鞍点处，横着看的话，鞍点就是个极小值点，但是竖着看的话，鞍点就是极大值点（线性代数和最优化算法过关的同学应该能反应过来，鞍点处的Hessian矩阵的特征值有正有负。不理解也没关系，小夕过几天就开始写最优化的文章啦~），因此鞍点容易给优化算法一个“我已经收敛了”的假象，殊不知其旁边有一个可以跳下去的万丈深渊。。。（可怕）</p><p>回到主线上来，小夕在<a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzIwNzc2NTk0NQ%3D%3D%26mid%3D2247483887%26idx%3D1%26sn%3De402f45fd2eaccf5bccf2b348d2f85c5%26chksm%3D970c2939a07ba02f53b125708b4d5069d2ad6d5b0e8b95d27e15ae36694814470e9d2bd7a9e1%26scene%3D21%23wechat_redirect" class=" wrap external" target="_blank" rel="nofollow noreferrer">《机器学习入门指导（4）》</a>中提到过，传统的最优化算法是无法自动的避开局部最优点的，对于鞍点也是理论上很头疼的东西。但是实际上，工程中却不怎么容易陷入很差劲的局部最优点或者鞍点，这是为什么呢？</p><p>暂且不说一些很高深的理论如“神经网络的loss曲面中的局部最优点与全局最优点差不太多”，我们就从最简单的角度想~</p><p>想一想，样本量少的时候会带来很大的方差，而这个大方差恰好会导致我们在梯度下降到很差的局部最优点（只是微微凸下去的最优点）和鞍点的时候不稳定，一不小心就因为一个大噪声的到来导致炸出了局部最优点，或者炸下了马（此处请保持纯洁的心态！），从而有机会去寻找更优的最优点。</p><p>因此，与之相反的，当样本量很多时，方差很小（咦？最开始的时候好像在说标准差来着，反正方差与标准差就差个根号，没影响的哈~），对梯度的估计要准确和稳定的多，因此反而在差劲的局部最优点和鞍点时反而容易自信的呆着不走了，从而导致神经网络收敛到很差的点上，跟出了bug一样的差劲。</p><p>小总结一下，batch的size设置的不能太大也不能太小，因此实际工程中最常用的就是mini-batch，一般size设置为几十或者几百。但是！</p><p>好像这篇文章的转折有点多了诶。。。</p><p>细心的读者可能注意到了，这之前我们的讨论是基于梯度下降的，而且默认是一阶的（即没有利用二阶导数信息，仅仅使用一阶导数去优化）。因此对于SGD（随机梯度下降）及其改良的一阶优化算法如Adagrad、Adam等是没问题的，但是对于强大的二阶优化算法如共轭梯度法、L-BFGS来说，如果估计不好一阶导数，那么对二阶导数的估计会有更大的误差，这对于这些算法来说是致命的。</p><p>因此，对于二阶优化算法，减小batch换来的收敛速度提升远不如引入大量噪声导致的性能下降，因此在使用二阶优化算法时，往往要采用大batch哦。此时往往batch设置成几千甚至一两万才能发挥出最佳性能。</p><p>另外，听说GPU对2的幂次的batch可以发挥更佳的性能，因此设置成16、32、64、128...时往往要比设置为整10、整100的倍数时表现更优（不过我没有验证过，有兴趣的同学可以试验一下~</p></span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/61607442/answer/204525634"><span data-tooltip="发布于 2017-07-27 15:48">编辑于 2017-07-27</span></a></div></div><div class="ContentItem-actions RichContent-actions"><span><button aria-label="赞同 218" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 218</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>20 条评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover15-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover15-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button><button data-zop-retract-question="true" type="button" class="Button ContentItem-action ContentItem-rightButton Button--plain"><span class="RichContent-collapsedText">收起</span><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--ArrowDown ContentItem-arrowIcon is-active" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M12 13L8.285 9.218a.758.758 0 0 0-1.064 0 .738.738 0 0 0 0 1.052l4.249 4.512a.758.758 0 0 0 1.064 0l4.246-4.512a.738.738 0 0 0 0-1.052.757.757 0 0 0-1.063 0L12.002 13z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="2" data-zop="{&quot;authorName&quot;:&quot;知乎用户&quot;,&quot;itemId&quot;:192204021,&quot;title&quot;:&quot;怎么选取训练神经网络时的Batch size?&quot;,&quot;type&quot;:&quot;answer&quot;}" name="192204021" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="2" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;192204021&quot;,&quot;upvote_num&quot;:45,&quot;comment_num&quot;:8,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;61607442&quot;,&quot;author_member_hash_id&quot;:&quot;6eab0ad9ca59e60ed71ae99e209f1a32&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="知乎用户"><meta itemprop="image" content="https://pic4.zhimg.com/da8e974dc_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/"><meta itemprop="zhihu:followerCount" content="932"><span class="UserLink AuthorInfo-avatarWrapper"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/da8e974dc_xs.jpg" srcset="https://pic4.zhimg.com/da8e974dc_l.jpg 2x" alt="知乎用户"></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name">知乎用户</span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button type="button" class="Button Button--plain">45 人赞同了该回答</button></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="45"><meta itemprop="url" content="https://www.zhihu.com/question/61607442/answer/192204021"><meta itemprop="dateCreated" content="2017-07-01T17:39:09.000Z"><meta itemprop="dateModified" content="2017-07-03T18:50:07.000Z"><meta itemprop="commentCount" content="8"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text"><p>batch size 不是永远越大越好。</p><p>更大的 batch size 更能准确地计算出梯度，mini-batch 只是用更少的计算来近似梯度。直觉上好像更大的 batch 更好因为更“准确”。</p><p>实践结果比较反直觉，因为深度模型的代价函数非常不平整，完整的梯度也只能往局部极小值跑，所以大 batch size 带来的准确的梯度可能也没什么用。反而有时候小 batch size 带来的噪声可以更快的找到不错的极小值。</p><p>但是，更大的 batch size 在并行计算上更占优势，太小的 batch-size 发挥不了 gpu 的性能。所以当我们觉得 batch 要更大，主要是为了更好的并行，而不是为了更精确的梯度。 </p><p>在深度学习里好像追求精确意义不大，比如更高精度的浮点数并没有什么帮助，还不如减小精度来换取计算速度。比如调参时 ramdom search 居然不比 grid search 表现差，非常反直觉。</p><br><p>补充：</p><p>@dengdan在评论里指出，小batch训练的稳定性较差。小batch确实有这个缺点，而且对设置学习速率有更高的要求，否则可能引起恶性的震荡无法收敛。但是小batch的优点仍然是显著的，DL书里建议使用逐步增加的batch size来兼并两者的优点。</p></span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/61607442/answer/192204021"><span data-tooltip="发布于 2017-07-02 01:39">编辑于 2017-07-04</span></a></div></div><div class="ContentItem-actions RichContent-actions"><span><button aria-label="赞同 45" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 45</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>8 条评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover16-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover16-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button><button data-zop-retract-question="true" type="button" class="Button ContentItem-action ContentItem-rightButton Button--plain"><span class="RichContent-collapsedText">收起</span><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--ArrowDown ContentItem-arrowIcon is-active" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M12 13L8.285 9.218a.758.758 0 0 0-1.064 0 .738.738 0 0 0 0 1.052l4.249 4.512a.758.758 0 0 0 1.064 0l4.246-4.512a.738.738 0 0 0 0-1.052.757.757 0 0 0-1.063 0L12.002 13z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div class="Pc-word"><div class="Pc-word-card"><a target="_blank" href="https://www.togocareer.com/services.html?tgcChannel==zhihu&amp;tuwen0805"><div class="Pc-word-card-brand"><div class="Pc-word-card-brand-wrapper"><img width="20" height="20" src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/v2-b02efb32af2aada63f09e5d0a275df75_250x250.jpeg" alt="logo"><span>Togocareer</span></div></div></a><div class="Pc-word-card-sign"><div class="Pc-word-card-sign-label">广告​<svg class="Icon Icon--triangle Pc-word-card-sign-svg" viewBox="0 0 24 24"><path d="M9.218 16.78a.737.737 0 0 0 1.052 0l4.512-4.249a.758.758 0 0 0 0-1.063L10.27 7.22a.737.737 0 0 0-1.052 0 .759.759 0 0 0-.001 1.063L13 12l-3.782 3.716a.758.758 0 0 0 0 1.063z" fill-rule="evenodd"></path></svg></div><div class="Pc-word-card-sign-popup Pc-word-card-sign-popup--isHidden"><span class="Pc-word-card-sign-popup-arrow"></span><div class="Pc-word-card-sign-popup-menu"><button type="button">不感兴趣</button><a target="_blank" rel="noopener noreferrer" href="https://www.zhihu.com/promotion-intro">知乎广告介绍</a></div></div></div><a target="_blank" href="https://www.togocareer.com/services.html?tgcChannel==zhihu&amp;tuwen0805"><h2 class="Pc-word-card-title">对于留学生来说，回国求职，如何进入名企工作？实习机会怎么找？</h2><div class="Pc-word-card-content  "><span>留学生回国，在国内的发展情况如何？2019年校招、社招信息、500强名企内推名额预约，实习，求职更快捷！</span><span class="Pc-word-card-content-cta  ">查看详情</span></div></a></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="4" data-zop="{&quot;authorName&quot;:&quot;Lunarnai&quot;,&quot;itemId&quot;:440944387,&quot;title&quot;:&quot;怎么选取训练神经网络时的Batch size?&quot;,&quot;type&quot;:&quot;answer&quot;}" name="440944387" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="4" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;440944387&quot;,&quot;upvote_num&quot;:44,&quot;comment_num&quot;:16,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;61607442&quot;,&quot;author_member_hash_id&quot;:&quot;355b43e4f8511d2554b090b4f4a8e997&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="Lunarnai"><meta itemprop="image" content="https://pic1.zhimg.com/v2-3fc1dbe922d9981feb59b33dcce4b39d_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/geeking-lcq"><meta itemprop="zhihu:followerCount" content="306"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover17-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover17-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/geeking-lcq"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/v2-3fc1dbe922d9981feb59b33dcce4b39d_xs.jpg" srcset="https://pic1.zhimg.com/v2-3fc1dbe922d9981feb59b33dcce4b39d_l.jpg 2x" alt="Lunarnai"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover18-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover18-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/geeking-lcq">Lunarnai</a></div></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="ztext AuthorInfo-badgeText">也不知道说啥  http://lunarnai.cn</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button type="button" class="Button Button--plain">44 人赞同了该回答</button></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="44"><meta itemprop="url" content="https://www.zhihu.com/question/61607442/answer/440944387"><meta itemprop="dateCreated" content="2018-07-12T14:59:32.000Z"><meta itemprop="dateModified" content="2018-07-18T06:32:15.000Z"><meta itemprop="commentCount" content="16"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text"><p>首先<b>反对上面的尽可能调大batch size</b>的说法，在现在较前沿的视角来看，这种观点无疑是有些滞后的。</p><p>关于这个问题，我们来看下深度学习三巨头之一的LeCun杨乐春同志怎么说（想看结论直接翻到最后）：  </p><blockquote>Training with <b>large minibatches is bad</b> for your health. More importantly, it's bad for your test error. Friends don‘t let friends use minibatches larger than 32. Let's face it: the <i>only </i>people have switched to minibatch sizes larger than one since 2012 is because GPUs are inefficient for batch sizes smaller than 32. That's a terrible reason. It just means our hardware sucks.  </blockquote><p>翻译过来就是：</p><blockquote>使用<b>大的batch size有害身体健康。</b>更重要的是，它对测试集的error不利。一个真正的朋友不会让你使用大于32的batch size。直说了吧：2012年来人们开始转而使用更大batch size的原因只是我们的GPU不够强大，处理小于32的batch size时效率太低。这是个糟糕的理由，只说明了我们的硬件还很辣鸡。  </blockquote><p>那是什么使得大牛LeCun同志对大于32的batch size如此深恶痛绝而发此论呢？  </p><p>细究出处可以发现这些评论是他读完<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1804.07612" class=" wrap external" target="_blank" rel="nofollow noreferrer">Revisiting Small Batch Training for Deep Neural Networks</a> 的感想，这篇论文对batch size（以及其他一些超参数）在深度学习中的选取做了详尽的分析并提供了实验数据。结果表明： </p><blockquote>The best performance has been consistently obtained for mini-batch sizes between m=2 and m=32, which contrasts with recent work advocating the use of mini-batch sizes in the thousands. </blockquote><p>也就是<b>最好的实验表现都是在batch size处于2~32之间</b>得到的，这和最近深度学习界论文中习惯的动辄上千的batch size选取有很大的出入。  </p><p class="ztext-empty-paragraph"><br></p><p>其实回想我们使用mini-batch技术的原因，无外乎是因为mini-batch有这几个好处 ：</p><ul><li>提高了运行效率，相比batch-GD的每个epoch只更新一次参数，使用mini-batch可以在一个epoch中多次更新参数，加速收敛。</li><li>解决了某些任务中，训练集过大，无法一次性读入内存的问题。</li><li>虽然第一点是mini-batch提出的最初始的原因，但是后来人们发现，使用mini-batch还有个好处，即每次更新时由于没有使用全量数据而仅仅使用batch内数据，从而人为给训练带来了噪声，而这个操作却往往能够带领算法<b>走出局部最优（鞍点）。</b>理论证明参见COLT的这篇论文Escaping From Saddle Points-Online Stochastic Gradient for Tensor Decomposition。也就是说，曾经我们使用mini-batch主要是为了加快收敛和节省内存，同时也带来每次更新有些“不准”的副作用，但是现在的观点来看，<b>这些“副作用”反而对我们的训练有着更多的增益，也变成mini-batch技术最主要的优点。</b>（有点像伟哥西地那非，最早是被发明出来治疗心血管疾病的，但是发现其副作用是容易使人勃起，最后反而是用来助勃，而不是用来治疗心血管了）</li></ul><p class="ztext-empty-paragraph"><br></p><p>综上所述，我们选取batch size时不妨这样操作：</p><ol><li>当有足够算力时，选取batch size为32或更小一些。</li><li>算力不够时，在效率和泛化性之间做trade-off，尽量选择更小的batch size。</li><li>当模型训练到尾声，想更精细化地提高成绩（比如论文实验/比赛到最后），有一个有用的trick，就是设置batch size为1，即做纯SGD，慢慢把error磨低。</li></ol><h2>参考文献：</h2><p>[1] Dominic Masters, Carlo Luschi,<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1804.07612" class=" wrap external" target="_blank" rel="nofollow noreferrer">Revisiting Small Batch Training for Deep Neural Networks</a>, arXiv:1804.07612v1</p><p>[2] Ge, R., Huang, F., Jin, C., &amp; Yuan, Y. (2015, June). Escaping From Saddle Points-Online Stochastic Gradient for Tensor Decomposition. In COLT (pp. 797-842).</p><p>[3] Bottou, L. (2010). Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT'2010 (pp. 177-186). Physica-Verlag HD.</p></span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/61607442/answer/440944387"><span data-tooltip="发布于 2018-07-12 22:59">编辑于 2018-07-18</span></a></div></div><div class="ContentItem-actions RichContent-actions"><span><button aria-label="赞同 44" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 44</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>16 条评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover19-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover19-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button><button data-zop-retract-question="true" type="button" class="Button ContentItem-action ContentItem-rightButton Button--plain"><span class="RichContent-collapsedText">收起</span><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--ArrowDown ContentItem-arrowIcon is-active" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M12 13L8.285 9.218a.758.758 0 0 0-1.064 0 .738.738 0 0 0 0 1.052l4.249 4.512a.758.758 0 0 0 1.064 0l4.246-4.512a.738.738 0 0 0 0-1.052.757.757 0 0 0-1.063 0L12.002 13z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="5" data-zop="{&quot;authorName&quot;:&quot;Jiecheng Zhao&quot;,&quot;itemId&quot;:204586969,&quot;title&quot;:&quot;怎么选取训练神经网络时的Batch size?&quot;,&quot;type&quot;:&quot;answer&quot;}" name="204586969" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="5" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;204586969&quot;,&quot;upvote_num&quot;:85,&quot;comment_num&quot;:19,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;61607442&quot;,&quot;author_member_hash_id&quot;:&quot;6c845d577eafd3269b54f6029d3bbfee&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="Jiecheng Zhao"><meta itemprop="image" content="https://pic4.zhimg.com/8ecd35d781e1933645af47e1df0e2bc9_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/jiecheng-zhao-66"><meta itemprop="zhihu:followerCount" content="314"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover20-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover20-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/jiecheng-zhao-66"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/8ecd35d781e1933645af47e1df0e2bc9_xs.jpg" srcset="https://pic4.zhimg.com/8ecd35d781e1933645af47e1df0e2bc9_l.jpg 2x" alt="Jiecheng Zhao"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover21-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover21-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/jiecheng-zhao-66">Jiecheng Zhao</a></div></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="ztext AuthorInfo-badgeText">身在金融业的神经网络炼丹师</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button type="button" class="Button Button--plain">85 人赞同了该回答</button></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="85"><meta itemprop="url" content="https://www.zhihu.com/question/61607442/answer/204586969"><meta itemprop="dateCreated" content="2017-07-27T09:43:06.000Z"><meta itemprop="dateModified" content="2017-07-28T03:32:45.000Z"><meta itemprop="commentCount" content="19"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text"><p><span><span class="UserLink"><div class="Popover"><div id="Popover25-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover25-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/02c1927a26f1fa47d3d43ec304b81015">夕小瑶</a></div></div></span></span> 说的很具体了。<br>我来简单说一下怎么操作吧，<br>1. Batch size设置以喂饱你的硬件为主要标准。只要显卡塞得下，首先挑大的。<br>2. 当感觉训练时噪音不够时，比如收敛碰到鞍点或者局部最小值时，调小batch size。（很少会碰到）<br>3. 当感觉训练时噪音太大时，调大batch size到喂饱硬件（因为2很少碰到，这也很少做），再不行就调小learning rate，也可以直接调小learning rate。</p><p>综合起来用就是常见的带learning rate下降的sgd。开始时依赖batch带来的噪音快速下降，接下来使用较低的learning rate消除这些噪音寻求稳定收敛。一般而言只要batch不太大，样本里的噪音总是够用的。</p><p>cpu是非常讨厌16，32，64…… 这样大小的数组的，gpu好像没有类似的问题，但我还是要劝大家，超参的选取随意点。</p><p>欠两个引用，等下再补。<br>---- 补引用<br><a href="https://link.zhihu.com/?target=https%3A//danluu.com/3c-conflict/" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">https://</span><span class="visible">danluu.com/3c-conflict/</span><span class="invisible"></span></a> 这个详细说了CPU为什么讨厌16，32，知乎上也有个回答说到过，自己也碰到过，蛮常见的。当年在high performance computing的课上学的，当时gpu还不流行，有谁能告诉我gpu的情况吗？谢谢啦。</p><br><a href="https://link.zhihu.com/?target=http%3A//www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">http://www.</span><span class="visible">jmlr.org/papers/volume1</span><span class="invisible">3/bergstra12a/bergstra12a.pdf</span><span class="ellipsis"></span></a> 告诉大家选超参要随意点</span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/61607442/answer/204586969"><span data-tooltip="发布于 2017-07-27 17:43">编辑于 2017-07-28</span></a></div></div><div class="ContentItem-actions RichContent-actions"><span><button aria-label="赞同 85" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 85</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>19 条评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover22-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover22-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button><button data-zop-retract-question="true" type="button" class="Button ContentItem-action ContentItem-rightButton Button--plain"><span class="RichContent-collapsedText">收起</span><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--ArrowDown ContentItem-arrowIcon is-active" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M12 13L8.285 9.218a.758.758 0 0 0-1.064 0 .738.738 0 0 0 0 1.052l4.249 4.512a.758.758 0 0 0 1.064 0l4.246-4.512a.738.738 0 0 0 0-1.052.757.757 0 0 0-1.063 0L12.002 13z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div></div></div></div></div></div></div></div></div></div><div class="Question-sideColumn Question-sideColumn--sticky" data-za-detail-view-path-module="RightSideBar" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null}}"><div><div class="Sticky is-fixed" style="width: 296px; top: -416px; left: 1155.5px;"><div class="Card AppBanner"><a class="AppBanner-link" href="http://zhi.hu/BDXoI"><div class="AppBanner-layout"><img class="AppBanner-qrcode" src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/sidebar-download-qrcode.7caef4dd.png" alt="QR Code of Downloading Zhihu App"><div class="AppBanner-content"><div class="AppBanner-title">下载知乎客户端</div><div class="AppBanner-description">与世界分享知识、经验和见解</div></div></div></a></div><div class="Card"></div><div class="Card" data-za-detail-view-path-module="RelatedQuestions" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null}}"><div class="Card-header SimilarQuestions-title"><div class="Card-headerText">相关问题</div></div><div class="Card-section SimilarQuestions-list"><div class="SimilarQuestions-item" itemprop="zhihu:similarQuestion" itemtype="http://schema.org/Question" itemscope="" data-za-detail-view-path-module="QuestionItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;29529707&quot;}}}"><meta itemprop="name" content="用deep learning的toolbox中的dbn训练自己的数据时，为什么只能分出一类？"><meta itemprop="url" content="https://www.zhihu.com/question/29529707"><meta itemprop="answerCount" content="18"><meta itemprop="zhihu:followerCount" content="48"><a target="_blank" type="button" class="Button Button--plain" href="https://www.zhihu.com/question/29529707">用deep learning的toolbox中的dbn训练自己的数据时，为什么只能分出一类？</a> 18 个回答</div><div class="SimilarQuestions-item" itemprop="zhihu:similarQuestion" itemtype="http://schema.org/Question" itemscope="" data-za-detail-view-path-module="QuestionItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;294058968&quot;}}}"><meta itemprop="name" content="为什么神经网络同一套参数 训练出来的准确率不一样？"><meta itemprop="url" content="https://www.zhihu.com/question/294058968"><meta itemprop="answerCount" content="11"><meta itemprop="zhihu:followerCount" content="68"><a target="_blank" type="button" class="Button Button--plain" href="https://www.zhihu.com/question/294058968">为什么神经网络同一套参数 训练出来的准确率不一样？</a> 11 个回答</div><div class="SimilarQuestions-item" itemprop="zhihu:similarQuestion" itemtype="http://schema.org/Question" itemscope="" data-za-detail-view-path-module="QuestionItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;296645982&quot;}}}"><meta itemprop="name" content="神经网络类机器学习如果样本不平衡会导致分类偏向一边，那如何选择预测应该为哪一类呢？"><meta itemprop="url" content="https://www.zhihu.com/question/296645982"><meta itemprop="answerCount" content="13"><meta itemprop="zhihu:followerCount" content="194"><a target="_blank" type="button" class="Button Button--plain" href="https://www.zhihu.com/question/296645982">神经网络类机器学习如果样本不平衡会导致分类偏向一边，那如何选择预测应该为哪一类呢？</a> 13 个回答</div><div class="SimilarQuestions-item" itemprop="zhihu:similarQuestion" itemtype="http://schema.org/Question" itemscope="" data-za-detail-view-path-module="QuestionItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;289211327&quot;}}}"><meta itemprop="name" content="卷积神经网络（CNN）适合处理不具有空间结构的数据吗？"><meta itemprop="url" content="https://www.zhihu.com/question/289211327"><meta itemprop="answerCount" content="12"><meta itemprop="zhihu:followerCount" content="120"><a target="_blank" type="button" class="Button Button--plain" href="https://www.zhihu.com/question/289211327">卷积神经网络（CNN）适合处理不具有空间结构的数据吗？</a> 12 个回答</div><div class="SimilarQuestions-item" itemprop="zhihu:similarQuestion" itemtype="http://schema.org/Question" itemscope="" data-za-detail-view-path-module="QuestionItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;298719271&quot;}}}"><meta itemprop="name" content="深度学习中有什么方法可以找出训练集中脏数据(比如标记错误的数据)？"><meta itemprop="url" content="https://www.zhihu.com/question/298719271"><meta itemprop="answerCount" content="8"><meta itemprop="zhihu:followerCount" content="284"><a target="_blank" type="button" class="Button Button--plain" href="https://www.zhihu.com/question/298719271">深度学习中有什么方法可以找出训练集中脏数据(比如标记错误的数据)？</a> 8 个回答</div></div></div><div class="Card" data-za-detail-view-path-module="ContentList" data-za-detail-view-path-module_name="相关推荐" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null}}"><div class="Card-header RelatedCommodities-title"><div class="Card-headerText">相关推荐</div></div><div class="Card-section RelatedCommodities-list"><a target="_blank" href="https://www.zhihu.com/pub/book/119607639" type="button" class="Button RelatedCommodities-item Button--plain" data-za-detail-view-path-module="EBookItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;EBook&quot;,&quot;token&quot;:&quot;119607639&quot;}}}"><img class="RelatedCommodities-image" src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/v2-2e09f6c92cf4968142b2ad94cc9c32a8_250x0.jpg" alt="live"><div class="RelatedCommodities-content"><div class="RelatedCommodities-subject RelatedCommodities-subject-two" data-tooltip="21个项目玩转深度学习——基于TensorFlow的实践详解">21个项目玩转深度学习——基于TensorFlow的实践详解</div><div class="RelatedCommodities-meta"><div class="RelatedCommodities-bookMeta">65 人读过<span class="RelatedCommodities-bookRead"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Ebook" width="13" height="14" fill="currentColor" viewBox="0 0 24 24"><path d="M16 17.649V2.931a.921.921 0 0 0-.045-.283.943.943 0 0 0-1.182-.604L4.655 5.235A.932.932 0 0 0 4 6.122v14.947c0 .514.421.931.941.931H19.06c.52 0 .941-.417.941-.93V7.292a.936.936 0 0 0-.941-.931h-.773v12.834a.934.934 0 0 1-.83.924L6.464 21.416c-.02.002 2.94-.958 8.883-2.881a.932.932 0 0 0 .653-.886z" fill-rule="evenodd"></path></svg></span>阅读</span></div></div></div></a><a target="_blank" href="https://www.zhihu.com/lives/977268690101739520" type="button" class="Button RelatedCommodities-item Button--plain" data-za-detail-view-path-module="LiveItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Live&quot;,&quot;id&quot;:&quot;977268690101739520&quot;,&quot;author_member_hash_id&quot;:&quot;64fb64a673f7ec3df5470e8d368b1cb6&quot;}}}"><img class="RelatedCommodities-image" src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/v2-3cea28d030d574fa2ccc7bbc02dfd8a1_250x0.jpg" alt="live"><div class="RelatedCommodities-content"><div class="RelatedCommodities-subject" data-tooltip="玩转深度学习|实战+原理">玩转深度学习|实战+原理</div><div class="RelatedCommodities-description">遇见更好的自己</div><div class="RelatedCommodities-meta"><div class="RelatedCommodities-scoreWrapper"><div class="Rating"><svg viewBox="0 0 15 15" class="Icon Icon--rating" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#ffab2e" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z">   </path></g></g></svg><svg viewBox="0 0 15 15" class="Icon Icon--rating" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#ffab2e" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z">   </path></g></g></svg><svg viewBox="0 0 15 15" class="Icon Icon--rating" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#ffab2e" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z">   </path></g></g></svg><svg viewBox="0 0 15 15" class="Icon Icon--rating" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#ffab2e" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z">   </path></g></g></svg><svg viewBox="0 0 15 15" class="Icon Icon--ratingNone" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#d7d8d9" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#d7d8d9" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z"></path>   </g></g></svg></div>134 人参与</div></div></div></a><a target="_blank" href="https://www.zhihu.com/pub/book/119582792" type="button" class="Button RelatedCommodities-item Button--plain" data-za-detail-view-path-module="EBookItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;EBook&quot;,&quot;token&quot;:&quot;119582792&quot;}}}"><img class="RelatedCommodities-image" src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/v2-a229daccd3a21fb8f5ecec2c14aa7061_250x0.jpg" alt="live"><div class="RelatedCommodities-content"><div class="RelatedCommodities-subject RelatedCommodities-subject-two" data-tooltip="TensorFlow 机器学习项目实战">TensorFlow 机器学习项目实战</div><div class="RelatedCommodities-meta"><div class="RelatedCommodities-bookMeta">220 人读过<span class="RelatedCommodities-bookRead"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Ebook" width="13" height="14" fill="currentColor" viewBox="0 0 24 24"><path d="M16 17.649V2.931a.921.921 0 0 0-.045-.283.943.943 0 0 0-1.182-.604L4.655 5.235A.932.932 0 0 0 4 6.122v14.947c0 .514.421.931.941.931H19.06c.52 0 .941-.417.941-.93V7.292a.936.936 0 0 0-.941-.931h-.773v12.834a.934.934 0 0 1-.83.924L6.464 21.416c-.02.002 2.94-.958 8.883-2.881a.932.932 0 0 0 .653-.886z" fill-rule="evenodd"></path></svg></span>阅读</span></div></div></div></a></div></div><div class="Card"></div><footer class="Footer"><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://liukanshan.zhihu.com/">刘看山</a><span class="Footer-dot"></span><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://www.zhihu.com/question/19581624">知乎指南</a><span class="Footer-dot"></span><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://www.zhihu.com/terms">知乎协议</a><span class="Footer-dot"></span><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://www.zhihu.com/term/privacy">知乎隐私保护指引</a><br><a class="Footer-item" target="_blank" href="https://www.zhihu.com/app">应用</a><span class="Footer-dot"></span><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://app.mokahr.com/apply/zhihu">工作</a><span class="Footer-dot"></span><button type="button" class="Button OrgCreateButton">申请开通知乎机构号</button><br><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://zhuanlan.zhihu.com/p/51068775">侵权举报</a><span class="Footer-dot"></span><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="http://www.12377.cn/">网上有害信息举报专区</a><br><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://tsm.miit.gov.cn/dxxzsp/">京 ICP 证 110745 号</a><br><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=11010802020088"><img src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/v2-d0289dc0a46fc5b15b3363ffa78cf6c7.png">京公网安备 11010802010035 号</a><br><span class="Footer-item">违法和不良信息举报：010-82716601</span><br><a class="Footer-item" target="_blank" href="https://www.zhihu.com/jubao">儿童色情信息举报专区</a><br><a class="Footer-item" target="_blank" href="https://www.zhihu.com/certificates">证照中心</a><br><a class="Footer-item" target="_blank" href="https://www.zhihu.com/contact">联系我们</a><span> © 2019 知乎</span></footer></div><div class="Sticky--holder" style="position: static; top: auto; right: auto; bottom: auto; left: auto; display: block; float: none; margin: 0px; height: 1060px;"></div></div></div></div></div></main><div data-zop-usertoken="{}"></div><div class="CornerButtons"><div class="CornerAnimayedFlex"><button data-tooltip="建议反馈" data-tooltip-position="left" data-tooltip-will-hide-on-click="true" aria-label="建议反馈" type="button" class="Button CornerButton Button--plain"><svg class="Zi Zi--Feedback" title="建议反馈" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M19.99 6.99L18 5s-1-1-2-1H8C7 4 6 5 6 5L4 7S3 8 3 9v9s0 2 2.002 2H19c2 0 2-2 2-2V9c0-1-1.01-2.01-1.01-2.01zM16.5 5.5L19 8H5l2.5-2.5h9zm-2 5.5s.5 0 .5.5-.5.5-.5.5h-5s-.5 0-.5-.5.5-.5.5-.5h5z"></path></svg></button></div><div class="CornerAnimayedFlex"><button data-tooltip="回到顶部" data-tooltip-position="left" data-tooltip-will-hide-on-click="true" aria-label="回到顶部" type="button" class="Button CornerButton Button--plain"><svg class="Zi Zi--BackToTop" title="回到顶部" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M16.036 19.59a1 1 0 0 1-.997.995H9.032a.996.996 0 0 1-.997-.996v-7.005H5.03c-1.1 0-1.36-.633-.578-1.416L11.33 4.29a1.003 1.003 0 0 1 1.412 0l6.878 6.88c.782.78.523 1.415-.58 1.415h-3.004v7.005z"></path></svg></button></div></div></div></div><script id="js-clientConfig" type="text/json">{"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","zhuanlanHost":"zhuanlan.zhihu.com","apiHost":"api.zhihu.com"}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"question\u002Fget\u002F":false,"question\u002FgetAnswers\u002F61607442":false}},"entities":{"users":{},"questions":{"61607442":{"type":"question","id":61607442,"title":"怎么选取训练神经网络时的Batch size?","questionType":"normal","created":1498436644,"updatedTime":1501141798,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F61607442","isMuted":false,"isNormal":true,"isEditable":false,"adminClosedComment":false,"hasPublishingDraft":false,"answerCount":13,"visitCount":86829,"commentCount":0,"followerCount":978,"collapsedAnswerCount":0,"excerpt":"batch_size的选择和训练数据规模、神经网络层数、单元数有什么关系？谢谢！","commentPermission":"all","detail":"batch_size的选择和训练数据规模、神经网络层数、单元数有什么关系？谢谢！","editableDetail":"batch_size的选择和训练数据规模、神经网络层数、单元数有什么关系？谢谢！","status":{"isLocked":false,"isClose":false,"isEvaluate":false,"isSuggest":false},"relationship":{"isAuthor":false,"isFollowing":false,"isAnonymous":false,"canLock":false,"canStickAnswers":false,"canCollapseAnswers":false},"topics":[{"id":"19607065","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19607065","name":"神经网络","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002F6d0516fb4_is.jpg"},{"id":"19813032","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19813032","name":"深度学习（Deep Learning）","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002F5d3c206139ca2124997418db09b0bb11_is.jpg"},{"id":"20032249","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F20032249","name":"TensorFlow","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002F128599f9313a18bfbb109e9ba375a81d_is.jpg"}],"author":{"id":"a551ba800ff7b68ff5677afe9a3fc944","urlToken":"","name":"知乎用户","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fda8e974dc_is.jpg","avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fda8e974dc_{size}.jpg","isOrg":false,"type":"people","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fpeople\u002F0","userType":"people","headline":"","badge":[],"gender":1,"isAdvertiser":false,"isPrivacy":true},"canComment":{"status":true,"reason":""},"reviewInfo":{"type":"","tips":"","editTips":"","isReviewing":false},"relatedCards":[],"muteInfo":{"type":""},"showAuthor":false}},"answers":{"192204021":{"id":192204021,"type":"answer","answerType":"normal","question":{"type":"question","id":61607442,"title":"怎么选取训练神经网络时的Batch size?","questionType":"normal","created":1498436644,"updatedTime":1501141798,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F61607442","relationship":{}},"author":{"id":"6eab0ad9ca59e60ed71ae99e209f1a32","urlToken":"","name":"知乎用户","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fda8e974dc_is.jpg","avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fda8e974dc_{size}.jpg","isOrg":false,"type":"people","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fpeople\u002F0","userType":"people","headline":"","badge":[],"gender":1,"isAdvertiser":false,"followerCount":932,"isFollowed":false,"isPrivacy":true},"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fanswers\u002F192204021","isCollapsed":false,"createdTime":1498930749,"updatedTime":1499107807,"extras":"","isCopyable":true,"isNormal":true,"voteupCount":45,"commentCount":8,"isSticky":false,"adminClosedComment":false,"commentPermission":"all","canComment":{"reason":"","status":true},"reshipmentSettings":"allowed","content":"\u003Cp\u003Ebatch size 不是永远越大越好。\u003C\u002Fp\u003E\u003Cp\u003E更大的 batch size 更能准确地计算出梯度，mini-batch 只是用更少的计算来近似梯度。直觉上好像更大的 batch 更好因为更“准确”。\u003C\u002Fp\u003E\u003Cp\u003E实践结果比较反直觉，因为深度模型的代价函数非常不平整，完整的梯度也只能往局部极小值跑，所以大 batch size 带来的准确的梯度可能也没什么用。反而有时候小 batch size 带来的噪声可以更快的找到不错的极小值。\u003C\u002Fp\u003E\u003Cp\u003E但是，更大的 batch size 在并行计算上更占优势，太小的 batch-size 发挥不了 gpu 的性能。所以当我们觉得 batch 要更大，主要是为了更好的并行，而不是为了更精确的梯度。 \u003C\u002Fp\u003E\u003Cp\u003E在深度学习里好像追求精确意义不大，比如更高精度的浮点数并没有什么帮助，还不如减小精度来换取计算速度。比如调参时 ramdom search 居然不比 grid search 表现差，非常反直觉。\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\u003Cp\u003E补充：\u003C\u002Fp\u003E\u003Cp\u003E@dengdan在评论里指出，小batch训练的稳定性较差。小batch确实有这个缺点，而且对设置学习速率有更高的要求，否则可能引起恶性的震荡无法收敛。但是小batch的优点仍然是显著的，DL书里建议使用逐步增加的batch size来兼并两者的优点。\u003C\u002Fp\u003E","editableContent":"","excerpt":"batch size 不是永远越大越好。 更大的 batch size 更能准确地计算出梯度，mini-batch 只是用更少的计算来近似梯度。直觉上好像更大的 batch 更好因为更“准确”。 实践结果比较反直觉，因为深度模型的代价函数非常不平整，完整的梯度也只能往局部极小值跑，所以大 batch size 带来的准确的梯度可能也没什么用。反而有时候小 batch size 带来的噪声可以更快的找到不错的极小值。 但是，更大的 batch size 在并行计算上更占优势，…","collapsedBy":"nobody","collapseReason":"","annotationAction":null,"markInfos":[],"relevantInfo":{"isRelevant":false,"relevantType":"","relevantText":""},"suggestEdit":{"reason":"","status":false,"tip":"","title":"","unnormalDetails":{"status":"","description":"","reason":"","reasonId":0,"note":""},"url":""},"isLabeled":false,"rewardInfo":{"canOpenReward":false,"isRewardable":false,"rewardMemberCount":0,"rewardTotalMoney":0,"tagline":""},"relationship":{"isAuthor":false,"isAuthorized":false,"isNothelp":false,"isThanked":false,"isRecognized":false,"voting":0,"upvotedFollowees":[]}},"204525634":{"id":204525634,"type":"answer","answerType":"normal","question":{"type":"question","id":61607442,"title":"怎么选取训练神经网络时的Batch size?","questionType":"normal","created":1498436644,"updatedTime":1501141798,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F61607442","relationship":{}},"author":{"id":"02c1927a26f1fa47d3d43ec304b81015","urlToken":"tsxiyao","name":"夕小瑶","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-70ae276a4834426ad670577b953f4fbb_is.jpg","avatarUrlTemplate":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-70ae276a4834426ad670577b953f4fbb_{size}.jpg","isOrg":false,"type":"people","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fpeople\u002F02c1927a26f1fa47d3d43ec304b81015","userType":"people","headline":"NLPer \u002F MtF \u002F 订阅号“夕小瑶的卖萌屋” \u002F 仙女有毒，慎撩","badge":[{"type":"best_answerer","description":"优秀回答者","topics":[{"id":"19559450","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19559450","name":"机器学习","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fd3dd87a0feae0a3db82973157eee89c0_is.jpg"}]}],"gender":0,"isAdvertiser":false,"followerCount":14346,"isFollowed":false,"isPrivacy":false},"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fanswers\u002F204525634","isCollapsed":false,"createdTime":1501141682,"updatedTime":1501141683,"extras":"","isCopyable":false,"isNormal":true,"voteupCount":218,"commentCount":20,"isSticky":false,"adminClosedComment":false,"commentPermission":"all","canComment":{"reason":"","status":true},"reshipmentSettings":"need_payment","content":"\u003Cp\u003E谢邀。邀请我的一定是订阅号的粉丝，2333。\u003C\u002Fp\u003E\u003Cp\u003E这个问题我已经详细写过了，搬过来。\u003C\u002Fp\u003E\u003Cp\u003E原链接：\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fmp.weixin.qq.com\u002Fs%3F__biz%3DMzIwNzc2NTk0NQ%3D%3D%26mid%3D2247484570%26idx%3D1%26sn%3D4c0b6b76a7f2518d77818535b677e087%26chksm%3D970c2c4ca07ba55ad5cfe6b46f72dbef85a159574fb60b9932404e45747c95eed8c6c0f66d62%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E训练神经网络时如何确定batch的大小？\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch2\u003E前言\u003C\u002Fh2\u003E\u003Cp\u003E当我们要训练一个已经写好的神经网络时，我们就要直面诸多的超参数了。这些超参数一旦选不好，那么很有可能让神经网络跑的还不如感知机。因此在面对神经网络这种容量很大的model前，是很有必要深刻的理解一下各个超参数的意义及其对model的影响的。\u003C\u002Fp\u003E\u003Ch2\u003E回顾\u003C\u002Fh2\u003E\u003Cp\u003E简单回顾一下神经网络的\u003Cb\u003E一次\u003C\u002Fb\u003E迭代过程：\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-2e9e673571e3a56728798e3b15777afd_hd.jpg\" data-rawwidth=\"1729\" data-rawheight=\"1120\" class=\"origin_image zh-lightbox-thumb\" width=\"1729\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-2e9e673571e3a56728798e3b15777afd_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1729&#39; height=&#39;1120&#39;&gt;&lt;\u002Fsvg&gt;\" data-rawwidth=\"1729\" data-rawheight=\"1120\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1729\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-2e9e673571e3a56728798e3b15777afd_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-2e9e673571e3a56728798e3b15777afd_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E即，首先选择n个样本组成一个batch，然后将batch丢进神经网络，得到输出结果。再将输出结果与样本label丢给loss函数算出本轮的loss，而后就可以愉快的跑BP算法了（从后往前逐层计算参数之于loss的导数）。最后将每个参数的导数配合步长参数来进行参数更新。这就是训练过程的一次迭代。\u003C\u002Fp\u003E\u003Ch2\u003EBatch Size\u003C\u002Fh2\u003E\u003Cp\u003E由此，最直观的超参数就是batch的大小——我们可以一次性将整个数据集喂给神经网络，让神经网络利用全部样本来计算迭代时的梯度（即传统的梯度下降法），也可以一次只喂一个样本（即随机梯度下降法，也称在线梯度下降法），也可以取个折中的方案，即每次喂一部分样本让其完成本轮迭代（即batch梯度下降法）。\u003C\u002Fp\u003E\u003Cp\u003E数学基础不太好的初学者可能在这里犯迷糊——一次性喂500个样本并迭代一次，跟一次喂1个样本迭代500次相比，有区别吗？\u003C\u002Fp\u003E\u003Cp\u003E其实这两个做法就相当于：\u003C\u002Fp\u003E\u003Cp\u003E第一种：\u003Cbr\u002F\u003Etotal = 旧参下计算更新值1+旧参下计算更新值2+...+旧参下计算更新值500 ;\u003Cbr\u002F\u003E新参数 = 旧参数 + total\u003C\u002Fp\u003E\u003Cp\u003E第二种：\u003Cbr\u002F\u003E新参数1 = 旧参数 + 旧参数下计算更新值1；\u003Cbr\u002F\u003E新参数2 = 新参数1 + 新参数1下计算更新值1；\u003Cbr\u002F\u003E新参数3 = 新参数2 + 新参数2下计算更新值1；\u003Cbr\u002F\u003E...\u003Cbr\u002F\u003E新参数500 = 新参数500 + 新参数500下计算更新值1；\u003C\u002Fp\u003E\u003Cp\u003E也就是说，第一种是将参数一次性更新500个样本的量，第二种是迭代的更新500次参数。当然是不一样的啦。\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E那么问题来了，哪个更好呢？\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch2\u003EWhich one？\u003C\u002Fh2\u003E\u003Cp\u003E我们首先分析最简单的影响，哪种做法收敛更快呢？\u003C\u002Fp\u003E\u003Cp\u003E我们假设每个样本相对于大自然真实分布的标准差为σ，那么根据概率统计的知识，很容易推出n个样本的标准差为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csigma%2F%5Csqrt%7Bn%7D\" alt=\"\\sigma\u002F\\sqrt{n}\" eeimg=\"1\"\u002F\u003E （有疑问的同学快翻开概率统计的课本看一下推导过程）。从这里可以看出，我们使用样本来估计梯度的时候，1个样本带来σ的标准差，但是使用n个样本区估计梯度并不能让标准差线性降低（也就是并不能让误差降低为原来的1\u002Fn，即无法达到σ\u002Fn），而n个样本的计算量却是线性的（每个样本都要平等的跑一遍前向算法）。\u003C\u002Fp\u003E\u003Cp\u003E由此看出，显然在同等的计算量之下（一定的时间内），使用整个样本集的收敛速度要远慢于使用少量样本的情况。换句话说，要想收敛到同一个最优点，使用整个样本集时，虽然迭代次数少，但是每次迭代的时间长，耗费的总时间是大于使用少量样本多次迭代的情况的。\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E那么是不是样本越少，收敛越快呢？\u003C\u002Fp\u003E\u003Cp\u003E理论上确实是这样的，使用单个单核cpu的情况下也确实是这样的。但是我们要与工程实际相结合呀~实际上，工程上在使用GPU训练时，跑一个样本花的时间与跑几十个样本甚至几百个样本的时间是一样的！当然得益于GPU里面超多的核，超强的并行计算能力啦。因此，在工程实际中，从收敛速度的角度来说，小批量的样本集是最优的，也就是我们所说的mini-batch。这时的batch size往往从几十到几百不等，但一般不会超过几千（你有土豪显卡的话，当我没说）。\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E那么，如果我真有一个怪兽级显卡，使得一次计算10000个样本跟计算1个样本的时间相同的话，是不是设置10000就一定是最好的呢？虽然从收敛速度上来说是的，但！是！\u003C\u002Fp\u003E\u003Cp\u003E我们知道，神经网络是个复杂的model，它的损失函数也不是省油的灯，在实际问题中，神经网络的loss曲面（以model参数为自变量，以loss值为因变量画出来的曲面）往往是非凸的，这意味着很可能有多个局部最优点，而且很可能有鞍点！\u003C\u002Fp\u003E\u003Cp\u003E插播一下，鞍点就是loss曲面中像马鞍一样形状的地方的中心点，如下图：\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-817cf83b6e9b5da3859457cee2d70215_hd.jpg\" data-rawwidth=\"1003\" data-rawheight=\"549\" class=\"origin_image zh-lightbox-thumb\" width=\"1003\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-817cf83b6e9b5da3859457cee2d70215_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1003&#39; height=&#39;549&#39;&gt;&lt;\u002Fsvg&gt;\" data-rawwidth=\"1003\" data-rawheight=\"549\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1003\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-817cf83b6e9b5da3859457cee2d70215_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-817cf83b6e9b5da3859457cee2d70215_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E（图片来自《Deep Learning》）\u003C\u002Fp\u003E\u003Cp\u003E想象一下，在鞍点处，横着看的话，鞍点就是个极小值点，但是竖着看的话，鞍点就是极大值点（线性代数和最优化算法过关的同学应该能反应过来，鞍点处的Hessian矩阵的特征值有正有负。不理解也没关系，小夕过几天就开始写最优化的文章啦~），因此鞍点容易给优化算法一个“我已经收敛了”的假象，殊不知其旁边有一个可以跳下去的万丈深渊。。。（可怕）\u003C\u002Fp\u003E\u003Cp\u003E回到主线上来，小夕在\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fmp.weixin.qq.com\u002Fs%3F__biz%3DMzIwNzc2NTk0NQ%3D%3D%26mid%3D2247483887%26idx%3D1%26sn%3De402f45fd2eaccf5bccf2b348d2f85c5%26chksm%3D970c2939a07ba02f53b125708b4d5069d2ad6d5b0e8b95d27e15ae36694814470e9d2bd7a9e1%26scene%3D21%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E《机器学习入门指导（4）》\u003C\u002Fa\u003E中提到过，传统的最优化算法是无法自动的避开局部最优点的，对于鞍点也是理论上很头疼的东西。但是实际上，工程中却不怎么容易陷入很差劲的局部最优点或者鞍点，这是为什么呢？\u003C\u002Fp\u003E\u003Cp\u003E暂且不说一些很高深的理论如“神经网络的loss曲面中的局部最优点与全局最优点差不太多”，我们就从最简单的角度想~\u003C\u002Fp\u003E\u003Cp\u003E想一想，样本量少的时候会带来很大的方差，而这个大方差恰好会导致我们在梯度下降到很差的局部最优点（只是微微凸下去的最优点）和鞍点的时候不稳定，一不小心就因为一个大噪声的到来导致炸出了局部最优点，或者炸下了马（此处请保持纯洁的心态！），从而有机会去寻找更优的最优点。\u003C\u002Fp\u003E\u003Cp\u003E因此，与之相反的，当样本量很多时，方差很小（咦？最开始的时候好像在说标准差来着，反正方差与标准差就差个根号，没影响的哈~），对梯度的估计要准确和稳定的多，因此反而在差劲的局部最优点和鞍点时反而容易自信的呆着不走了，从而导致神经网络收敛到很差的点上，跟出了bug一样的差劲。\u003C\u002Fp\u003E\u003Cp\u003E小总结一下，batch的size设置的不能太大也不能太小，因此实际工程中最常用的就是mini-batch，一般size设置为几十或者几百。但是！\u003C\u002Fp\u003E\u003Cp\u003E好像这篇文章的转折有点多了诶。。。\u003C\u002Fp\u003E\u003Cp\u003E细心的读者可能注意到了，这之前我们的讨论是基于梯度下降的，而且默认是一阶的（即没有利用二阶导数信息，仅仅使用一阶导数去优化）。因此对于SGD（随机梯度下降）及其改良的一阶优化算法如Adagrad、Adam等是没问题的，但是对于强大的二阶优化算法如共轭梯度法、L-BFGS来说，如果估计不好一阶导数，那么对二阶导数的估计会有更大的误差，这对于这些算法来说是致命的。\u003C\u002Fp\u003E\u003Cp\u003E因此，对于二阶优化算法，减小batch换来的收敛速度提升远不如引入大量噪声导致的性能下降，因此在使用二阶优化算法时，往往要采用大batch哦。此时往往batch设置成几千甚至一两万才能发挥出最佳性能。\u003C\u002Fp\u003E\u003Cp\u003E另外，听说GPU对2的幂次的batch可以发挥更佳的性能，因此设置成16、32、64、128...时往往要比设置为整10、整100的倍数时表现更优（不过我没有验证过，有兴趣的同学可以试验一下~\u003C\u002Fp\u003E","editableContent":"","excerpt":"谢邀。邀请我的一定是订阅号的粉丝，2333。 这个问题我已经详细写过了，搬过来。 原链接： 训练神经网络时如何确定batch的大小？ 前言当我们要训练一个已经写好的神经网络时，我们就要直面诸多的超参数了。这些超参数一旦选不好，那么很有可能让神经网络跑的还不如感知机。因此在面对神经网络这种容量很大的model前，是很有必要深刻的理解一下各个超参数的意义及其对model的影响的。 回顾简单回顾一下神经网络的 一次迭代过程： [图片] …","collapsedBy":"nobody","collapseReason":"","annotationAction":null,"markInfos":[],"relevantInfo":{"isRelevant":false,"relevantType":"","relevantText":""},"suggestEdit":{"reason":"","status":false,"tip":"","title":"","unnormalDetails":{"status":"","description":"","reason":"","reasonId":0,"note":""},"url":""},"isLabeled":false,"rewardInfo":{"canOpenReward":false,"isRewardable":false,"rewardMemberCount":0,"rewardTotalMoney":0,"tagline":""},"relationship":{"isAuthor":false,"isAuthorized":false,"isNothelp":false,"isThanked":false,"isRecognized":false,"voting":0,"upvotedFollowees":[]}},"204586969":{"id":204586969,"type":"answer","answerType":"normal","question":{"type":"question","id":61607442,"title":"怎么选取训练神经网络时的Batch size?","questionType":"normal","created":1498436644,"updatedTime":1501141798,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F61607442","relationship":{}},"author":{"id":"6c845d577eafd3269b54f6029d3bbfee","urlToken":"jiecheng-zhao-66","name":"Jiecheng Zhao","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002F8ecd35d781e1933645af47e1df0e2bc9_is.jpg","avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002F8ecd35d781e1933645af47e1df0e2bc9_{size}.jpg","isOrg":false,"type":"people","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fpeople\u002F6c845d577eafd3269b54f6029d3bbfee","userType":"people","headline":"身在金融业的神经网络炼丹师","badge":[],"gender":1,"isAdvertiser":false,"followerCount":314,"isFollowed":false,"isPrivacy":false},"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fanswers\u002F204586969","isCollapsed":false,"createdTime":1501148586,"updatedTime":1501212765,"extras":"","isCopyable":true,"isNormal":true,"voteupCount":85,"commentCount":19,"isSticky":false,"adminClosedComment":false,"commentPermission":"all","canComment":{"reason":"","status":true},"reshipmentSettings":"allowed","content":"\u003Ca href=\"https:\u002F\u002Fwww.zhihu.com\u002Fpeople\u002F02c1927a26f1fa47d3d43ec304b81015\" data-hash=\"02c1927a26f1fa47d3d43ec304b81015\" class=\"member_mention\" data-hovercard=\"p$b$02c1927a26f1fa47d3d43ec304b81015\"\u003E@夕小瑶\u003C\u002Fa\u003E 说的很具体了。\u003Cbr\u002F\u003E我来简单说一下怎么操作吧，\u003Cbr\u002F\u003E1. Batch size设置以喂饱你的硬件为主要标准。只要显卡塞得下，首先挑大的。\u003Cbr\u002F\u003E2. 当感觉训练时噪音不够时，比如收敛碰到鞍点或者局部最小值时，调小batch size。（很少会碰到）\u003Cbr\u002F\u003E3. 当感觉训练时噪音太大时，调大batch size到喂饱硬件（因为2很少碰到，这也很少做），再不行就调小learning rate，也可以直接调小learning rate。\u003Cbr\u002F\u003E\u003Cbr\u002F\u003E综合起来用就是常见的带learning rate下降的sgd。开始时依赖batch带来的噪音快速下降，接下来使用较低的learning rate消除这些噪音寻求稳定收敛。一般而言只要batch不太大，样本里的噪音总是够用的。\u003Cbr\u002F\u003E\u003Cbr\u002F\u003Ecpu是非常讨厌16，32，64…… 这样大小的数组的，gpu好像没有类似的问题，但我还是要劝大家，超参的选取随意点。\u003Cbr\u002F\u003E\u003Cbr\u002F\u003E欠两个引用，等下再补。\u003Cbr\u002F\u003E---- 补引用\u003Cbr\u002F\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fdanluu.com\u002F3c-conflict\u002F\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttps:\u002F\u002F\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Edanluu.com\u002F3c-conflict\u002F\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E 这个详细说了CPU为什么讨厌16，32，知乎上也有个回答说到过，自己也碰到过，蛮常见的。当年在high performance computing的课上学的，当时gpu还不流行，有谁能告诉我gpu的情况吗？谢谢啦。\u003Cbr\u002F\u003E\u003Cbr\u002F\u003E\u003Cbr\u002F\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fwww.jmlr.org\u002Fpapers\u002Fvolume13\u002Fbergstra12a\u002Fbergstra12a.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttp:\u002F\u002Fwww.\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Ejmlr.org\u002Fpapers\u002Fvolume1\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003E3\u002Fbergstra12a\u002Fbergstra12a.pdf\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E 告诉大家选超参要随意点","editableContent":"","excerpt":"@夕小瑶 说的很具体了。 我来简单说一下怎么操作吧， 1. Batch size设置以喂饱你的硬件为主要标准。只要显卡塞得下，首先挑大的。 2. 当感觉训练时噪音不够时，比如收敛碰到鞍点或者局部最小值时，调小batch size。（很少会碰到） 3. 当感觉训练时噪音太大时，调大batch size到喂饱硬件（因为2很少碰到，这也很少做），再不行就调小learning rate，也可以直接调小learning rate。 综合起来用就是常见的带learning rate下降的sgd…","collapsedBy":"nobody","collapseReason":"","annotationAction":null,"markInfos":[],"relevantInfo":{"isRelevant":false,"relevantType":"","relevantText":""},"suggestEdit":{"reason":"","status":false,"tip":"","title":"","unnormalDetails":{"status":"","description":"","reason":"","reasonId":0,"note":""},"url":""},"isLabeled":false,"rewardInfo":{"canOpenReward":false,"isRewardable":false,"rewardMemberCount":0,"rewardTotalMoney":0,"tagline":""},"relationship":{"isAuthor":false,"isAuthorized":false,"isNothelp":false,"isThanked":false,"isRecognized":false,"voting":0,"upvotedFollowees":[]}},"440401209":{"id":440401209,"type":"answer","answerType":"normal","question":{"type":"question","id":61607442,"title":"怎么选取训练神经网络时的Batch size?","questionType":"normal","created":1498436644,"updatedTime":1501141798,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F61607442","relationship":{}},"author":{"id":"4eedf55cf1de9f94173b352f9c5a8d2b","urlToken":"YJango","name":"YJango","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ff764e30ee9e36f4b357eea4b925bb22_is.jpg","avatarUrlTemplate":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ff764e30ee9e36f4b357eea4b925bb22_{size}.jpg","isOrg":false,"type":"people","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fpeople\u002F4eedf55cf1de9f94173b352f9c5a8d2b","userType":"people","headline":"分享如何用人工智能思维提高自身能力","badge":[{"type":"identity","description":"日本会津大学 人机界面实验室博士在读","topics":[]},{"type":"best_answerer","description":"优秀回答者","topics":[{"id":"19559450","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19559450","name":"机器学习","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fd3dd87a0feae0a3db82973157eee89c0_is.jpg"},{"id":"19813032","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19813032","name":"深度学习（Deep Learning）","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002F5d3c206139ca2124997418db09b0bb11_is.jpg"}]}],"gender":1,"isAdvertiser":false,"followerCount":229420,"isFollowed":false,"isPrivacy":false},"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fanswers\u002F440401209","isCollapsed":false,"createdTime":1531366092,"updatedTime":1539070031,"extras":"","isCopyable":false,"isNormal":true,"voteupCount":583,"commentCount":47,"isSticky":false,"adminClosedComment":false,"commentPermission":"all","canComment":{"reason":"","status":true},"reshipmentSettings":"need_payment","content":"\u003Ch2\u003E目录\u003C\u002Fh2\u003E\u003Cul\u003E\u003Cli\u003Ebatch size 需要调参\u003C\u002Fli\u003E\u003Cli\u003E个人经验分享\u003C\u002Fli\u003E\u003Cli\u003E为什么 batch size 会影响训练结果\u003C\u002Fli\u003E\u003Cli\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fplayground.tensorflow.org\u002F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003ETensorflow Playground\u003C\u002Fa\u003E 演示\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Chr\u002F\u003E\u003Ch2\u003Ebatch size 需要\u003Cb\u003E调参\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E可以确定的是，\u003Cb\u003Ebatch size 绝非越大越好\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Cp\u003E因为 batch size 的极限是训练集样本总个数，而这是当初神经网络还未如此之火时的训练方式 Gradient Descent (GD)。\u003C\u002Fp\u003E\u003Cp\u003E2014 年初我上的人工神经网络 (还不叫深度学习，非常旧的教学材料) 课程，老师留的作业就是用自己写个神经网络去实现 4 输入\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fwenku.baidu.com\u002Fview\u002Fbba0bf1090c69ec3d5bb7537.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E异或门\u003C\u002Fa\u003E。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002Fv2-2ba1c4a00fa00982dd27b4f879e31211_hd.jpg\" data-size=\"normal\" data-rawwidth=\"218\" data-rawheight=\"203\" data-default-watermark-src=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-03a4161ef84e407b821ba9568d9ea200_hd.jpg\" class=\"content_image\" width=\"218\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;218&#39; height=&#39;203&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"218\" data-rawheight=\"203\" data-default-watermark-src=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-03a4161ef84e407b821ba9568d9ea200_hd.jpg\" class=\"content_image lazy\" width=\"218\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002Fv2-2ba1c4a00fa00982dd27b4f879e31211_hd.jpg\"\u002F\u003E\u003Cfigcaption\u003E3 输入异或门\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E中间层的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=W\" alt=\"W\" eeimg=\"1\"\u002F\u003E 的梯度是纯粹用解析公式计算而得 (不是如今的计算图)，激活函数是 sigmoid (不是 Relu)，而且每次更新都使用所有的 16 个样本，就是单纯的去完全拟合训练集，只是为了查看神经网络拟合任意函数的能力 (这很不现实，因为如果知道所有样本，压根就不需要去学习。现实的任务，是需要从有限的训练样本中训练出可以用于预测测试数据的模型)。\u003C\u002Fp\u003E\u003Cp\u003E下面是我当时交作业的 loss 下降图。可以看到 loss 呈阶梯式下降。当时被解读为是陷入局部最小值。22000次 (横坐标) 迭代才能够拟合 (纵坐标是 loss 大小)，用全部样本作为 batch size 的 Gradient Descent (GD) 效果是很差的。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-4f584772442b50916a5fcc71e5394dde_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"406\" data-rawheight=\"361\" data-default-watermark-src=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fv2-239c27a037fdc88f8ea020c07bbb618d_hd.jpg\" class=\"content_image\" width=\"406\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;406&#39; height=&#39;361&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"406\" data-rawheight=\"361\" data-default-watermark-src=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fv2-239c27a037fdc88f8ea020c07bbb618d_hd.jpg\" class=\"content_image lazy\" width=\"406\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-4f584772442b50916a5fcc71e5394dde_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E深度学习和以前的人工神经网络差在哪里了？为什么近几年突然变成各个媒体吹上天的神话？事实上深度学习的核心技术在二三十年前就有，是各个技术组合方式的才造成了今天的差异 (也因如此，刚入门的伙伴很容易因某个细节不同甚至无法收敛)。这可以从 keras, lasagne 等库包的文档分类注意到\u003Cb\u003E深度学习的跃进来源于不同技术的组合\u003C\u002Fb\u003E：\u003Cb\u003E层\u003C\u002Fb\u003E、\u003Cb\u003E梯度更新方式\u003C\u002Fb\u003E、\u003Cb\u003E初始化方式\u003C\u002Fb\u003E、\u003Cb\u003E非线性\u003C\u002Fb\u003E、\u003Cb\u003E目标函数\u003C\u002Fb\u003E、\u003Cb\u003E正规项\u003C\u002Fb\u003E等。(研究时也会分别去提升不同的技术细节)\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-062251e9c131082c263b8a6eb1bb7f14_hd.jpg\" data-size=\"normal\" data-rawwidth=\"157\" data-rawheight=\"175\" class=\"content_image\" width=\"157\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;157&#39; height=&#39;175&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"157\" data-rawheight=\"175\" class=\"content_image lazy\" width=\"157\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-062251e9c131082c263b8a6eb1bb7f14_hd.jpg\"\u002F\u003E\u003Cfigcaption\u003Elasagne 文档分类\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E现在人们分析深度学习崛起原因时人们常会说：因为大数据、GPU 的高并行计算能力、ReLU，等等，但还有一个常被忽略，简单却又意外强大的因素就是 Gradient Descent (GD) 替换成了 Stochastic Gradient Descent (SGD)。随后又有大批的弥补 朴素SGD 不足的更新算法，也就是在 \u003Cb\u003E梯度更新方式\u003C\u002Fb\u003E 这一类。可以参考这篇文章 \u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fruder.io\u002Foptimizing-gradient-descent\u002F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EAn overview of gradient descent optimization algorithms\u003C\u002Fa\u003E。\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003EGradient Descent：所有样本算出的梯度的平均值来更新每一步\u003C\u002Fli\u003E\u003Cli\u003EStochastic Gradient Descent：一个样本算出的梯度来更新每一步\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003E虽然从\u003Cb\u003E直觉上\u003C\u002Fb\u003E，Gradient Descent 可以快速准确的将训练集的 loss 降低到最小，但实际上很容易陷入局部最小值或鞍点 (不知道什么是鞍点的看 \u003Ca class=\"member_mention\" href=\"https:\u002F\u002Fwww.zhihu.com\u002Fpeople\u002F02c1927a26f1fa47d3d43ec304b81015\" data-hash=\"02c1927a26f1fa47d3d43ec304b81015\" data-hovercard=\"p$b$02c1927a26f1fa47d3d43ec304b81015\"\u003E@夕小瑶Elsa\u003C\u002Fa\u003E 的 \u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fmp.weixin.qq.com\u002Fs%3F__biz%3DMzIwNzc2NTk0NQ%3D%3D%26mid%3D2247484570%26idx%3D1%26sn%3D4c0b6b76a7f2518d77818535b677e087%26chksm%3D970c2c4ca07ba55ad5cfe6b46f72dbef85a159574fb60b9932404e45747c95eed8c6c0f66d62%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003Ebatch\u003C\u002Fa\u003E) 无法继续下降，反而达不到最小。而且 GD 每一步的计算量巨大。即使是可以成功将训练集的 loss 降到最低，可我们并不关心训练集的表现，我们关心的是\u003Cb\u003E测试集\u003C\u002Fb\u003E的表现。训练集表现好，测试集表现差的过拟合 (overfitting) 并不是我们想要的。\u003C\u002Fp\u003E\u003Cp\u003EStochastic Gradient Descent 的随机性有利于跳出鞍点，又具有加强普遍性 (测试集上表现优秀) 的作用。可观看以下视频感受 \u003Cb\u003ESGD 如何缓解陷入局部最小值\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Ca class=\"video-box\" href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fwww.zhihu.com\u002Fvideo\u002F1031175825587978240\" target=\"_blank\" data-video-id=\"\" data-video-playable=\"true\" data-name=\"缓解陷入局部最小值\" data-poster=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c7dd8a8e4a1511326ce444652033fc0f.jpg\" data-lens-id=\"1031175825587978240\"\u003E\u003Cimg class=\"thumbnail\" src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c7dd8a8e4a1511326ce444652033fc0f.jpg\"\u002F\u003E\u003Cspan class=\"content\"\u003E\u003Cspan class=\"title\"\u003E缓解陷入局部最小值\u003Cspan class=\"z-ico-extern-gray\"\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"z-ico-extern-blue\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"url\"\u003E\u003Cspan class=\"z-ico-video\"\u003E\u003C\u002Fspan\u003Ehttps:\u002F\u002Fwww.zhihu.com\u002Fvideo\u002F1031175825587978240\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E可惜训练耗时，同时过大的样本差异会使训练比较震荡，所以有了 Minibatch Gradient Descent 方法，同时具有二者的特点，是以  \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=n\" alt=\"n\" eeimg=\"1\"\u002F\u003E  个样本算出的梯度的平均值来更新每一步。然而不得不苦逼的根据不同的任务去寻找最优的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=n\" alt=\"n\" eeimg=\"1\"\u002F\u003E 。不过在计算机视觉中，由于 batch normalization 的帮助，可以使用极大的 batch size，这时的 batch size 往往是受限于 GPU 的显存大小。\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Ch2\u003E个人经验分享\u003C\u002Fh2\u003E\u003Cp\u003E个人习惯从 batch size 以 128 为分界线。向下 (x0.5) 和向上 (x2) 训练后比较测试结果。若向下更好则再 x0.5，直到结果不再提升。\u003C\u002Fp\u003E\u003Cp\u003E我遇到过很多情况。\u003C\u002Fp\u003E\u003Cp\u003E比如同样的 cnn+rnn 模型结构，adam 更新法，learning rate 和其他参数全部相同。\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cb\u003E例1：\u003C\u002Fb\u003E遇到某个语音、画面、自然语言不同任务时，最好的 batch size 分别为 8, 32, 16。\u003C\u002Fli\u003E\u003Cli\u003E\u003Cb\u003E例2：\u003C\u002Fb\u003E遇到某个多信号语音识别任务时， \u003Cb\u003Ebatch size 为 1 最好\u003C\u002Fb\u003E，可是训练时间让人抓狂，尤其是使用循环网络时。而将这些多个信号之间做 inversion mapping 时，结果却是 batch size 设为 32 后效果依然提升，但由于 GPU 显存限制无法继续加大 batch size。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003E可以看出来，调参也有时间和空间的限制：\u003C\u002Fp\u003E\u003Cblockquote\u003E\u003Cb\u003E大 batch size 限于空间，小 batch size 苦于时间。\u003C\u002Fb\u003E\u003C\u002Fblockquote\u003E\u003Chr\u002F\u003E\u003Ch2\u003E为什么 batch size 会影响训练结果\u003C\u002Fh2\u003E\u003Cp\u003E\u003Ca class=\"member_mention\" href=\"https:\u002F\u002Fwww.zhihu.com\u002Fpeople\u002Fc4cfdf10563bff74ebf3e58d9f1c8592\" data-hash=\"c4cfdf10563bff74ebf3e58d9f1c8592\" data-hovercard=\"p$b$c4cfdf10563bff74ebf3e58d9f1c8592\"\u003E@Unstoppable\u003C\u002Fa\u003E 有问到“一直很难理解为什么 batch size 会影响训练结果，答主能解读一下么”。\u003C\u002Fp\u003E\u003Cp\u003E举一个特别简化的例子。\u003C\u002Fp\u003E\u003Cp\u003E假如要学习一个 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=f%3Ax%5Crightarrow+y\" alt=\"f:x\\rightarrow y\" eeimg=\"1\"\u002F\u003E 去拟合下面这样的一维数据，如下图所示。有 6 个训练样本 (编号 1,2,3,4,5,6)。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-7ffb5d7ee7b6d582a2be9768dc6fa4b1_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"396\" data-rawheight=\"324\" data-default-watermark-src=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fv2-6b8ead921a3bc78883a287bd6a75a4ea_hd.jpg\" class=\"content_image\" width=\"396\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;396&#39; height=&#39;324&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"396\" data-rawheight=\"324\" data-default-watermark-src=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fv2-6b8ead921a3bc78883a287bd6a75a4ea_hd.jpg\" class=\"content_image lazy\" width=\"396\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-7ffb5d7ee7b6d582a2be9768dc6fa4b1_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E如果使用 Gradient Descent，那么每一步的更新都会向类似于黄线那种同时满足 6 个样本的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=f\" alt=\"f\" eeimg=\"1\"\u002F\u003E 去贴近 (不是一步到位，而是一点点更新变形)。\u003C\u002Fp\u003E\u003Cp\u003E如果使用 Stochastic Gradient Descent，那么每一步的更新都会向能穿过该样本点的\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=f\" alt=\"f\" eeimg=\"1\"\u002F\u003E 去贴近。\u003C\u002Fp\u003E\u003Cp\u003E如果使用 3-Batch Gradient Descent，那么每一步都会向同时满足 3 个样本点的形状更新，比如 1,2,3 样本组成的 batch 会往黑色虚线的形状变化。下一步可能是向同时满足 1,5,6 的 batch 的形状去变化。\u003C\u002Fp\u003E\u003Cp\u003E同理，若使用 2-Batch Gradient Descent，绿色虚线是例子。\u003C\u002Fp\u003E\u003Cp\u003E打个比方。这好比素描绘画，不同的 batch 策略决定了用什么样的线条 (线段、曲线段、角度等) 去描绘 (允许覆盖已画的线条)，最后画出的图形是由这些线条构成的。神经网络训练所得的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=f\" alt=\"f\" eeimg=\"1\"\u002F\u003E 也是根据梯度 (类比线条) 一点点更新出来的。所以能够影响梯度的 batch size 会影响神经网络训练出的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=f\" alt=\"f\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cp\u003E诚然用 Gradient Descent 同时参考所有的训练样本去选择绘画线条的话，感觉上最能够画出拟合所有训练集样本的形状，但是这样会使每次绘画的线条从绘画开始到结束都是一样的 (对应神经网络陷入鞍点)，仅仅是不断的描粗线条，反而容易画不出想要的形状。即便是能够成功画出，但别忘了，我们真正想要画的\u003Cb\u003E形状是连同测试样本在内的 (编号 a,b) 也同时满足的 \u003C\u002Fb\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=f\" alt=\"f\" eeimg=\"1\"\u002F\u003E。\u003C\u002Fp\u003E\u003Cp\u003E可以把神经网络的训练想象成一种特殊的绘画。绘画过程中不允许使用测试样本作为参考，只允许用\u003Cb\u003E有限的\u003C\u002Fb\u003E训练样本作为参考，还要求所画的形状覆盖测试样本。\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Ch2\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fplayground.tensorflow.org\u002F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003ETensorflow Playground\u003C\u002Fa\u003E 演示\u003C\u002Fh2\u003E\u003Cp\u003E我们可以用\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fplayground.tensorflow.org\u002F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003ETensorflow — Neural Network Playground\u003C\u002Fa\u003E来实际的体验一下这种“绘画”过程。\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E下图是 batch size 30 时，拟合 300 epoch 的情况 (每个 epoch 是指遍历完所有训练集样本)。拟合很容易卡在某个形状不动。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002Fv2-ab3bf9b9c3e4cf669bf51686752f8374_hd.jpg\" data-size=\"normal\" data-rawwidth=\"262\" data-rawheight=\"329\" data-default-watermark-src=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fv2-ad67d04db7c01d98fee5d4c42dd227f6_hd.jpg\" class=\"content_image\" width=\"262\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;262&#39; height=&#39;329&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"262\" data-rawheight=\"329\" data-default-watermark-src=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fv2-ad67d04db7c01d98fee5d4c42dd227f6_hd.jpg\" class=\"content_image lazy\" width=\"262\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002Fv2-ab3bf9b9c3e4cf669bf51686752f8374_hd.jpg\"\u002F\u003E\u003Cfigcaption\u003Ebatch size 30 的拟合\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cul\u003E\u003Cli\u003E下图是 batch size 1 时，拟合 150 epoch 的情况，很快的就拟合成功了。但注意 loss 曲线的下降。比 batch size 30 时，有上下波动的情况。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002Fv2-405ab493008fdbd788d350162a830cd9_hd.jpg\" data-size=\"normal\" data-rawwidth=\"262\" data-rawheight=\"330\" data-default-watermark-src=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-13e5472fe93fcf62557c76b468876941_hd.jpg\" class=\"content_image\" width=\"262\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;262&#39; height=&#39;330&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"262\" data-rawheight=\"330\" data-default-watermark-src=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-13e5472fe93fcf62557c76b468876941_hd.jpg\" class=\"content_image lazy\" width=\"262\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002Fv2-405ab493008fdbd788d350162a830cd9_hd.jpg\"\u002F\u003E\u003Cfigcaption\u003Ebatch size 1 的拟合\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E从 \u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fplayground.tensorflow.org\u002F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EPlayground\u003C\u002Fa\u003E 的例子中就可以看出， batch size 并非越大越好。\u003C\u002Fp\u003E","editableContent":"","excerpt":"目录batch size 需要调参个人经验分享为什么 batch size 会影响训练结果Tensorflow Playground 演示batch size 需要调参可以确定的是， batch size 绝非越大越好。因为 batch size 的极限是训练集样本总个数，而这是当初神经网络还未如此之火时的训练方式 Gradient Descent (GD)。 2014 年初我上的人工神经网络 (还不叫深度学习，非常旧的教学材料) 课程，老师留的作业就是用自己写个神经网络去实现 4 输入 异或门 。 [图片] 中间层的 [公式] …","collapsedBy":"nobody","collapseReason":"","annotationAction":null,"markInfos":[],"relevantInfo":{"isRelevant":false,"relevantType":"","relevantText":""},"suggestEdit":{"reason":"","status":false,"tip":"","title":"","unnormalDetails":{"status":"","description":"","reason":"","reasonId":0,"note":""},"url":""},"isLabeled":false,"rewardInfo":{"canOpenReward":false,"isRewardable":false,"rewardMemberCount":0,"rewardTotalMoney":0,"tagline":""},"relationship":{"isAuthor":false,"isAuthorized":false,"isNothelp":false,"isThanked":false,"isRecognized":false,"voting":0,"upvotedFollowees":[]}},"440944387":{"id":440944387,"type":"answer","answerType":"normal","question":{"type":"question","id":61607442,"title":"怎么选取训练神经网络时的Batch size?","questionType":"normal","created":1498436644,"updatedTime":1501141798,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F61607442","relationship":{}},"author":{"id":"355b43e4f8511d2554b090b4f4a8e997","urlToken":"geeking-lcq","name":"Lunarnai","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-3fc1dbe922d9981feb59b33dcce4b39d_is.jpg","avatarUrlTemplate":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-3fc1dbe922d9981feb59b33dcce4b39d_{size}.jpg","isOrg":false,"type":"people","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fpeople\u002F355b43e4f8511d2554b090b4f4a8e997","userType":"people","headline":"也不知道说啥  http:\u002F\u002Flunarnai.cn","badge":[],"gender":1,"isAdvertiser":false,"followerCount":306,"isFollowed":false,"isPrivacy":false},"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fanswers\u002F440944387","isCollapsed":false,"createdTime":1531407572,"updatedTime":1531895535,"extras":"","isCopyable":true,"isNormal":true,"voteupCount":44,"commentCount":16,"isSticky":false,"adminClosedComment":false,"commentPermission":"all","canComment":{"reason":"","status":true},"reshipmentSettings":"allowed","content":"\u003Cp\u003E首先\u003Cb\u003E反对上面的尽可能调大batch size\u003C\u002Fb\u003E的说法，在现在较前沿的视角来看，这种观点无疑是有些滞后的。\u003C\u002Fp\u003E\u003Cp\u003E关于这个问题，我们来看下深度学习三巨头之一的LeCun杨乐春同志怎么说（想看结论直接翻到最后）：  \u003C\u002Fp\u003E\u003Cblockquote\u003ETraining with \u003Cb\u003Elarge minibatches is bad\u003C\u002Fb\u003E for your health. More importantly, it&#39;s bad for your test error. Friends don‘t let friends use minibatches larger than 32. Let&#39;s face it: the \u003Ci\u003Eonly \u003C\u002Fi\u003Epeople have switched to minibatch sizes larger than one since 2012 is because GPUs are inefficient for batch sizes smaller than 32. That&#39;s a terrible reason. It just means our hardware sucks.  \u003C\u002Fblockquote\u003E\u003Cp\u003E翻译过来就是：\u003C\u002Fp\u003E\u003Cblockquote\u003E使用\u003Cb\u003E大的batch size有害身体健康。\u003C\u002Fb\u003E更重要的是，它对测试集的error不利。一个真正的朋友不会让你使用大于32的batch size。直说了吧：2012年来人们开始转而使用更大batch size的原因只是我们的GPU不够强大，处理小于32的batch size时效率太低。这是个糟糕的理由，只说明了我们的硬件还很辣鸡。  \u003C\u002Fblockquote\u003E\u003Cp\u003E那是什么使得大牛LeCun同志对大于32的batch size如此深恶痛绝而发此论呢？  \u003C\u002Fp\u003E\u003Cp\u003E细究出处可以发现这些评论是他读完\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fabs\u002F1804.07612\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003ERevisiting Small Batch Training for Deep Neural Networks\u003C\u002Fa\u003E 的感想，这篇论文对batch size（以及其他一些超参数）在深度学习中的选取做了详尽的分析并提供了实验数据。结果表明： \u003C\u002Fp\u003E\u003Cblockquote\u003EThe best performance has been consistently obtained for mini-batch sizes between m=2 and m=32, which contrasts with recent work advocating the use of mini-batch sizes in the thousands. \u003C\u002Fblockquote\u003E\u003Cp\u003E也就是\u003Cb\u003E最好的实验表现都是在batch size处于2~32之间\u003C\u002Fb\u003E得到的，这和最近深度学习界论文中习惯的动辄上千的batch size选取有很大的出入。  \u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E其实回想我们使用mini-batch技术的原因，无外乎是因为mini-batch有这几个好处 ：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E提高了运行效率，相比batch-GD的每个epoch只更新一次参数，使用mini-batch可以在一个epoch中多次更新参数，加速收敛。\u003C\u002Fli\u003E\u003Cli\u003E解决了某些任务中，训练集过大，无法一次性读入内存的问题。\u003C\u002Fli\u003E\u003Cli\u003E虽然第一点是mini-batch提出的最初始的原因，但是后来人们发现，使用mini-batch还有个好处，即每次更新时由于没有使用全量数据而仅仅使用batch内数据，从而人为给训练带来了噪声，而这个操作却往往能够带领算法\u003Cb\u003E走出局部最优（鞍点）。\u003C\u002Fb\u003E理论证明参见COLT的这篇论文Escaping From Saddle Points-Online Stochastic Gradient for Tensor Decomposition。也就是说，曾经我们使用mini-batch主要是为了加快收敛和节省内存，同时也带来每次更新有些“不准”的副作用，但是现在的观点来看，\u003Cb\u003E这些“副作用”反而对我们的训练有着更多的增益，也变成mini-batch技术最主要的优点。\u003C\u002Fb\u003E（有点像伟哥西地那非，最早是被发明出来治疗心血管疾病的，但是发现其副作用是容易使人勃起，最后反而是用来助勃，而不是用来治疗心血管了）\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E综上所述，我们选取batch size时不妨这样操作：\u003C\u002Fp\u003E\u003Col\u003E\u003Cli\u003E当有足够算力时，选取batch size为32或更小一些。\u003C\u002Fli\u003E\u003Cli\u003E算力不够时，在效率和泛化性之间做trade-off，尽量选择更小的batch size。\u003C\u002Fli\u003E\u003Cli\u003E当模型训练到尾声，想更精细化地提高成绩（比如论文实验\u002F比赛到最后），有一个有用的trick，就是设置batch size为1，即做纯SGD，慢慢把error磨低。\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Ch2\u003E参考文献：\u003C\u002Fh2\u003E\u003Cp\u003E[1] Dominic Masters, Carlo Luschi,\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fabs\u002F1804.07612\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003ERevisiting Small Batch Training for Deep Neural Networks\u003C\u002Fa\u003E, arXiv:1804.07612v1\u003C\u002Fp\u003E\u003Cp\u003E[2] Ge, R., Huang, F., Jin, C., &amp; Yuan, Y. (2015, June). Escaping From Saddle Points-Online Stochastic Gradient for Tensor Decomposition. In COLT (pp. 797-842).\u003C\u002Fp\u003E\u003Cp\u003E[3] Bottou, L. (2010). Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT&#39;2010 (pp. 177-186). Physica-Verlag HD.\u003C\u002Fp\u003E","editableContent":"","excerpt":"首先 反对上面的尽可能调大batch size的说法，在现在较前沿的视角来看，这种观点无疑是有些滞后的。关于这个问题，我们来看下深度学习三巨头之一的LeCun杨乐春同志怎么说（想看结论直接翻到最后）： Training with large minibatches is bad for your health. More importantly, it&#39;s bad for your test error. Friends don‘t let friends use minibatches larger than 32. Let&#39;s face it: the only people have switched to mi…","collapsedBy":"nobody","collapseReason":"","annotationAction":null,"markInfos":[],"relevantInfo":{"isRelevant":false,"relevantType":"","relevantText":""},"suggestEdit":{"reason":"","status":false,"tip":"","title":"","unnormalDetails":{"status":"","description":"","reason":"","reasonId":0,"note":""},"url":""},"isLabeled":false,"rewardInfo":{"canOpenReward":false,"isRewardable":false,"rewardMemberCount":0,"rewardTotalMoney":0,"tagline":""},"relationship":{"isAuthor":false,"isAuthorized":false,"isNothelp":false,"isThanked":false,"isRecognized":false,"voting":0,"upvotedFollowees":[]}}},"articles":{},"columns":{},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"clubs":{}},"currentUser":"","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false}},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{}},"env":{"ab":{"config":{"experiments":[{"expId":"launch-us_foltopic_user-10","expPrefix":"us_foltopic_user","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false}],"params":[{"id":"se_webmajorob","type":"String","value":"0","chainId":"_all_"},{"id":"se_whitelist","type":"String","value":"0","chainId":"_all_"},{"id":"se_wannasearch","type":"String","value":"0","chainId":"_all_"},{"id":"se_college_cm","type":"String","value":"0","chainId":"_all_"},{"id":"se_featured","type":"String","value":"1","chainId":"_all_"},{"id":"zr_rec_answer_cp","type":"String","value":"close","chainId":"_all_"},{"id":"se_agency","type":"String","value":" 0","chainId":"_all_"},{"id":"top_new_feed","type":"String","value":"5","chainId":"_all_"},{"id":"top_recall_exp_v1","type":"String","value":"1","chainId":"_all_"},{"id":"web_heifetz_grow_ad","type":"String","value":"1"},{"id":"li_tjys_ec_ab","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_tag","type":"String","value":"open","chainId":"_all_"},{"id":"zr_rel_search","type":"String","value":"base","chainId":"_all_"},{"id":"se_p_slideshow","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sft_v2","type":"String","value":"d","chainId":"_all_"},{"id":"top_recall_deep_user","type":"String","value":"1","chainId":"_all_"},{"id":"top_test_4_liguangyi","type":"String","value":"1","chainId":"_all_"},{"id":"ug_zero_follow","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_hotctr","type":"String","value":"1","chainId":"_all_"},{"id":"se_col_boost","type":"String","value":"0","chainId":"_all_"},{"id":"top_rank","type":"String","value":"0","chainId":"_all_"},{"id":"ug_follow_answerer","type":"String","value":"0","chainId":"_all_"},{"id":"li_ts_sample","type":"String","value":"old","chainId":"_all_"},{"id":"zr_slot_cold_start","type":"String","value":"default","chainId":"_all_"},{"id":"top_quality","type":"String","value":"0","chainId":"_all_"},{"id":"top_ebook","type":"String","value":"0","chainId":"_all_"},{"id":"ug_goodcomment_0","type":"String","value":"1","chainId":"_all_"},{"id":"web_sem_ab","type":"String","value":"0"},{"id":"li_qa_cover","type":"String","value":"old","chainId":"_all_"},{"id":"zr_km_item_cf","type":"String","value":"close","chainId":"_all_"},{"id":"se_ltr_ck","type":"String","value":"0","chainId":"_all_"},{"id":"soc_notification","type":"String","value":"0","chainId":"_all_"},{"id":"ug_newtag","type":"String","value":"0","chainId":"_all_"},{"id":"li_back","type":"String","value":"0","chainId":"_all_"},{"id":"zr_man_intervene","type":"String","value":"0","chainId":"_all_"},{"id":"se_colorfultab","type":"String","value":"1","chainId":"_all_"},{"id":"se_auto_syn","type":"String","value":"0","chainId":"_all_"},{"id":"se_mobileweb","type":"String","value":"1","chainId":"_all_"},{"id":"top_gr_ab","type":"String","value":"0","chainId":"_all_"},{"id":"li_android_vip","type":"String","value":"0","chainId":"_all_"},{"id":"zr_infinity_small","type":"String","value":"256","chainId":"_all_"},{"id":"top_vipconsume","type":"String","value":"1","chainId":"_all_"},{"id":"ug_follow_answerer_0","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_paid_answer","type":"String","value":"0","chainId":"_all_"},{"id":"se_webrs","type":"String","value":"1","chainId":"_all_"},{"id":"se_dnn_slabel","type":"String","value":"0","chainId":"_all_"},{"id":"tp_meta_card","type":"String","value":"0","chainId":"_all_"},{"id":"pf_newguide_vertical","type":"String","value":"0","chainId":"_all_"},{"id":"qa_answerlist_ad","type":"String","value":"0","chainId":"_all_"},{"id":"se_ios_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"se_ltr_gc","type":"String","value":"0","chainId":"_all_"},{"id":"se_ltr_nn","type":"String","value":"1","chainId":"_all_"},{"id":"se_movietab","type":"String","value":"1","chainId":"_all_"},{"id":"tp_header_style","type":"String","value":"1","chainId":"_all_"},{"id":"pf_creator_card","type":"String","value":"1","chainId":"_all_"},{"id":"ls_fmp4","type":"String","value":"0","chainId":"_all_"},{"id":"gue_anonymous","type":"String","value":"show"},{"id":"li_qa_new_cover","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_metacard","type":"String","value":"1","chainId":"_all_"},{"id":"ug_follow_topic_1","type":"String","value":"2","chainId":"_all_"},{"id":"li_price_test","type":"String","value":"1","chainId":"_all_"},{"id":"gue_zhuantikapian","type":"String","value":"0"},{"id":"se_ri","type":"String","value":"0","chainId":"_all_"},{"id":"top_recall_exp_v2","type":"String","value":"1","chainId":"_all_"},{"id":"tsp_childbillboard","type":"String","value":"1","chainId":"_all_"},{"id":"soc_special","type":"String","value":"0","chainId":"_all_"},{"id":"pf_foltopic_usernum","type":"String","value":"50","chainId":"_all_"},{"id":"top_ydyq","type":"String","value":"X","chainId":"_all_"},{"id":"zr_ans_rec","type":"String","value":"gbrank","chainId":"_all_"},{"id":"se_search_feed","type":"String","value":"N","chainId":"_all_"},{"id":"tp_topic_head","type":"String","value":"1","chainId":"_all_"},{"id":"li_album_liutongab","type":"String","value":"0","chainId":"_all_"},{"id":"li_pay_banner_type","type":"String","value":"0","chainId":"_all_"},{"id":"se_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"se_backsearch","type":"String","value":"0","chainId":"_all_"},{"id":"se_subtext","type":"String","value":"0","chainId":"_all_"},{"id":"li_hot_score_ab","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_paid_answer","type":"String","value":"0","chainId":"_all_"},{"id":"zr_video_recall","type":"String","value":"current_recall","chainId":"_all_"},{"id":"se_dnn_muli_task","type":"String","value":"0","chainId":"_all_"},{"id":"soc_bignew","type":"String","value":"1","chainId":"_all_"},{"id":"ls_videoad","type":"String","value":"2","chainId":"_all_"},{"id":"web_answer_update","type":"String","value":"0"},{"id":"zr_km_slot_style","type":"String","value":"event_card","chainId":"_all_"},{"id":"se_go_ztext","type":"String","value":"0","chainId":"_all_"},{"id":"se_amovietab","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_qa","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_metacard_top","type":"String","value":"top","chainId":"_all_"},{"id":"tsp_newchild","type":"String","value":"1","chainId":"_all_"},{"id":"web_answer_list_ad","type":"String","value":"1"},{"id":"zr_art_rec","type":"String","value":"base","chainId":"_all_"},{"id":"se_ad_index","type":"String","value":"10","chainId":"_all_"},{"id":"se_websearch","type":"String","value":"3","chainId":"_all_"},{"id":"top_hotcommerce","type":"String","value":"1","chainId":"_all_"},{"id":"tsp_vote","type":"String","value":"1","chainId":"_all_"},{"id":"ug_zero_follow_0","type":"String","value":"0","chainId":"_all_"},{"id":"web_answerlist_ad","type":"String","value":"0"},{"id":"zr_km_style","type":"String","value":"base","chainId":"_all_"},{"id":"se_rr","type":"String","value":"0","chainId":"_all_"},{"id":"qa_test","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_answer","type":"String","value":"open_cvr","chainId":"_all_"},{"id":"se_webtimebox","type":"String","value":"0","chainId":"_all_"},{"id":"se_lottery","type":"String","value":"0","chainId":"_all_"},{"id":"se_limit","type":"String","value":"0","chainId":"_all_"},{"id":"se_site_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"se_billboardsearch","type":"String","value":"0","chainId":"_all_"},{"id":"se_new_topic","type":"String","value":"0","chainId":"_all_"},{"id":"top_v_album","type":"String","value":"1","chainId":"_all_"},{"id":"zr_answer_rec_cp","type":"String","value":"open","chainId":"_all_"},{"id":"zr_video_rank_nn","type":"String","value":"new_rank","chainId":"_all_"},{"id":"se_topicdirect","type":"String","value":"2","chainId":"_all_"},{"id":"top_root","type":"String","value":"0","chainId":"_all_"},{"id":"pf_noti_entry_num","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_lastread","type":"String","value":"0","chainId":"_all_"},{"id":"pf_fuceng","type":"String","value":"1","chainId":"_all_"},{"id":"zr_article_rec_rank","type":"String","value":"close","chainId":"_all_"},{"id":"zr_video_rank","type":"String","value":"new_rank","chainId":"_all_"},{"id":"zr_test_aa1","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sticky_android","type":"String","value":"0","chainId":"_all_"},{"id":"top_universalebook","type":"String","value":"1","chainId":"_all_"},{"id":"ug_fw_answ_aut_1","type":"String","value":"0","chainId":"_all_"},{"id":"li_search_answer","type":"String","value":"0","chainId":"_all_"},{"id":"zw_sameq_sorce","type":"String","value":"999","chainId":"_all_"},{"id":"zr_search_xgb","type":"String","value":"0","chainId":"_all_"},{"id":"se_likebutton","type":"String","value":"0","chainId":"_all_"},{"id":"se_zu_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"top_native_answer","type":"String","value":"1","chainId":"_all_"},{"id":"se_mclick","type":"String","value":"0","chainId":"_all_"},{"id":"se_college","type":"String","value":"default","chainId":"_all_"},{"id":"se_waterfall","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_toast","type":"String","value":"1","chainId":"_all_"},{"id":"soc_update","type":"String","value":"1","chainId":"_all_"},{"id":"web_question_invite","type":"String","value":"B"},{"id":"zr_km_xgb_model","type":"String","value":"new_xgb","chainId":"_all_"},{"id":"se_mclick1","type":"String","value":"0","chainId":"_all_"},{"id":"se_preset_tech","type":"String","value":"0","chainId":"_all_"},{"id":"soc_bigone","type":"String","value":"0","chainId":"_all_"},{"id":"se_zu_recommend","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_kv","type":"String","value":"0","chainId":"_all_"},{"id":"se_expired_ob","type":"String","value":"0","chainId":"_all_"},{"id":"se_payconsult","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sft","type":"String","value":"a","chainId":"_all_"},{"id":"ls_new_upload","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_album_card","type":"String","value":"0","chainId":"_all_"},{"id":"web_column_auto_invite","type":"String","value":"0"},{"id":"se_topiclabel","type":"String","value":"1","chainId":"_all_"},{"id":"li_se_xgb","type":"String","value":"0","chainId":"_all_"},{"id":"gue_new_special_page","type":"String","value":"0"},{"id":"zr_intervene","type":"String","value":"0","chainId":"_all_"},{"id":"se_dnn_cpyramid","type":"String","value":"0","chainId":"_all_"},{"id":"se_famous","type":"String","value":"1","chainId":"_all_"},{"id":"se_time_threshold","type":"String","value":"0","chainId":"_all_"},{"id":"tp_m_intro_re_topic","type":"String","value":"1","chainId":"_all_"},{"id":"web_n_web_msg","type":"String","value":"0"}],"chains":[{"chainId":"_all_"}]},"triggers":{}},"userAgent":{"Edge":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":false,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"isWebView":false,"origin":"Mozilla\u002F5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F76.0.3809.100 Safari\u002F537.36"},"ctx":{"path":"\u002Fquestion\u002F61607442"},"trafficSource":"production","edition":{"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false},"theme":"light","enableShortcut":true,"referer":"https:\u002F\u002Fwww.cupoy.com\u002Fqa\u002Fkwassist\u002Fai_tw\u002F0000016BC80F2A76000000376375706F795F72656C656173655155455354","conf":{},"ipInfo":{},"logged":false,"tdkInfo":{}},"me":{"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"active":{"sendDigitsError":null,"activeConfirmSucceeded":null,"activeConfirmError":null},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false,"captchaBase64String":null,"captchaValidationMessage":null,"loginCaptchaExpires":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"currentCreatorUrlToken":null,"homeData":{"recommendQuestions":[]},"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0},"goodatTopics":[]},"customPromotion":{"itemLists":{}},"recommend":{"recommendTimes":{}}},"explore":{"academy":{"tabs":[],"article":{}}},"rights":[],"rightsStatus":{},"levelUpperLimit":10,"account":{"growthLevel":{}},"applyStatus":{}},"question":{"followers":{},"concernedFollowers":{},"answers":{"61607442":{"isFetching":false,"isDrained":false,"ids":[440401209,204525634,192204021,440944387,204586969],"newIds":[440401209,204525634,192204021,440944387,204586969],"totals":13,"isPrevDrained":true,"previous":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F61607442\u002Fanswers?include=data%5B%2A%5D.is_normal%2Cadmin_closed_comment%2Creward_info%2Cis_collapsed%2Cannotation_action%2Cannotation_detail%2Ccollapse_reason%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Ccreated_time%2Cupdated_time%2Creview_info%2Crelevant_info%2Cquestion%2Cexcerpt%2Crelationship.is_authorized%2Cis_author%2Cvoting%2Cis_thanked%2Cis_nothelp%2Cis_labeled%2Cis_recognized%2Cpaid_info%2Cpaid_info_content%3Bdata%5B%2A%5D.mark_infos%5B%2A%5D.url%3Bdata%5B%2A%5D.author.follower_count%2Cbadge%5B%2A%5D.topics&limit=5&offset=0&platform=desktop&sort_by=default","next":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F61607442\u002Fanswers?include=data%5B%2A%5D.is_normal%2Cadmin_closed_comment%2Creward_info%2Cis_collapsed%2Cannotation_action%2Cannotation_detail%2Ccollapse_reason%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Ccreated_time%2Cupdated_time%2Creview_info%2Crelevant_info%2Cquestion%2Cexcerpt%2Crelationship.is_authorized%2Cis_author%2Cvoting%2Cis_thanked%2Cis_nothelp%2Cis_labeled%2Cis_recognized%2Cpaid_info%2Cpaid_info_content%3Bdata%5B%2A%5D.mark_infos%5B%2A%5D.url%3Bdata%5B%2A%5D.author.follower_count%2Cbadge%5B%2A%5D.topics&limit=5&offset=5&platform=desktop&sort_by=default"}},"hiddenAnswers":{},"updatedAnswers":{},"collapsedAnswers":{},"notificationAnswers":{},"invitedQuestions":{"total":{"count":null,"isEnd":false,"isLoading":false,"questions":[]},"followees":{"count":null,"isEnd":false,"isLoading":false,"questions":[]}},"laterQuestions":{"count":null,"globalWriteAnimate":false,"isEnd":false,"isLoading":false,"questions":[]},"waitingQuestions":{"hot":{"isEnd":false,"isLoading":false,"questions":[]},"value":{"isEnd":false,"isLoading":false,"questions":[]},"newest":{"isEnd":false,"isLoading":false,"questions":[]},"easy":{"isEnd":false,"isLoading":false,"questions":[]}},"invitationCandidates":{},"inviters":{},"invitees":{},"similarQuestions":{},"relatedCommodities":{},"recommendReadings":{},"bio":{},"brand":{},"permission":{},"adverts":{"3":{"ad":{"adVerb":"","brand":{"id":0,"logo":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-b02efb32af2aada63f09e5d0a275df75_250x250.jpeg","name":"Togocareer"},"category":1,"clickTracks":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805&ui=1.171.46.33&ts=1566397201&au=4280&idi=2006&ar=0.0000003217603730973035&pf=4&ed=CjEEfh4wM30sQTYFBmIvAEZ8Cy8Obm5xLRR_XlBlfB0OfQEsAHo7cyoXZQIXMTYNWXYPbFkqPn14FWtUA2RqQAJ4CHoIem15aEY7WgdidAdYdx8pTH5qZi1Kb1QAYWpSG3gKewllNjVzFUZk-gy3_AXl&nt=0&ut=deb9c276440c470f9c386c2db6870abe&pdi=1523617890848616"],"closeTrack":"https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?nt=0&tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805&idi=2006&ar=0.0000003217603730973035&ui=1.171.46.33&ed=CjEEfR4wM30sQTYFBmIvAEZ8Cy8Obm5xLRR_XlBlfB0OfQEsAHo7cyoXZQIXMTYNWXYPbFkqPn14FWtUA2RqQAJ4CHoIem15aEY7WgdidAdYdx8pTH5qZi1Kb1QAYWpSG3gKewllNjVzFXhY1WhwaKbv&au=4280&ut=deb9c276440c470f9c386c2db6870abe&pf=4&ts=1566397201&pdi=1523617890848616","closeTracks":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?nt=0&tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805&idi=2006&ar=0.0000003217603730973035&ui=1.171.46.33&ed=CjEEfR4wM30sQTYFBmIvAEZ8Cy8Obm5xLRR_XlBlfB0OfQEsAHo7cyoXZQIXMTYNWXYPbFkqPn14FWtUA2RqQAJ4CHoIem15aEY7WgdidAdYdx8pTH5qZi1Kb1QAYWpSG3gKewllNjVzFXhY1WhwaKbv&au=4280&ut=deb9c276440c470f9c386c2db6870abe&pf=4&ts=1566397201&pdi=1523617890848616"],"conversionTracks":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805&pdi=1523617890848616&au=4280&ar=0.0000003217603730973035&ui=1.171.46.33&ts=1566397201&idi=2006&ut=deb9c276440c470f9c386c2db6870abe&pf=4&nt=0&ed=CjEEfx4wM30sQTYFBmIvAEZ8Cy8Obm5xLRR_XlBlfB0OfQEsAHo7cyoXZQIXMTYNWXYPbFkqPn14FWtUA2RqQAJ4CHoIem15aEY7WgdidAdYdx8pTH5qZi1Kb1QAYWpSG3gKewllNjVzFV1Ake3Ops-w"],"creatives":[{"appPromotionUrl":"","brand":{"id":0,"logo":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-b02efb32af2aada63f09e5d0a275df75_250x250.jpeg","name":"Togocareer"},"cta":{"value":"查看详情"},"description":"留学生回国，在国内的发展情况如何？2019年校招、社招信息、500强名企内推名额预约，实习，求职更快捷！","footer":{"value":""},"landingUrl":"https:\u002F\u002Fwww.togocareer.com\u002Fservices.html?tgcChannel==zhihu&tuwen0805","title":"对于留学生来说，回国求职，如何进入名企工作？实习机会怎么找？"}],"debugTracks":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?au=4280&idi=2006&pf=4&ut=deb9c276440c470f9c386c2db6870abe&ts=1566397201&nt=0&tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805&ui=1.171.46.33&ed=CjEEcx4wM30sQTYFBmIvAEZ8Cy8Obm5xLRR_XlBlfB0OfQEsAHo7cyoXZQIXMTYNWXYPbFkqPn14FWtUA2RqQAJ4CHoIem15aEY7WgdidAdYdx8pTH5qZi1Kb1QAYWpSG3gKewllNjVzFVRz0XP3azkL&pdi=1523617890848616&ar=0.0000003217603730973035"],"displayAdvertisingTag":true,"experimentInfo":"{\"ad_card_close_hotspot\":{\"close_area\":\"old\"},\"ad_follow\":{\"is_new_page\":0},\"canvas_form\":{\"new_privacy\":\"1\"},\"fe\":{\"fp_click\":0,\"fp_desc_isshow\":0,\"fp_video_isactive\":0,\"fp_video_isplay\":0,\"fp_video_style\":\"nil\",\"fp_word_isactive\":0},\"feed_new_page\":{\"new_page\":\"none\"},\"l_a_p\":{\"p_as_prefetch\":\"0\"},\"l_ad_30\":{\"new_30\":\"0\"},\"l_ad_314_d\":{\"p_ad_314_d\":5},\"l_c_color\":{\"p_c_color\":\"gray\"},\"l_c_st\":{\"p_c_st\":\"0\"},\"l_c_st_31\":{\"p_c_st_31\":\"none\"},\"l_c_st_bi_30\":{\"p_c_st_bi_30\":\"0\"},\"l_c_st_pw_30\":{\"p_c_st_pw_30\":\"0\"},\"l_c_st_pw_8_r\":{\"p_c_st_pw_8_r\":\"0\"},\"l_c_st_v_30\":{\"p_c_st_v_30\":\"0\"},\"l_c_style\":{\"p_c_style\":\"1\"},\"l_creative_st_bi_8\":{\"p_creative_st_bi_8\":\"0\"},\"l_creative_st_v_8\":{\"p_creative_st_v_8\":\"0\"},\"l_f_h\":{\"p_f_h\":\"0\"},\"l_m_bfa\":{\"p_m_bfa\":\"a\"},\"l_m_cf_2\":{\"p_m_cf_2\":\"0\"},\"l_m_cf_3\":{\"p_m_cf_3\":\"0\"},\"l_m_cf_4\":{\"p_m_cf_4\":\"0\"},\"l_m_cf_5\":{\"p_m_cf_5\":\"0\"},\"l_m_cv_3\":{\"p_m_cv_3\":\"0\"},\"l_m_cv_5\":{\"p_m_cv_5\":\"0\"},\"l_native_ad\":{\"label\":\"old\"},\"lps_switch\":{\"p_lps_switch\":0,\"p_lps_switch_android\":0},\"lps_switch2\":{\"p_lps_switch2\":\"0\"},\"morph_switch\":{\"p_morph_switch\":\"on\"}}","id":609324,"impressionTracks":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?idi=2006&ar=0.0000003217603730973035&nt=0&ed=CjEEeB4wM30sQTYFBmIvAEZ8Cy8Obm5xLRR_XlBlfB0OfQEsAHo7cyoXZQIXMTYNWXYPbFkqPn14FWtUA2RqQAJ4CHoIem15aEY7WgdidAdYdx8pTH5qZi1Kb1QAYWpSG3gKewllNjVzFQ1cinE9bEAF&tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805&pf=4&pdi=1523617890848616&ut=deb9c276440c470f9c386c2db6870abe&au=4280&ui=1.171.46.33&ts=1566397201"],"isEvergreen":false,"isNewWebview":true,"isSpeeding":false,"isWebp":false,"landPrefetch":false,"name":"","nativePrefetch":true,"partyId":-2,"revertCloseTrack":"https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?ar=0.0000003217603730973035&ut=deb9c276440c470f9c386c2db6870abe&au=4280&idi=2006&pdi=1523617890848616&pf=4&ui=1.171.46.33&ts=1566397201&nt=0&tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805&ed=CjEEch4wM30sQTYFBmIvAEZ8Cy8Obm5xLRR_XlBlfB0OfQEsAHo7cyoXZQIXMTYNWXYPbFkqPn14FWtUA2RqQAJ4CHoIem15aEY7WgdidAdYdx8pTH5qZi1Kb1QAYWpSG3gKewllNjVzFVvKeli6wXPp","template":"web_word","viewTrack":"https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?ts=1566397201&nt=0&ar=0.0000003217603730973035&ui=1.171.46.33&tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805&au=4280&pf=4&pdi=1523617890848616&idi=2006&ut=deb9c276440c470f9c386c2db6870abe&ed=CjEEeR4wM30sQTYFBmIvAEZ8Cy8Obm5xLRR_XlBlfB0OfQEsAHo7cyoXZQIXMTYNWXYPbFkqPn14FWtUA2RqQAJ4CHoIem15aEY7WgdidAdYdx8pTH5qZi1Kb1QAYWpSG3gKewllNjVzFYwL9DVwInbe","viewTracks":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?ts=1566397201&nt=0&ar=0.0000003217603730973035&ui=1.171.46.33&tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805&au=4280&pf=4&pdi=1523617890848616&idi=2006&ut=deb9c276440c470f9c386c2db6870abe&ed=CjEEeR4wM30sQTYFBmIvAEZ8Cy8Obm5xLRR_XlBlfB0OfQEsAHo7cyoXZQIXMTYNWXYPbFkqPn14FWtUA2RqQAJ4CHoIem15aEY7WgdidAdYdx8pTH5qZi1Kb1QAYWpSG3gKewllNjVzFYwL9DVwInbe"],"zaAdInfo":"CKyYJRDsASIBMV2murpOYPyvJg==","zaAdInfoJson":"{\"ad_id\":609324,\"ad_zone_id\":236,\"category\":\"1\",\"timestamp\":1566397200,\"creative_id\":628732}"},"adjson":"{\"ads\":[{\"id\":609324,\"ad_zone_id\":236,\"template\":\"web_word\",\"impression_tracks\":[\"https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?ut=deb9c276440c470f9c386c2db6870abe\\u0026pf=4\\u0026ar=0.0000003217603730973035\\u0026pdi=1523617890848616\\u0026ts=1566397201\\u0026ed=CjEEeB4wM30sQTYFBmIvAEZ8Cy8Obm5xLRR_XlBlfB0OfQEsAHo7cyoXZQIXMTYNWXYPbFkqPn14FWtUA2RqQAJ4CHoIem15aEY7WgdidAdYdx8pTH5qZi1Kb1QAYWpSG3gKewllNjVzFQ1cinE9bEAF\\u0026ui=1.171.46.33\\u0026au=4280\\u0026idi=2006\\u0026tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805\\u0026nt=0\"],\"view_tracks\":[\"https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?pf=4\\u0026ui=1.171.46.33\\u0026pdi=1523617890848616\\u0026idi=2006\\u0026au=4280\\u0026ed=CjEEeR4wM30sQTYFBmIvAEZ8Cy8Obm5xLRR_XlBlfB0OfQEsAHo7cyoXZQIXMTYNWXYPbFkqPn14FWtUA2RqQAJ4CHoIem15aEY7WgdidAdYdx8pTH5qZi1Kb1QAYWpSG3gKewllNjVzFYwL9DVwInbe\\u0026ar=0.0000003217603730973035\\u0026nt=0\\u0026ts=1566397201\\u0026ut=deb9c276440c470f9c386c2db6870abe\\u0026tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805\"],\"click_tracks\":[\"https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?ts=1566397201\\u0026idi=2006\\u0026ed=CjEEfh4wM30sQTYFBmIvAEZ8Cy8Obm5xLRR_XlBlfB0OfQEsAHo7cyoXZQIXMTYNWXYPbFkqPn14FWtUA2RqQAJ4CHoIem15aEY7WgdidAdYdx8pTH5qZi1Kb1QAYWpSG3gKewllNjVzFUZk-gy3_AXl\\u0026pdi=1523617890848616\\u0026ui=1.171.46.33\\u0026pf=4\\u0026ut=deb9c276440c470f9c386c2db6870abe\\u0026nt=0\\u0026au=4280\\u0026ar=0.0000003217603730973035\\u0026tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805\"],\"close_tracks\":[\"https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?ts=1566397201\\u0026tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805\\u0026nt=0\\u0026au=4280\\u0026ui=1.171.46.33\\u0026idi=2006\\u0026ar=0.0000003217603730973035\\u0026ed=CjEEfR4wM30sQTYFBmIvAEZ8Cy8Obm5xLRR_XlBlfB0OfQEsAHo7cyoXZQIXMTYNWXYPbFkqPn14FWtUA2RqQAJ4CHoIem15aEY7WgdidAdYdx8pTH5qZi1Kb1QAYWpSG3gKewllNjVzFXhY1WhwaKbv\\u0026pdi=1523617890848616\\u0026ut=deb9c276440c470f9c386c2db6870abe\\u0026pf=4\"],\"debug_tracks\":[\"https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?idi=2006\\u0026ar=0.0000003217603730973035\\u0026au=4280\\u0026ut=deb9c276440c470f9c386c2db6870abe\\u0026nt=0\\u0026ed=CjEEcx4wM30sQTYFBmIvAEZ8Cy8Obm5xLRR_XlBlfB0OfQEsAHo7cyoXZQIXMTYNWXYPbFkqPn14FWtUA2RqQAJ4CHoIem15aEY7WgdidAdYdx8pTH5qZi1Kb1QAYWpSG3gKewllNjVzFVRz0XP3azkL\\u0026ts=1566397201\\u0026pf=4\\u0026pdi=1523617890848616\\u0026tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805\\u0026ui=1.171.46.33\"],\"conversion_tracks\":[\"https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?ut=deb9c276440c470f9c386c2db6870abe\\u0026ed=CjEEfx4wM30sQTYFBmIvAEZ8Cy8Obm5xLRR_XlBlfB0OfQEsAHo7cyoXZQIXMTYNWXYPbFkqPn14FWtUA2RqQAJ4CHoIem15aEY7WgdidAdYdx8pTH5qZi1Kb1QAYWpSG3gKewllNjVzFV1Ake3Ops-w\\u0026idi=2006\\u0026au=4280\\u0026tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805\\u0026pdi=1523617890848616\\u0026ar=0.0000003217603730973035\\u0026pf=4\\u0026nt=0\\u0026ts=1566397201\\u0026ui=1.171.46.33\"],\"za_ad_info\":\"CKyYJRDsASIBMV2murpOYPyvJg==\",\"za_ad_info_json\":\"{\\\"ad_id\\\":609324,\\\"ad_zone_id\\\":236,\\\"category\\\":\\\"1\\\",\\\"timestamp\\\":1566397200,\\\"creative_id\\\":628732}\",\"creatives\":[{\"id\":628732,\"asset\":{\"brand_name\":\"Togocareer\",\"brand_logo\":\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-b02efb32af2aada63f09e5d0a275df75_250x250.jpeg\",\"title\":\"对于留学生来说，回国求职，如何进入名企工作？实习机会怎么找？\",\"desc\":\"留学生回国，在国内的发展情况如何？2019年校招、社招信息、500强名企内推名额预约，实习，求职更快捷！\",\"landing_url\":\"https:\u002F\u002Fwww.togocareer.com\u002Fservices.html?tgcChannel==zhihu\\u0026tuwen0805\",\"img_size\":0,\"cta\":\"查看详情\"}}],\"expand\":{\"display_advertising_tag\":true,\"is_new_webview\":true,\"is_cdn_speeding\":false},\"experiment_info\":\"{\\\"ad_card_close_hotspot\\\":{\\\"close_area\\\":\\\"old\\\"},\\\"ad_follow\\\":{\\\"is_new_page\\\":0},\\\"canvas_form\\\":{\\\"new_privacy\\\":\\\"1\\\"},\\\"fe\\\":{\\\"fp_click\\\":0,\\\"fp_desc_isshow\\\":0,\\\"fp_video_isactive\\\":0,\\\"fp_video_isplay\\\":0,\\\"fp_video_style\\\":\\\"nil\\\",\\\"fp_word_isactive\\\":0},\\\"feed_new_page\\\":{\\\"new_page\\\":\\\"none\\\"},\\\"l_a_p\\\":{\\\"p_as_prefetch\\\":\\\"0\\\"},\\\"l_ad_30\\\":{\\\"new_30\\\":\\\"0\\\"},\\\"l_ad_314_d\\\":{\\\"p_ad_314_d\\\":5},\\\"l_c_color\\\":{\\\"p_c_color\\\":\\\"gray\\\"},\\\"l_c_st\\\":{\\\"p_c_st\\\":\\\"0\\\"},\\\"l_c_st_31\\\":{\\\"p_c_st_31\\\":\\\"none\\\"},\\\"l_c_st_bi_30\\\":{\\\"p_c_st_bi_30\\\":\\\"0\\\"},\\\"l_c_st_pw_30\\\":{\\\"p_c_st_pw_30\\\":\\\"0\\\"},\\\"l_c_st_pw_8_r\\\":{\\\"p_c_st_pw_8_r\\\":\\\"0\\\"},\\\"l_c_st_v_30\\\":{\\\"p_c_st_v_30\\\":\\\"0\\\"},\\\"l_c_style\\\":{\\\"p_c_style\\\":\\\"1\\\"},\\\"l_creative_st_bi_8\\\":{\\\"p_creative_st_bi_8\\\":\\\"0\\\"},\\\"l_creative_st_v_8\\\":{\\\"p_creative_st_v_8\\\":\\\"0\\\"},\\\"l_f_h\\\":{\\\"p_f_h\\\":\\\"0\\\"},\\\"l_m_bfa\\\":{\\\"p_m_bfa\\\":\\\"a\\\"},\\\"l_m_cf_2\\\":{\\\"p_m_cf_2\\\":\\\"0\\\"},\\\"l_m_cf_3\\\":{\\\"p_m_cf_3\\\":\\\"0\\\"},\\\"l_m_cf_4\\\":{\\\"p_m_cf_4\\\":\\\"0\\\"},\\\"l_m_cf_5\\\":{\\\"p_m_cf_5\\\":\\\"0\\\"},\\\"l_m_cv_3\\\":{\\\"p_m_cv_3\\\":\\\"0\\\"},\\\"l_m_cv_5\\\":{\\\"p_m_cv_5\\\":\\\"0\\\"},\\\"l_native_ad\\\":{\\\"label\\\":\\\"old\\\"},\\\"lps_switch\\\":{\\\"p_lps_switch\\\":0,\\\"p_lps_switch_android\\\":0},\\\"lps_switch2\\\":{\\\"p_lps_switch2\\\":\\\"0\\\"},\\\"morph_switch\\\":{\\\"p_morph_switch\\\":\\\"on\\\"}}\",\"view_x_tracks\":[\"https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?nt=0\\u0026pdi=1523617890848616\\u0026idi=2006\\u0026ar=0.0000003217603730973035\\u0026pf=4\\u0026tu=https%3A%2F%2Fwww.togocareer.com%2Fservices.html%3FtgcChannel%3D%3Dzhihu%26tuwen0805\\u0026ui=1.171.46.33\\u0026ed=CjEEewhlKSlzRzYDU2d-U1toAHhddXd0f0ZjSggxeQBGIAFyXntjIX1BYFBUdi1KVncKfB4iMyRzE2JeAmJ4FhssBHsIc2N3dwMxDgxmfghcdgtsWzdncGhGPVoCYX0WCTUEeQlyfCw7GGK_IWaLj3_z_w%3D%3D\\u0026ut=deb9c276440c470f9c386c2db6870abe\\u0026au=4280\\u0026ts=1566397201\"]}]}"}},"advancedStyle":{},"commonAnswerCount":0,"hiddenAnswerCount":0,"meta":{},"autoInvitation":{},"simpleConcernedFollowers":{},"draftStatus":{},"disclaimers":{}},"shareTexts":{},"answers":{"voters":{},"copyrightApplicants":{},"favlists":{},"newAnswer":{},"concernedUpvoters":{},"simpleConcernedUpvoters":{},"paidContent":{},"settings":{}},"banner":{},"topic":{"bios":{},"hot":{},"newest":{},"top":{},"unanswered":{},"questions":{},"followers":{},"contributors":{},"parent":{},"children":{},"bestAnswerers":{},"wikiMeta":{},"index":{},"intro":{},"meta":{},"schema":{},"creatorWall":{},"wikiEditInfo":{},"committedWiki":{}},"explore":{"recommendations":{},"specials":{},"roundtables":{},"collections":{},"columns":{}},"articles":{"voters":{}},"favlists":{"relations":{}},"pins":{"voters":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotListCategories":[],"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]}},"upload":{},"video":{"data":{},"shareVideoDetail":{},"last":{}},"guide":{"guide":{"isFetching":false,"isShowGuide":false}},"reward":{"answer":{},"article":{},"question":{}},"search":{"recommendSearch":[],"topSearch":{},"searchValue":{},"suggestSearch":{},"attachedInfo":{},"nextOffset":{},"topicReview":{},"generalByQuery":{},"generalByQueryInADay":{},"generalByQueryInAWeek":{},"generalByQueryInThreeMonths":{},"peopleByQuery":{},"topicByQuery":{},"columnByQuery":{},"liveByQuery":{},"albumByQuery":{},"eBookByQuery":{}},"publicEditPermission":{},"readStatus":{},"draftHistory":{"history":{},"drafts":{}},"notifications":{"recent":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"history":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"notificationActors":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"recentNotificationEntry":"all"},"specials":{"entities":{},"all":{"data":[],"paging":{},"isLoading":false}},"collections":{"hot":{"data":[],"paging":{},"isLoading":false}}},"subAppName":"main"}</script><script src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/vendor.7842b402f56b92d57f3e.js.下載"></script><script src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/main.app.7c8634e8d9de8fd5d961.js.下載"></script><script src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/main.question-routes.cd051ae1fd8b5a422985.js.下載"></script><script src="./怎么选取训练神经网络时的Batch size_ - 知乎_files/zap.js.下載"></script><div><div style="display: none;">想来知乎工作？请发送邮件到 jobs@zhihu.com</div></div></body></html>